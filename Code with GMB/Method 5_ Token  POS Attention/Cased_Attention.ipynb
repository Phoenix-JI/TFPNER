{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Cased_Attention.ipynb","provenance":[],"collapsed_sections":["DR6WnnwSiiPx"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DR6WnnwSiiPx"},"source":["## **1. Find the corresponding positive values for NER, POS, Chunk tags**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"stBkFifLiMpd","executionInfo":{"status":"ok","timestamp":1639203009618,"user_tz":-480,"elapsed":386,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"552788b8-b232-4ec6-838c-eb6f7cb60923"},"source":["Ner_Tag = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n","Ner_Number = [i for i in range(len(Ner_Tag))]\n","Ner = list(zip(Ner_Tag,Ner_Number))\n","print(Ner)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('O', 0), ('B-PER', 1), ('I-PER', 2), ('B-ORG', 3), ('I-ORG', 4), ('B-LOC', 5), ('I-LOC', 6), ('B-MISC', 7), ('I-MISC', 8)]\n"]}]},{"cell_type":"code","source":["NerDict = dict((v,k) for k,v in dict(Ner).items())\n","NerDict[3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"1NZ1Arx7h03f","executionInfo":{"status":"ok","timestamp":1639203010036,"user_tz":-480,"elapsed":22,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"a618ba25-1a7d-4607-b397-da63d12ba72d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'B-ORG'"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ehS8Npr8hViP","executionInfo":{"status":"ok","timestamp":1639203010037,"user_tz":-480,"elapsed":18,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"dd9fd17c-4d2d-445f-fd07-76b7cb7ed38f"},"source":["Chunk_Tag = ['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP']\n","Chunk_Number = [i for i in range(len(Chunk_Tag))]\n","Chunk = list(zip(Chunk_Tag,Chunk_Number))\n","print(Chunk)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('O', 0), ('B-ADJP', 1), ('I-ADJP', 2), ('B-ADVP', 3), ('I-ADVP', 4), ('B-CONJP', 5), ('I-CONJP', 6), ('B-INTJ', 7), ('I-INTJ', 8), ('B-LST', 9), ('I-LST', 10), ('B-NP', 11), ('I-NP', 12), ('B-PP', 13), ('I-PP', 14), ('B-PRT', 15), ('I-PRT', 16), ('B-SBAR', 17), ('I-SBAR', 18), ('B-UCP', 19), ('I-UCP', 20), ('B-VP', 21), ('I-VP', 22)]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJVSOQ0LfL0z","executionInfo":{"status":"ok","timestamp":1639203010037,"user_tz":-480,"elapsed":15,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"0e3877ed-7e28-488a-bd59-8b985370e8c6"},"source":["POS_Tag = ['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n","POS_Number = [i for i in range(len(POS_Tag))]\n","POS = list(zip(POS_Tag,POS_Number))\n","print(POS)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('\"', 0), (\"''\", 1), ('#', 2), ('$', 3), ('(', 4), (')', 5), (',', 6), ('.', 7), (':', 8), ('``', 9), ('CC', 10), ('CD', 11), ('DT', 12), ('EX', 13), ('FW', 14), ('IN', 15), ('JJ', 16), ('JJR', 17), ('JJS', 18), ('LS', 19), ('MD', 20), ('NN', 21), ('NNP', 22), ('NNPS', 23), ('NNS', 24), ('NN|SYM', 25), ('PDT', 26), ('POS', 27), ('PRP', 28), ('PRP$', 29), ('RB', 30), ('RBR', 31), ('RBS', 32), ('RP', 33), ('SYM', 34), ('TO', 35), ('UH', 36), ('VB', 37), ('VBD', 38), ('VBG', 39), ('VBN', 40), ('VBP', 41), ('VBZ', 42), ('WDT', 43), ('WP', 44), ('WP$', 45), ('WRB', 46)]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xIG91lbdtIFX","executionInfo":{"status":"ok","timestamp":1639203010037,"user_tz":-480,"elapsed":13,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"cb653a5c-148b-43a1-9012-ee28aaea225c"},"source":["POS_Number = [i for i in range(len(POS_Tag))]\n","POS = list(zip(POS_Tag,POS_Number))\n","print(POS)\n","New_POSDict = dict((v,k) for k,v in dict(POS).items())\n","print(New_POSDict)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('\"', 0), (\"''\", 1), ('#', 2), ('$', 3), ('(', 4), (')', 5), (',', 6), ('.', 7), (':', 8), ('``', 9), ('CC', 10), ('CD', 11), ('DT', 12), ('EX', 13), ('FW', 14), ('IN', 15), ('JJ', 16), ('JJR', 17), ('JJS', 18), ('LS', 19), ('MD', 20), ('NN', 21), ('NNP', 22), ('NNPS', 23), ('NNS', 24), ('NN|SYM', 25), ('PDT', 26), ('POS', 27), ('PRP', 28), ('PRP$', 29), ('RB', 30), ('RBR', 31), ('RBS', 32), ('RP', 33), ('SYM', 34), ('TO', 35), ('UH', 36), ('VB', 37), ('VBD', 38), ('VBG', 39), ('VBN', 40), ('VBP', 41), ('VBZ', 42), ('WDT', 43), ('WP', 44), ('WP$', 45), ('WRB', 46)]\n","{0: '\"', 1: \"''\", 2: '#', 3: '$', 4: '(', 5: ')', 6: ',', 7: '.', 8: ':', 9: '``', 10: 'CC', 11: 'CD', 12: 'DT', 13: 'EX', 14: 'FW', 15: 'IN', 16: 'JJ', 17: 'JJR', 18: 'JJS', 19: 'LS', 20: 'MD', 21: 'NN', 22: 'NNP', 23: 'NNPS', 24: 'NNS', 25: 'NN|SYM', 26: 'PDT', 27: 'POS', 28: 'PRP', 29: 'PRP$', 30: 'RB', 31: 'RBR', 32: 'RBS', 33: 'RP', 34: 'SYM', 35: 'TO', 36: 'UH', 37: 'VB', 38: 'VBD', 39: 'VBG', 40: 'VBN', 41: 'VBP', 42: 'VBZ', 43: 'WDT', 44: 'WP', 45: 'WP$', 46: 'WRB'}\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdC1YDaLEF0e","executionInfo":{"status":"ok","timestamp":1639203010038,"user_tz":-480,"elapsed":12,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"ef00fb0f-f261-4007-9ed7-f0094136a8b3"},"source":["New_POSDict"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: '\"',\n"," 1: \"''\",\n"," 2: '#',\n"," 3: '$',\n"," 4: '(',\n"," 5: ')',\n"," 6: ',',\n"," 7: '.',\n"," 8: ':',\n"," 9: '``',\n"," 10: 'CC',\n"," 11: 'CD',\n"," 12: 'DT',\n"," 13: 'EX',\n"," 14: 'FW',\n"," 15: 'IN',\n"," 16: 'JJ',\n"," 17: 'JJR',\n"," 18: 'JJS',\n"," 19: 'LS',\n"," 20: 'MD',\n"," 21: 'NN',\n"," 22: 'NNP',\n"," 23: 'NNPS',\n"," 24: 'NNS',\n"," 25: 'NN|SYM',\n"," 26: 'PDT',\n"," 27: 'POS',\n"," 28: 'PRP',\n"," 29: 'PRP$',\n"," 30: 'RB',\n"," 31: 'RBR',\n"," 32: 'RBS',\n"," 33: 'RP',\n"," 34: 'SYM',\n"," 35: 'TO',\n"," 36: 'UH',\n"," 37: 'VB',\n"," 38: 'VBD',\n"," 39: 'VBG',\n"," 40: 'VBN',\n"," 41: 'VBP',\n"," 42: 'VBZ',\n"," 43: 'WDT',\n"," 44: 'WP',\n"," 45: 'WP$',\n"," 46: 'WRB'}"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"wNF30bg7jgaO"},"source":["# **2. Data Preprocessing for BERT Model (Apply Hugging Face Data)**"]},{"cell_type":"code","metadata":{"id":"BM9Vp2YBtkxg"},"source":["model_pretrained = 'bert-base-cased'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IXOdIATZoNVQ"},"source":["### (2) Covert Data to BERT Input Style"]},{"cell_type":"markdown","source":["## New. Bank Dataset"],"metadata":{"id":"nnPcDOGeLH0b"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C7RE4jRsLEay","executionInfo":{"status":"ok","timestamp":1639220885278,"user_tz":-480,"elapsed":19378,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"bcec260d-136b-475a-cc14-0a4761c51784"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["model_pretrained = 'bert-base-cased'"],"metadata":{"id":"NLv-hSziMYT1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers seqeval[gpu]"],"metadata":{"id":"-e5r14tELQgF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification"],"metadata":{"id":"tHKxpxa1LZuB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv(\"/content/drive/MyDrive/NLP/Final Project/New Dataset/Bank/ner_datasetreference_new.csv\", encoding='unicode_escape')\n","data.head()\n","\n","data.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kX6J7PvCLfi4","executionInfo":{"status":"ok","timestamp":1639220915603,"user_tz":-480,"elapsed":3618,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"6e5be177-5704-4f91-a16a-f69e92cdb47b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sentence #      47959\n","Word          1048575\n","POS           1048575\n","Tag           1048575\n","dtype: int64"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["entities_to_remove = [\"B-art\", \"I-art\", \"B-eve\", \"I-eve\", \"B-nat\", \"I-nat\"]\n","data = data[~data.Tag.isin(entities_to_remove)]\n","data.head()\n","data.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1GeMoY62LiZh","executionInfo":{"status":"ok","timestamp":1639220927957,"user_tz":-480,"elapsed":364,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"4086ea4c-00e5-4de3-dc28-7f36943a4ad3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sentence #      47920\n","Word          1047063\n","POS           1047063\n","Tag           1047063\n","dtype: int64"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["\"\"\"We create 2 dictionaries for NE: one that maps individual tags to indices, and one that maps indices to their individual tags. This is necessary in order to create the labels (as computers work with numbers = indices, rather than words = tags) - see further in this notebook.\"\"\"\n","\n","labels_to_ids = {k: v for v, k in enumerate(data.Tag.unique())}\n","ids_to_labels = {v: k for v, k in enumerate(data.Tag.unique())}\n","print(labels_to_ids)\n","print(ids_to_labels)\n","\n","\"\"\"We create 2 dictionaries for POS: one that maps individual tags to indices, and one that maps indices to their individual tags. This is necessary in order to create the labels (as computers work with numbers = indices, rather than words = tags) - see further in this notebook.\"\"\"\n","\n","POS_labels_to_ids = {k: v for v, k in enumerate(data.POS.unique())}\n","POS_ids_to_labels = {v: k for v, k in enumerate(data.POS.unique())}\n","print(POS_labels_to_ids)\n","print(POS_ids_to_labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5bYtZ4PWLmgr","executionInfo":{"status":"ok","timestamp":1639220955864,"user_tz":-480,"elapsed":482,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"b61670b9-6387-4754-d262-4a894c73dbef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'I-per': 8, 'I-gpe': 9, 'I-tim': 10}\n","{0: 'O', 1: 'B-geo', 2: 'B-gpe', 3: 'B-per', 4: 'I-geo', 5: 'B-org', 6: 'I-org', 7: 'B-tim', 8: 'I-per', 9: 'I-gpe', 10: 'I-tim'}\n","{'NNS': 0, 'IN': 1, 'VBP': 2, 'VBN': 3, 'NNP': 4, 'TO': 5, 'VB': 6, 'DT': 7, 'NN': 8, 'CC': 9, 'JJ': 10, '.': 11, 'VBD': 12, 'WP': 13, '``': 14, 'CD': 15, 'PRP': 16, 'VBZ': 17, 'POS': 18, 'VBG': 19, 'RB': 20, ',': 21, 'WRB': 22, 'PRP$': 23, 'MD': 24, 'WDT': 25, 'JJR': 26, ':': 27, 'JJS': 28, 'WP$': 29, 'RP': 30, 'PDT': 31, 'NNPS': 32, 'EX': 33, 'RBS': 34, 'LRB': 35, 'RRB': 36, '$': 37, 'RBR': 38, ';': 39, 'UH': 40, 'FW': 41}\n","{0: 'NNS', 1: 'IN', 2: 'VBP', 3: 'VBN', 4: 'NNP', 5: 'TO', 6: 'VB', 7: 'DT', 8: 'NN', 9: 'CC', 10: 'JJ', 11: '.', 12: 'VBD', 13: 'WP', 14: '``', 15: 'CD', 16: 'PRP', 17: 'VBZ', 18: 'POS', 19: 'VBG', 20: 'RB', 21: ',', 22: 'WRB', 23: 'PRP$', 24: 'MD', 25: 'WDT', 26: 'JJR', 27: ':', 28: 'JJS', 29: 'WP$', 30: 'RP', 31: 'PDT', 32: 'NNPS', 33: 'EX', 34: 'RBS', 35: 'LRB', 36: 'RRB', 37: '$', 38: 'RBR', 39: ';', 40: 'UH', 41: 'FW'}\n"]}]},{"cell_type":"code","source":["print(\"Number of NE tags: {}\".format(len(data.Tag.unique()))) # 17个\n","frequencies_NE = data.Tag.value_counts()\n","frequencies_NE\n","Ner_Tag = list(data.Tag.unique())\n","Ner_Number = [i for i in range(len(Ner_Tag))]\n","Ner = list(zip(Ner_Tag,Ner_Number))\n","print(Ner)\n","\n","# count POS tag\n","print(\"Number of POS tags: {}\".format(len(data.POS.unique()))) # 42个\n","frequencies_POS = data.POS.value_counts()\n","frequencies_POS\n","POS_Tag = list(data.POS.unique())\n","POS_Number = [i for i in range(len(POS_Tag))]\n","POS = list(zip(POS_Tag,POS_Number))\n","print(POS)\n","print(len(POS_Tag))\n","New_POSDict = dict((v,k) for k,v in dict(POS).items())\n","print(New_POSDict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lN__quLqLw3l","executionInfo":{"status":"ok","timestamp":1639221016766,"user_tz":-480,"elapsed":513,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"5475adaf-51a3-4c0b-bbf3-6e6555c873bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of NE tags: 11\n","[('O', 0), ('B-geo', 1), ('B-gpe', 2), ('B-per', 3), ('I-geo', 4), ('B-org', 5), ('I-org', 6), ('B-tim', 7), ('I-per', 8), ('I-gpe', 9), ('I-tim', 10)]\n","Number of POS tags: 42\n","[('NNS', 0), ('IN', 1), ('VBP', 2), ('VBN', 3), ('NNP', 4), ('TO', 5), ('VB', 6), ('DT', 7), ('NN', 8), ('CC', 9), ('JJ', 10), ('.', 11), ('VBD', 12), ('WP', 13), ('``', 14), ('CD', 15), ('PRP', 16), ('VBZ', 17), ('POS', 18), ('VBG', 19), ('RB', 20), (',', 21), ('WRB', 22), ('PRP$', 23), ('MD', 24), ('WDT', 25), ('JJR', 26), (':', 27), ('JJS', 28), ('WP$', 29), ('RP', 30), ('PDT', 31), ('NNPS', 32), ('EX', 33), ('RBS', 34), ('LRB', 35), ('RRB', 36), ('$', 37), ('RBR', 38), (';', 39), ('UH', 40), ('FW', 41)]\n","42\n","{0: 'NNS', 1: 'IN', 2: 'VBP', 3: 'VBN', 4: 'NNP', 5: 'TO', 6: 'VB', 7: 'DT', 8: 'NN', 9: 'CC', 10: 'JJ', 11: '.', 12: 'VBD', 13: 'WP', 14: '``', 15: 'CD', 16: 'PRP', 17: 'VBZ', 18: 'POS', 19: 'VBG', 20: 'RB', 21: ',', 22: 'WRB', 23: 'PRP$', 24: 'MD', 25: 'WDT', 26: 'JJR', 27: ':', 28: 'JJS', 29: 'WP$', 30: 'RP', 31: 'PDT', 32: 'NNPS', 33: 'EX', 34: 'RBS', 35: 'LRB', 36: 'RRB', 37: '$', 38: 'RBR', 39: ';', 40: 'UH', 41: 'FW'}\n"]}]},{"cell_type":"code","source":["data = data.fillna(method='ffill')\n","data.head()"],"metadata":{"id":"_XSC-yp2L79K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# let's create a new column called \"sentence\" which groups the words by sentence \n","data['sentence'] = data[['Sentence #','Word','Tag', 'POS']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n","# let's also create a new column called \"word_labels\" which groups the tags by sentence \n","data['word_labels'] = data[['Sentence #','Word','Tag', 'POS']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n","data['word_labels_POS'] = data[['Sentence #','Word','Tag', 'POS']].groupby(['Sentence #'])['POS'].transform(lambda x: '_,_,_'.join(x))\n","data.head()"],"metadata":{"id":"uKjHCFNmL_ui"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = data[[\"sentence\", \"word_labels\", 'word_labels_POS']].drop_duplicates().reset_index(drop=True)\n","data.head()\n","\n","len(data)\n","\"\"\"Let's verify that a random sentence and its corresponding tags are correct:\"\"\"\n","\n","print(data.iloc[41].sentence)\n","print(data.iloc[41].word_labels)\n","print(data.iloc[41].word_labels_POS)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XaseMwtwMCy_","executionInfo":{"status":"ok","timestamp":1639221183809,"user_tz":-480,"elapsed":2127,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"c13b00b4-1913-43ef-bb78-f2e26bc469cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bedfordshire police said Tuesday that Omar Khayam was arrested in Bedford for breaching the conditions of his parole .\n","B-gpe,O,O,B-tim,O,B-per,I-per,O,O,O,B-geo,O,O,O,O,O,O,O,O\n","NNP_,_,_NNS_,_,_VBD_,_,_NNP_,_,_IN_,_,_NNP_,_,_NNP_,_,_VBD_,_,_VBN_,_,_IN_,_,_NNP_,_,_IN_,_,_VBG_,_,_DT_,_,_NNS_,_,_IN_,_,_PRP$_,_,_NN_,_,_.\n"]}]},{"cell_type":"code","source":["train_df, validate_df, test_df = \\\n","              np.split(data.sample(frac=1, random_state=42), \n","                       [int(.85*len(data)), int(.925*len(data))])\n","\n","train_df = train_df.reset_index(drop=True)\n","validate_df = validate_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","dataset = {'train':train_df, 'validation':validate_df, 'test':test_df}"],"metadata":{"id":"ZKsgZTRaMK55"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_LEN = 128     \n","TRAIN_BATCH_SIZE = 4\n","TEST_BATCH_SIZE = 2\n","tokenizer = BertTokenizerFast.from_pretrained(model_pretrained)"],"metadata":{"id":"bFzI1P-aMMhI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset['train'].keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0WfUE7UWRaZy","executionInfo":{"status":"ok","timestamp":1639222490905,"user_tz":-480,"elapsed":512,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"63a60660-81b6-47d9-ffcb-b6be8e0a1da4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['sentence', 'word_labels', 'word_labels_POS'], dtype='object')"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["print(dataset['train']['sentence'][0].strip().split())\n","print(dataset['train']['word_labels'][0])\n","print(dataset['train']['word_labels_POS'][0])\n","print(dataset['train']['word_labels_POS'][0].split(\"_,_,_\"))\n","print(labels_to_ids)\n","print(POS_labels_to_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gnQgfxdWRk6z","executionInfo":{"status":"ok","timestamp":1639222970865,"user_tz":-480,"elapsed":378,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"52f30704-ac4c-4835-b237-d3e8785d17e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Associates', 'of', 'Tomar', 'told', 'the', 'Associated', 'Press', 'that', 'the', 'magazine', 'had', 'no', 'intention', 'of', 'offending', 'Muslims', '.']\n","B-org,I-org,I-org,O,O,B-org,I-org,O,O,O,O,O,O,O,O,B-org,O\n","NNP_,_,_IN_,_,_NNP_,_,_VBD_,_,_DT_,_,_NNP_,_,_NNP_,_,_IN_,_,_DT_,_,_NN_,_,_VBD_,_,_DT_,_,_NN_,_,_IN_,_,_VBG_,_,_NNS_,_,_.\n","['NNP', 'IN', 'NNP', 'VBD', 'DT', 'NNP', 'NNP', 'IN', 'DT', 'NN', 'VBD', 'DT', 'NN', 'IN', 'VBG', 'NNS', '.']\n","{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'I-per': 8, 'I-gpe': 9, 'I-tim': 10}\n","{'NNS': 0, 'IN': 1, 'VBP': 2, 'VBN': 3, 'NNP': 4, 'TO': 5, 'VB': 6, 'DT': 7, 'NN': 8, 'CC': 9, 'JJ': 10, '.': 11, 'VBD': 12, 'WP': 13, '``': 14, 'CD': 15, 'PRP': 16, 'VBZ': 17, 'POS': 18, 'VBG': 19, 'RB': 20, ',': 21, 'WRB': 22, 'PRP$': 23, 'MD': 24, 'WDT': 25, 'JJR': 26, ':': 27, 'JJS': 28, 'WP$': 29, 'RP': 30, 'PDT': 31, 'NNPS': 32, 'EX': 33, 'RBS': 34, 'LRB': 35, 'RRB': 36, '$': 37, 'RBR': 38, ';': 39, 'UH': 40, 'FW': 41}\n"]}]},{"cell_type":"code","source":["print('Length of vocabulary size: ',tokenizer.vocab_size)\n","print('Number of POS: ',len(POS_Tag))\n","\n","# Add [] outside the pos \n","New_POS = []\n","for pos in POS_Tag:\n","    pos = '['+ pos +']'\n","    pos = pos\n","    New_POS.append(pos)\n","print(New_POS)\n","\n","# Tokenizer add new words -> pos\n","tokenizer.add_tokens(New_POS)\n","\n","# Check added new pos\n","print(\"New added words: \",tokenizer.get_added_vocab())\n","print('Length of added words: ',len(tokenizer.get_added_vocab()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aKtVUuAzU86n","executionInfo":{"status":"ok","timestamp":1639223589515,"user_tz":-480,"elapsed":565,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"c9214e2b-9de7-4dea-fd72-7ffec66ca7e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of vocabulary size:  28996\n","Number of POS:  42\n","['[NNS]', '[IN]', '[VBP]', '[VBN]', '[NNP]', '[TO]', '[VB]', '[DT]', '[NN]', '[CC]', '[JJ]', '[.]', '[VBD]', '[WP]', '[``]', '[CD]', '[PRP]', '[VBZ]', '[POS]', '[VBG]', '[RB]', '[,]', '[WRB]', '[PRP$]', '[MD]', '[WDT]', '[JJR]', '[:]', '[JJS]', '[WP$]', '[RP]', '[PDT]', '[NNPS]', '[EX]', '[RBS]', '[LRB]', '[RRB]', '[$]', '[RBR]', '[;]', '[UH]', '[FW]']\n","New added words:  {'[RBR]': 29034, '[VBP]': 28998, '[WDT]': 29021, '[WRB]': 29018, '[CC]': 29005, '[WP]': 29009, '[FW]': 29037, '[.]': 29007, '[VB]': 29002, '[RP]': 29026, '[UH]': 29036, '[,]': 29017, '[``]': 29010, '[CD]': 29011, '[JJR]': 29022, '[VBZ]': 29013, '[VBN]': 28999, '[PDT]': 29027, '[TO]': 29001, '[VBG]': 29015, '[JJS]': 29024, '[DT]': 29003, '[NNPS]': 29028, '[NNS]': 28996, '[PRP]': 29012, '[MD]': 29020, '[JJ]': 29006, '[LRB]': 29031, '[POS]': 29014, '[:]': 29023, '[$]': 29033, '[;]': 29035, '[EX]': 29029, '[NN]': 29004, '[WP$]': 29025, '[RB]': 29016, '[RBS]': 29030, '[RRB]': 29032, '[PRP$]': 29019, '[VBD]': 29008, '[NNP]': 29000, '[IN]': 28997}\n","Length of added words:  42\n"]}]},{"cell_type":"code","source":["class Preprocess_Data(Dataset):\n","  def __init__(self, dataset, tokenizer, max_len,usage): #usage -> train, validation, test\n","\n","        self.len = len(dataset[usage])\n","        self.data = dataset[usage]\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","  def __getitem__(self, index):\n","\n","        # step 1: get the sentence and word labels \n","        s1 = self.data['sentence'][index].strip().split()\n","        s2 = self.data['word_labels_POS'][index].split(\"_,_,_\")\n","\n","        s2_fix = [POS_labels_to_ids[s] for s in s2]\n","        # print(str(s2_fix))\n","        s3 = [New_POS[s] for s in s2_fix]\n","\n","        word_labels = self.data['word_labels'][index].split(\",\")\n","        word_labels = [labels_to_ids[label] for label in word_labels]\n","\n","\n","        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n","        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n","        encoding = self.tokenizer(s1,\n","                              is_split_into_words=True,\n","                              return_offsets_mapping=True,  #Set to True to return (char_start, char_end) for each token (default False)\n","                              padding='max_length', \n","                              truncation=True, \n","                              max_length=self.max_len)\n","\n","        encoding_pos = self.tokenizer(s3,\n","                              is_split_into_words=True,\n","                              return_offsets_mapping=True,  #Set to True to return (char_start, char_end) for each token (default False)\n","                              padding='max_length', \n","                              truncation=True, \n","                              max_length=self.max_len)\n","        \n","        \n","        # step 3: create token labels only for first word pieces of each tokenized word\n","\n","        labels = word_labels\n","        pos = encoding_pos['input_ids']\n","\n","        # create an empty array of -100 of length max_length\n","        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n","\n","        encoded_pos = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * 0\n","        \n","        # set only labels whose first offset position is 0 and the second is not 0\n","        i = 0\n","        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n","          if mapping[0] == 0 and mapping[1] != 0:\n","            # overwrite label\n","            encoded_labels[idx] = labels[i]\n","            i += 1\n","\n","        # set whole offset positions have pos\n","        i = 0\n","        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n","    \n","            # overwrite pos\n","          encoded_pos[idx] = pos[i]\n","\n","          if mapping[0] != 0 and mapping[1] != 0:\n","            encoded_pos[idx] = encoded_pos[idx-1]\n","            continue\n","\n","          i += 1\n","\n","\n","        # step 4: turn everything into PyTorch tensors\n","        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n","        item['labels'] = torch.as_tensor(encoded_labels)\n","        item['pos_ids'] = torch.as_tensor(encoded_pos)\n","        \n","        return item\n","\n","  def __len__(self):\n","        return self.len"],"metadata":{"id":"oqU-Afz_MV5-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_set = Preprocess_Data(dataset, tokenizer, MAX_LEN, 'train')\n","validation_set = Preprocess_Data(dataset, tokenizer, MAX_LEN, 'validation')\n","testing_set = Preprocess_Data(dataset, tokenizer, MAX_LEN, 'test')\n","print(len(training_set),len(validation_set),len(testing_set))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5cGAdOcxMhmJ","executionInfo":{"status":"ok","timestamp":1639223659460,"user_tz":-480,"elapsed":394,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"c2278dd8-d2b4-4d37-c4c6-a67ffca54c46"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["40439 3568 3569\n"]}]},{"cell_type":"code","source":["training_set[0].keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yInJGdAvOCIt","executionInfo":{"status":"ok","timestamp":1639223661263,"user_tz":-480,"elapsed":4,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"169d7ab3-44dc-466c-cc5f-6a2b4a4264e6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels', 'pos_ids'])"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["#Verify the encoding result\n","i = 2\n","for token,token_type_id,number,mask,pos_tag,pos,label in zip(tokenizer.convert_ids_to_tokens(training_set[i][\"input_ids\"]), training_set[i][\"token_type_ids\"],training_set[i][\"input_ids\"],training_set[i][\"attention_mask\"],tokenizer.convert_ids_to_tokens(training_set[i][\"pos_ids\"]),training_set[i][\"pos_ids\"],training_set[i][\"labels\"]):\n","  print('{0:10}  {1:10}  {2:10}  {3:10}  {4:10}  {5:10} {6}'.format(token, token_type_id, number, mask, pos_tag,pos, label))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WzTxpcACMlqU","executionInfo":{"status":"ok","timestamp":1639224481246,"user_tz":-480,"elapsed":376,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"d6cfa6f7-30df-40c9-92bb-663f72efd1f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS]                0         101           1  [CLS]              101 -100\n","Many                 0        2408           1  [JJ]             29006 0\n","Taliban              0       17677           1  [NNP]            29000 5\n","and                  0        1105           1  [CC]             29005 0\n","al                   0        2393           1  [NNP]            29000 5\n","-                    0         118           1  [NNP]            29000 -100\n","Q                    0         154           1  [NNP]            29000 -100\n","##aid                0       19954           1  [NNP]            29000 -100\n","##a                  0        1161           1  [NNP]            29000 -100\n","militants            0       19342           1  [NNS]            28996 0\n","fled                 0        6192           1  [VBD]            29008 0\n","to                   0        1106           1  [TO]             29001 0\n","Pakistan             0        3658           1  [NNP]            29000 1\n","from                 0        1121           1  [IN]             28997 0\n","neighboring           0        8480           1  [VBG]            29015 0\n","Afghanistan           0        6469           1  [NNP]            29000 2\n","after                0        1170           1  [IN]             28997 0\n","the                  0        1103           1  [DT]             29003 0\n","U                    0         158           1  [JJ]             29006 0\n",".                    0         119           1  [JJ]             29006 -100\n","S                    0         156           1  [JJ]             29006 -100\n",".                    0         119           1  [JJ]             29006 -100\n","-                    0         118           1  [JJ]             29006 -100\n","led                  0        1521           1  [JJ]             29006 -100\n","invasion             0        4923           1  [NN]             29004 0\n","in                   0        1107           1  [IN]             28997 0\n","Afghanistan           0        6469           1  [NNP]            29000 2\n","top                  0        1499           1  [VBD]            29008 0\n","##pled               0       13229           1  [VBD]            29008 -100\n","the                  0        1103           1  [DT]             29003 0\n","Taliban              0       17677           1  [NNP]            29000 5\n","in                   0        1107           1  [IN]             28997 0\n","late                 0        1523           1  [JJ]             29006 0\n","2001                 0        1630           1  [CD]             29011 7\n",".                    0         119           1  [.]              29007 0\n","[SEP]                0         102           1  [SEP]              102 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n","[PAD]                0           0           0  [PAD]                0 -100\n"]}]},{"cell_type":"code","source":["training_loader = DataLoader(training_set, batch_size = TRAIN_BATCH_SIZE, shuffle=True,num_workers=0)\n","validation_loader = DataLoader(validation_set,batch_size = TRAIN_BATCH_SIZE, shuffle=True,num_workers=0)\n","testing_loader = DataLoader(testing_set,batch_size = TEST_BATCH_SIZE, shuffle=True,num_workers=0)"],"metadata":{"id":"ZrG1rBNxMueF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(training_loader),len(validation_loader),len(testing_loader))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80VZwp85Mu37","executionInfo":{"status":"ok","timestamp":1639223777243,"user_tz":-480,"elapsed":464,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"a44ba6cf-d0a4-41bd-a7dc-6baf38f5ba44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10110 892 1785\n"]}]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions,TokenClassifierOutput\n","from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification, BertModel,BertPreTrainedModel"],"metadata":{"id":"Qo6hAwQYXuKI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BertForTokenClassification(BertPreTrainedModel):\n","  \n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.bert = BertModel(config, add_pooling_layer=False)\n","\n","        # Attention for token and pos\n","        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads,batch_first=True)\n","\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","\n","        self.layerNorm = nn.LayerNorm(embed_dim)\n","\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        pos_ids = None, # New added pos\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n","            1]``.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        # for token\n","        \n","        outputs_token = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        # for pos\n","        with torch.no_grad():\n","          outputs_pos = self.bert(\n","              pos_ids,\n","              attention_mask=attention_mask,\n","              token_type_ids=token_type_ids,\n","              position_ids=position_ids,\n","              head_mask=head_mask,\n","              inputs_embeds=inputs_embeds,\n","              output_attentions=output_attentions,\n","              output_hidden_states=output_hidden_states,\n","              return_dict=return_dict,\n","          )\n","      \n","      # define Q K V for multihead_attention\n","\n","        query = outputs_token[0]\n","        key = outputs_pos[0]\n","        value = outputs_pos[0]\n","\n","        residual = outputs_token[0]\n","\n","        #key_padding_mask = attention_mask\n","\n","        attn_output, attn_output_weights = self.multihead_attn(query, key, value)\n","        sequence_output = attn_output\n","        sequence_output = self.dropout(sequence_output)\n","        \n","        # Residual connection and layer norm\n","        sequence_output = sequence_output+ residual\n","        sequence_output = self.layerNorm(sequence_output)\n","        \n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)\n","                active_labels = torch.where(\n","                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n","                )\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return TokenClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            # hidden_states=outputs.hidden_states,\n","            attentions=attn_output_weights\n","        )"],"metadata":{"id":"RhqkjP6LXqih"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6hF-6klfKomd"},"source":["# **3. Define the Model**"]},{"cell_type":"markdown","metadata":{"id":"IKXls9ALdeJt"},"source":["### 1) Train the Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ndNwQePxFFdY","executionInfo":{"status":"ok","timestamp":1639223786821,"user_tz":-480,"elapsed":424,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"ef7b8aa3-1d7d-4a6e-9f0b-ba8fb3693359"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Dec 11 11:56:26 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    22W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6LvvmRHQMH2g","executionInfo":{"status":"ok","timestamp":1639223790775,"user_tz":-480,"elapsed":367,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"d93cb7a3-b26d-4478-c08f-d0f615c6fd94"},"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","metadata":{"id":"VCEY_Fb2a7Hm"},"source":["EPOCHS = 5\n","LEARNING_RATE = 1e-05\n","MAX_GRAD_NORM = 10\n","embed_dim = 768\n","num_heads = 8"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K8vCJCAFLWgK"},"source":["# Define the model by just BertForTokenClassification\n","model = BertForTokenClassification.from_pretrained(model_pretrained, num_labels=len(Ner_Tag))\n","model.resize_token_embeddings(len(tokenizer))\n","model.to(device)\n","optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k65MdBHOH-p6"},"source":["import time\n","import os\n","Step_loss = []\n","Train_time = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3800AVERAin"},"source":["def train(epoch):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    # put model in training mode\n","    model.train()\n","\n","    for idx, batch in enumerate(training_loader):\n","\n","        # if idx >200:\n","        #   break\n","        \n","        ids = batch['input_ids'].to(device, dtype = torch.long)\n","        mask = batch['attention_mask'].to(device, dtype = torch.long)\n","        labels = batch['labels'].to(device, dtype = torch.long)\n","        pos_ids = batch['pos_ids'].to(device, dtype = torch.long)\n","        #token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n","\n","        outputs = model(input_ids=ids, attention_mask=mask, labels=labels, pos_ids=pos_ids)\n","        loss = outputs[0]\n","        tr_logits = outputs[1]\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 100==0:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","            Step_loss.append(loss_step)\n","            if idx!=0:\n","              time_spent = time.time() - start_time\n","              Train_time.append(time_spent)\n","              print(\"--- %s seconds ---\" % (time_spent))\n","            start_time = time.time()    \n","           \n","        # compute training accuracy\n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","        \n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        \n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","\n","        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","        tr_accuracy += tmp_tr_accuracy\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","    print(f\"Training loss epoch: {epoch_loss}\")\n","    print(f\"Training accuracy epoch: {tr_accuracy}\")\n","\n","    # --------------------------------------------------------------------------------------------------------------------\n","    # After the completion of each training epoch\n","    # measure our performance on validation set.\n","\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(validation_loader):\n","\n","            # if idx >200:\n","            #   break\n","            \n","            ids = batch['input_ids'].to(device, dtype = torch.long)\n","            mask = batch['attention_mask'].to(device, dtype = torch.long)\n","            labels = batch['labels'].to(device, dtype = torch.long)\n","            pos_ids = batch['pos_ids'].to(device, dtype = torch.long)\n","            #token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n","\n","            outputs = model(input_ids=ids, attention_mask=mask, labels=labels, pos_ids=pos_ids)\n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    print(f\"Validation Loss: {eval_loss}\")\n","    print(f\"Validation Accuracy: {eval_accuracy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bYynGVa0aUYt","executionInfo":{"status":"ok","timestamp":1639223900345,"user_tz":-480,"elapsed":368,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"527142f5-52d1-4c2f-8101-f8f0470e3645"},"source":["len(training_loader),len(validation_loader),len(testing_loader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10110, 892, 1785)"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions,TokenClassifierOutput\n","from seqeval.metrics import classification_report\n","New_NerDict = dict((v,k) for k,v in dict(Ner).items())\n","New_NerDict"],"metadata":{"id":"uIi2xMC__xHj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639224613354,"user_tz":-480,"elapsed":7,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"aaf371ce-ba5c-4f1b-805c-08e60bb419bc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'O',\n"," 1: 'B-geo',\n"," 2: 'B-gpe',\n"," 3: 'B-per',\n"," 4: 'I-geo',\n"," 5: 'B-org',\n"," 6: 'I-org',\n"," 7: 'B-tim',\n"," 8: 'I-per',\n"," 9: 'I-gpe',\n"," 10: 'I-tim'}"]},"metadata":{},"execution_count":99}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jlyi-LRGMlLR","outputId":"8ba53b64-38ba-4c6f-d6d3-c329d1f43d5d","executionInfo":{"status":"ok","timestamp":1639227330712,"user_tz":-480,"elapsed":2712820,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}}},"source":["# Pos Bert is fixed\n","EPOCHS = 3\n","\n","for epoch in range(EPOCHS):\n","\n","    directory = \"/content/drive/MyDrive/NLP/Final Project/New Dataset/Bank/Token POS Attention/Cased/Model_\"+str(epoch+1)\n","\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train(epoch)\n","    labels, predictions = valid(model, testing_loader)\n","\n","    labels_value = [[New_NerDict[i.item()] for i in labels]]\n","    pred_value = [[New_NerDict[i.item()] for i in predictions]]\n","\n","    print(classification_report(labels_value, pred_value))\n","\n","    tokenizer.save_vocabulary(directory)\n","    # save the model weights and its configuration file\n","    model.save_pretrained(directory)\n","    print('All files saved')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training epoch: 1\n","Training loss per 100 training steps: 2.9280693531036377\n","Training loss per 100 training steps: 0.726953187348819\n","--- 7.709433555603027 seconds ---\n","Training loss per 100 training steps: 0.5182724272582069\n","--- 7.625300645828247 seconds ---\n","Training loss per 100 training steps: 0.41129908758986033\n","--- 7.768185138702393 seconds ---\n","Training loss per 100 training steps: 0.353606902964023\n","--- 8.021301746368408 seconds ---\n","Training loss per 100 training steps: 0.3180320815465779\n","--- 8.125985383987427 seconds ---\n","Training loss per 100 training steps: 0.28962163454907236\n","--- 7.821030378341675 seconds ---\n","Training loss per 100 training steps: 0.2673045913276984\n","--- 8.104331493377686 seconds ---\n","Training loss per 100 training steps: 0.2510740271962472\n","--- 8.15462064743042 seconds ---\n","Training loss per 100 training steps: 0.23944452753679188\n","--- 7.797260999679565 seconds ---\n","Training loss per 100 training steps: 0.22880431564969497\n","--- 8.000443696975708 seconds ---\n","Training loss per 100 training steps: 0.2203815873034745\n","--- 7.872869253158569 seconds ---\n","Training loss per 100 training steps: 0.2120780664686886\n","--- 7.847416639328003 seconds ---\n","Training loss per 100 training steps: 0.20623173365522632\n","--- 7.961529970169067 seconds ---\n","Training loss per 100 training steps: 0.20016787803767092\n","--- 8.101166725158691 seconds ---\n","Training loss per 100 training steps: 0.1941939182275488\n","--- 8.210795640945435 seconds ---\n","Training loss per 100 training steps: 0.18898742570007204\n","--- 8.272655487060547 seconds ---\n","Training loss per 100 training steps: 0.18403713795838814\n","--- 8.107055902481079 seconds ---\n","Training loss per 100 training steps: 0.1806151075006826\n","--- 8.198683977127075 seconds ---\n","Training loss per 100 training steps: 0.17735357595723258\n","--- 8.487241506576538 seconds ---\n","Training loss per 100 training steps: 0.1736765713447231\n","--- 8.2102210521698 seconds ---\n","Training loss per 100 training steps: 0.1714632213891425\n","--- 7.969577312469482 seconds ---\n","Training loss per 100 training steps: 0.16887437230355187\n","--- 7.8857197761535645 seconds ---\n","Training loss per 100 training steps: 0.16686288846809674\n","--- 7.884983777999878 seconds ---\n","Training loss per 100 training steps: 0.16458397889773563\n","--- 7.897091865539551 seconds ---\n","Training loss per 100 training steps: 0.16247747759524656\n","--- 7.901118516921997 seconds ---\n","Training loss per 100 training steps: 0.16036267075782534\n","--- 7.916837215423584 seconds ---\n","Training loss per 100 training steps: 0.15821235973018324\n","--- 8.068552732467651 seconds ---\n","Training loss per 100 training steps: 0.1565134122306717\n","--- 7.935158729553223 seconds ---\n","Training loss per 100 training steps: 0.1544721675097688\n","--- 8.275508165359497 seconds ---\n","Training loss per 100 training steps: 0.15303344621255704\n","--- 7.939682960510254 seconds ---\n","Training loss per 100 training steps: 0.1514682929875177\n","--- 7.9827492237091064 seconds ---\n","Training loss per 100 training steps: 0.14990961007514642\n","--- 7.953686952590942 seconds ---\n","Training loss per 100 training steps: 0.1482829245530894\n","--- 8.023393392562866 seconds ---\n","Training loss per 100 training steps: 0.14681475270786387\n","--- 8.003450870513916 seconds ---\n","Training loss per 100 training steps: 0.1453306343252083\n","--- 8.272228002548218 seconds ---\n","Training loss per 100 training steps: 0.14420808517528372\n","--- 8.248708724975586 seconds ---\n","Training loss per 100 training steps: 0.1430093407947925\n","--- 7.950986385345459 seconds ---\n","Training loss per 100 training steps: 0.14173565254798706\n","--- 7.86194920539856 seconds ---\n","Training loss per 100 training steps: 0.14060985958365418\n","--- 7.8930463790893555 seconds ---\n","Training loss per 100 training steps: 0.1396866518110784\n","--- 7.8862504959106445 seconds ---\n","Training loss per 100 training steps: 0.13852512840348163\n","--- 7.959805011749268 seconds ---\n","Training loss per 100 training steps: 0.13792150268531025\n","--- 8.233861446380615 seconds ---\n","Training loss per 100 training steps: 0.1370131899418926\n","--- 7.866158962249756 seconds ---\n","Training loss per 100 training steps: 0.13633883842559655\n","--- 7.950475215911865 seconds ---\n","Training loss per 100 training steps: 0.1355652924008986\n","--- 8.282339334487915 seconds ---\n","Training loss per 100 training steps: 0.13471755139328856\n","--- 8.191790103912354 seconds ---\n","Training loss per 100 training steps: 0.1340004528639863\n","--- 8.13032865524292 seconds ---\n","Training loss per 100 training steps: 0.1332761176593129\n","--- 7.8132641315460205 seconds ---\n","Training loss per 100 training steps: 0.13275548249440686\n","--- 7.76975154876709 seconds ---\n","Training loss per 100 training steps: 0.1320447913227329\n","--- 7.807292222976685 seconds ---\n","Training loss per 100 training steps: 0.13139383948226724\n","--- 7.951839447021484 seconds ---\n","Training loss per 100 training steps: 0.1309388233044203\n","--- 8.164074182510376 seconds ---\n","Training loss per 100 training steps: 0.1301990291241526\n","--- 8.231062412261963 seconds ---\n","Training loss per 100 training steps: 0.12946757710354256\n","--- 8.115727663040161 seconds ---\n","Training loss per 100 training steps: 0.12876851095769012\n","--- 8.18034315109253 seconds ---\n","Training loss per 100 training steps: 0.12795588912079134\n","--- 8.15061616897583 seconds ---\n","Training loss per 100 training steps: 0.12727343623001475\n","--- 8.031152486801147 seconds ---\n","Training loss per 100 training steps: 0.1268553535324315\n","--- 8.308162689208984 seconds ---\n","Training loss per 100 training steps: 0.12627473458759744\n","--- 8.275075912475586 seconds ---\n","Training loss per 100 training steps: 0.1256769678167035\n","--- 8.404564619064331 seconds ---\n","Training loss per 100 training steps: 0.1251835414292945\n","--- 8.244357109069824 seconds ---\n","Training loss per 100 training steps: 0.12466792488461324\n","--- 8.163288831710815 seconds ---\n","Training loss per 100 training steps: 0.12390310981803207\n","--- 7.862358093261719 seconds ---\n","Training loss per 100 training steps: 0.1236101068510241\n","--- 7.726637840270996 seconds ---\n","Training loss per 100 training steps: 0.12329826927119067\n","--- 7.833481788635254 seconds ---\n","Training loss per 100 training steps: 0.12292695908661438\n","--- 7.758542776107788 seconds ---\n","Training loss per 100 training steps: 0.12225086865798808\n","--- 7.723457336425781 seconds ---\n","Training loss per 100 training steps: 0.12192276753947544\n","--- 7.811626195907593 seconds ---\n","Training loss per 100 training steps: 0.12154386948172559\n","--- 7.796241998672485 seconds ---\n","Training loss per 100 training steps: 0.12114378447751865\n","--- 7.76248836517334 seconds ---\n","Training loss per 100 training steps: 0.12072562592404518\n","--- 7.746009826660156 seconds ---\n","Training loss per 100 training steps: 0.12032520772286794\n","--- 7.734774827957153 seconds ---\n","Training loss per 100 training steps: 0.11997314040920015\n","--- 7.697752237319946 seconds ---\n","Training loss per 100 training steps: 0.1197812953268105\n","--- 7.953007936477661 seconds ---\n","Training loss per 100 training steps: 0.11958959081307258\n","--- 7.825929403305054 seconds ---\n","Training loss per 100 training steps: 0.1192279259361181\n","--- 7.819292306900024 seconds ---\n","Training loss per 100 training steps: 0.11874316412389213\n","--- 8.414333581924438 seconds ---\n","Training loss per 100 training steps: 0.11838200567079621\n","--- 7.930743932723999 seconds ---\n","Training loss per 100 training steps: 0.1181962624779442\n","--- 7.793033123016357 seconds ---\n","Training loss per 100 training steps: 0.11778505082050159\n","--- 7.750994920730591 seconds ---\n","Training loss per 100 training steps: 0.11739440397487184\n","--- 7.881996154785156 seconds ---\n","Training loss per 100 training steps: 0.11723437299271249\n","--- 8.184253454208374 seconds ---\n","Training loss per 100 training steps: 0.11691590970902926\n","--- 7.960120916366577 seconds ---\n","Training loss per 100 training steps: 0.11679871731490872\n","--- 7.750900983810425 seconds ---\n","Training loss per 100 training steps: 0.1165117620864798\n","--- 7.752598285675049 seconds ---\n","Training loss per 100 training steps: 0.11615317519417012\n","--- 8.138560056686401 seconds ---\n","Training loss per 100 training steps: 0.11582251365173371\n","--- 8.042587757110596 seconds ---\n","Training loss per 100 training steps: 0.1154730015679576\n","--- 7.764474630355835 seconds ---\n","Training loss per 100 training steps: 0.11519441381445393\n","--- 7.76343035697937 seconds ---\n","Training loss per 100 training steps: 0.11490720076900059\n","--- 7.76779317855835 seconds ---\n","Training loss per 100 training steps: 0.11468908368148355\n","--- 8.1970534324646 seconds ---\n","Training loss per 100 training steps: 0.11448434159843425\n","--- 8.077155113220215 seconds ---\n","Training loss per 100 training steps: 0.114196716206825\n","--- 8.144707918167114 seconds ---\n","Training loss per 100 training steps: 0.11396027536115418\n","--- 8.210967779159546 seconds ---\n","Training loss per 100 training steps: 0.11361640750081879\n","--- 8.207053422927856 seconds ---\n","Training loss per 100 training steps: 0.11344567295501075\n","--- 8.095835208892822 seconds ---\n","Training loss per 100 training steps: 0.11318840858106732\n","--- 8.149112462997437 seconds ---\n","Training loss per 100 training steps: 0.11297695781648869\n","--- 7.84677267074585 seconds ---\n","Training loss per 100 training steps: 0.11258408905390795\n","--- 7.879321575164795 seconds ---\n","Training loss per 100 training steps: 0.11233614617874273\n","--- 7.841614723205566 seconds ---\n","Training loss per 100 training steps: 0.11212801955522275\n","--- 7.760952949523926 seconds ---\n","Training loss epoch: 0.11207999630073388\n","Training accuracy epoch: 0.9658769460612739\n","Validation loss per 100 evaluation steps: 0.2438203990459442\n","Validation loss per 100 evaluation steps: 0.0756792142067455\n","Validation loss per 100 evaluation steps: 0.0800706113506785\n","Validation loss per 100 evaluation steps: 0.08149884184868851\n","Validation loss per 100 evaluation steps: 0.08357887814301031\n","Validation loss per 100 evaluation steps: 0.08371477677536099\n","Validation loss per 100 evaluation steps: 0.08222129374929238\n","Validation loss per 100 evaluation steps: 0.08504945993142471\n","Validation loss per 100 evaluation steps: 0.08698463213898022\n","Validation Loss: 0.08553302885219037\n","Validation Accuracy: 0.9723205750901739\n","Testing loss per 100 testing steps: 0.25866270065307617\n","Testing loss per 100 testing steps: 0.08844239468511084\n","Testing loss per 100 testing steps: 0.08256130948193549\n","Testing loss per 100 testing steps: 0.07526182160290773\n","Testing loss per 100 testing steps: 0.0760057354898328\n","Testing loss per 100 testing steps: 0.08207168538678172\n","Testing loss per 100 testing steps: 0.08123840073208029\n","Testing loss per 100 testing steps: 0.08329790422610524\n","Testing loss per 100 testing steps: 0.08449256521839942\n","Testing loss per 100 testing steps: 0.08232367709212761\n","Testing loss per 100 testing steps: 0.08219686332236446\n","Testing loss per 100 testing steps: 0.08151150275503273\n","Testing loss per 100 testing steps: 0.0815751773464745\n","Testing loss per 100 testing steps: 0.08049965037425137\n","Testing loss per 100 testing steps: 0.08104334009955161\n","Testing loss per 100 testing steps: 0.08193916914411466\n","Testing loss per 100 testing steps: 0.08202388628007139\n","Testing loss per 100 testing steps: 0.08240165318193275\n","Testing Loss: 0.08289338686375355\n","Testing Accuracy: 0.9740366543347438\n","              precision    recall  f1-score   support\n","\n","         geo       0.83      0.90      0.86      2734\n","         gpe       0.97      0.95      0.96      1153\n","         org       0.71      0.70      0.70      1465\n","         per       0.74      0.77      0.75      1191\n","         tim       0.89      0.82      0.85      1546\n","\n","   micro avg       0.82      0.84      0.83      8089\n","   macro avg       0.83      0.83      0.83      8089\n","weighted avg       0.83      0.84      0.83      8089\n","\n","All files saved\n","Training epoch: 2\n","Training loss per 100 training steps: 0.053520068526268005\n","Training loss per 100 training steps: 0.07712935020981154\n","--- 7.892699241638184 seconds ---\n","Training loss per 100 training steps: 0.07602883619024646\n","--- 9.642494916915894 seconds ---\n","Training loss per 100 training steps: 0.07428333679085715\n","--- 8.010614156723022 seconds ---\n","Training loss per 100 training steps: 0.0714790189982464\n","--- 8.03293776512146 seconds ---\n","Training loss per 100 training steps: 0.07224461887526268\n","--- 8.170142412185669 seconds ---\n","Training loss per 100 training steps: 0.07242401014973167\n","--- 7.909931421279907 seconds ---\n","Training loss per 100 training steps: 0.07299129770061974\n","--- 7.953641176223755 seconds ---\n","Training loss per 100 training steps: 0.07252490446500517\n","--- 7.960902690887451 seconds ---\n","Training loss per 100 training steps: 0.07118370064617456\n","--- 7.93052077293396 seconds ---\n","Training loss per 100 training steps: 0.07213377020822803\n","--- 8.086212635040283 seconds ---\n","Training loss per 100 training steps: 0.07320239415405391\n","--- 8.469247817993164 seconds ---\n","Training loss per 100 training steps: 0.0742500851644084\n","--- 8.367153406143188 seconds ---\n","Training loss per 100 training steps: 0.07449850198624604\n","--- 8.027602910995483 seconds ---\n","Training loss per 100 training steps: 0.07416489473871869\n","--- 8.127831220626831 seconds ---\n","Training loss per 100 training steps: 0.07381968331980603\n","--- 8.279327392578125 seconds ---\n","Training loss per 100 training steps: 0.07368981018507208\n","--- 8.218563318252563 seconds ---\n","Training loss per 100 training steps: 0.0738821395550933\n","--- 7.921526193618774 seconds ---\n","Training loss per 100 training steps: 0.07349977944487938\n","--- 7.837197303771973 seconds ---\n","Training loss per 100 training steps: 0.07356063343563042\n","--- 8.0799720287323 seconds ---\n","Training loss per 100 training steps: 0.07333657093059473\n","--- 7.947556018829346 seconds ---\n","Training loss per 100 training steps: 0.07384855941022274\n","--- 7.790470600128174 seconds ---\n","Training loss per 100 training steps: 0.07362100985446242\n","--- 7.875136613845825 seconds ---\n","Training loss per 100 training steps: 0.07325213626209497\n","--- 7.969706773757935 seconds ---\n","Training loss per 100 training steps: 0.07260646886937834\n","--- 7.824229001998901 seconds ---\n","Training loss per 100 training steps: 0.07217345088188983\n","--- 8.143985748291016 seconds ---\n","Training loss per 100 training steps: 0.0722046142252294\n","--- 7.859277248382568 seconds ---\n","Training loss per 100 training steps: 0.07230311293518596\n","--- 7.841159105300903 seconds ---\n","Training loss per 100 training steps: 0.07212224541151069\n","--- 7.7848124504089355 seconds ---\n","Training loss per 100 training steps: 0.07216039243772741\n","--- 7.763874769210815 seconds ---\n","Training loss per 100 training steps: 0.07232854437082889\n","--- 7.983639717102051 seconds ---\n","Training loss per 100 training steps: 0.072513759405943\n","--- 7.942976951599121 seconds ---\n","Training loss per 100 training steps: 0.07266826594364392\n","--- 7.990679740905762 seconds ---\n","Training loss per 100 training steps: 0.07259180559922199\n","--- 8.122517824172974 seconds ---\n","Training loss per 100 training steps: 0.07239334043615303\n","--- 8.171947002410889 seconds ---\n","Training loss per 100 training steps: 0.07204336137844188\n","--- 8.076011657714844 seconds ---\n","Training loss per 100 training steps: 0.0723635540176763\n","--- 8.163622617721558 seconds ---\n","Training loss per 100 training steps: 0.07230935636796622\n","--- 8.079580783843994 seconds ---\n","Training loss per 100 training steps: 0.07237411310456741\n","--- 7.921785116195679 seconds ---\n","Training loss per 100 training steps: 0.07235112832830062\n","--- 8.281649351119995 seconds ---\n","Training loss per 100 training steps: 0.07269885056043923\n","--- 7.973682403564453 seconds ---\n","Training loss per 100 training steps: 0.07253740110273608\n","--- 7.974518537521362 seconds ---\n","Training loss per 100 training steps: 0.07270048805559735\n","--- 7.74836802482605 seconds ---\n","Training loss per 100 training steps: 0.07267765523869743\n","--- 7.784522771835327 seconds ---\n","Training loss per 100 training steps: 0.07284536733343704\n","--- 7.879165172576904 seconds ---\n","Training loss per 100 training steps: 0.07287188720836445\n","--- 8.188414335250854 seconds ---\n","Training loss per 100 training steps: 0.07292809643076567\n","--- 8.156164169311523 seconds ---\n","Training loss per 100 training steps: 0.07274321772627447\n","--- 8.241400718688965 seconds ---\n","Training loss per 100 training steps: 0.07283184318154734\n","--- 8.245222091674805 seconds ---\n","Training loss per 100 training steps: 0.07252968741647045\n","--- 7.876307487487793 seconds ---\n","Training loss per 100 training steps: 0.07243558441611199\n","--- 7.712503433227539 seconds ---\n","Training loss per 100 training steps: 0.07269264209878123\n","--- 7.745866060256958 seconds ---\n","Training loss per 100 training steps: 0.07250909185816917\n","--- 7.880585432052612 seconds ---\n","Training loss per 100 training steps: 0.07244650926394287\n","--- 7.894508123397827 seconds ---\n","Training loss per 100 training steps: 0.07246026425730757\n","--- 7.731571912765503 seconds ---\n","Training loss per 100 training steps: 0.07267381661532347\n","--- 7.802306890487671 seconds ---\n","Training loss per 100 training steps: 0.07294443414982177\n","--- 7.817570924758911 seconds ---\n","Training loss per 100 training steps: 0.07305600469952032\n","--- 8.32706093788147 seconds ---\n","Training loss per 100 training steps: 0.07306147058941367\n","--- 7.781134843826294 seconds ---\n","Training loss per 100 training steps: 0.07296169241485195\n","--- 7.7629313468933105 seconds ---\n","Training loss per 100 training steps: 0.07301163592011516\n","--- 8.024465084075928 seconds ---\n","Training loss per 100 training steps: 0.0728805105546099\n","--- 8.2092924118042 seconds ---\n","Training loss per 100 training steps: 0.0728546290842978\n","--- 8.185250282287598 seconds ---\n","Training loss per 100 training steps: 0.0727315796536878\n","--- 8.19315242767334 seconds ---\n","Training loss per 100 training steps: 0.07269241985584093\n","--- 8.36799693107605 seconds ---\n","Training loss per 100 training steps: 0.07269966280377801\n","--- 8.494922161102295 seconds ---\n","Training loss per 100 training steps: 0.07273195405492759\n","--- 8.275209426879883 seconds ---\n","Training loss per 100 training steps: 0.07262089621850304\n","--- 8.169695377349854 seconds ---\n","Training loss per 100 training steps: 0.07253780205082125\n","--- 8.055941581726074 seconds ---\n","Training loss per 100 training steps: 0.07260802797785991\n","--- 8.088409662246704 seconds ---\n","Training loss per 100 training steps: 0.07272726512087986\n","--- 7.939905166625977 seconds ---\n","Training loss per 100 training steps: 0.07274816545458279\n","--- 7.885197877883911 seconds ---\n","Training loss per 100 training steps: 0.07265818697432029\n","--- 7.875776290893555 seconds ---\n","Training loss per 100 training steps: 0.07279320474209744\n","--- 8.194344997406006 seconds ---\n","Training loss per 100 training steps: 0.07281481143706472\n","--- 7.794925689697266 seconds ---\n","Training loss per 100 training steps: 0.07279530173826829\n","--- 7.868377447128296 seconds ---\n","Training loss per 100 training steps: 0.07270111557834051\n","--- 7.766787052154541 seconds ---\n","Training loss per 100 training steps: 0.07257230800234549\n","--- 7.974226713180542 seconds ---\n","Training loss per 100 training steps: 0.07257320885584634\n","--- 8.901777505874634 seconds ---\n","Training loss per 100 training steps: 0.07263994881484893\n","--- 8.253157615661621 seconds ---\n","Training loss per 100 training steps: 0.07254399996477712\n","--- 8.141705513000488 seconds ---\n","Training loss per 100 training steps: 0.07243912479402213\n","--- 8.039496183395386 seconds ---\n","Training loss per 100 training steps: 0.07247450080444143\n","--- 7.900563955307007 seconds ---\n","Training loss per 100 training steps: 0.07244432379295519\n","--- 7.827694654464722 seconds ---\n","Training loss per 100 training steps: 0.07243979851920362\n","--- 7.9254395961761475 seconds ---\n","Training loss per 100 training steps: 0.07244304057473225\n","--- 7.9899983406066895 seconds ---\n","Training loss per 100 training steps: 0.07238479054229265\n","--- 7.8851072788238525 seconds ---\n","Training loss per 100 training steps: 0.07239622681252936\n","--- 7.980098009109497 seconds ---\n","Training loss per 100 training steps: 0.07234733218441602\n","--- 7.793347120285034 seconds ---\n","Training loss per 100 training steps: 0.07231274078307605\n","--- 7.858712196350098 seconds ---\n","Training loss per 100 training steps: 0.07230772382774672\n","--- 7.861597537994385 seconds ---\n","Training loss per 100 training steps: 0.07245492150352432\n","--- 7.837859869003296 seconds ---\n","Training loss per 100 training steps: 0.07241047130479547\n","--- 7.800220489501953 seconds ---\n","Training loss per 100 training steps: 0.07238355894814594\n","--- 7.880042791366577 seconds ---\n","Training loss per 100 training steps: 0.07240950509938303\n","--- 7.775648832321167 seconds ---\n","Training loss per 100 training steps: 0.07233484689211656\n","--- 8.079894065856934 seconds ---\n","Training loss per 100 training steps: 0.07227098661117132\n","--- 8.207645893096924 seconds ---\n","Training loss per 100 training steps: 0.07218685135507356\n","--- 7.804570436477661 seconds ---\n","Training loss per 100 training steps: 0.07214942524795431\n","--- 7.826633930206299 seconds ---\n","Training loss per 100 training steps: 0.07226680157314197\n","--- 7.715671539306641 seconds ---\n","Training loss per 100 training steps: 0.07228805681528232\n","--- 7.842514753341675 seconds ---\n","Training loss per 100 training steps: 0.07242817496957848\n","--- 7.861180067062378 seconds ---\n","Training loss epoch: 0.0724190522471263\n","Training accuracy epoch: 0.9757790744155748\n","Validation loss per 100 evaluation steps: 0.0940323993563652\n","Validation loss per 100 evaluation steps: 0.08053058432184602\n","Validation loss per 100 evaluation steps: 0.0828161115273926\n","Validation loss per 100 evaluation steps: 0.07736928346986492\n","Validation loss per 100 evaluation steps: 0.07682748520992141\n","Validation loss per 100 evaluation steps: 0.07687502313446404\n","Validation loss per 100 evaluation steps: 0.0781820978502609\n","Validation loss per 100 evaluation steps: 0.0771792807107537\n","Validation loss per 100 evaluation steps: 0.07804115531066015\n","Validation Loss: 0.0795781166369924\n","Validation Accuracy: 0.9730686536024499\n","Testing loss per 100 testing steps: 0.013646737672388554\n","Testing loss per 100 testing steps: 0.08943033443426997\n","Testing loss per 100 testing steps: 0.07499761320742884\n","Testing loss per 100 testing steps: 0.07147373902588519\n","Testing loss per 100 testing steps: 0.06859069500866266\n","Testing loss per 100 testing steps: 0.07234579614190673\n","Testing loss per 100 testing steps: 0.07054061597271889\n","Testing loss per 100 testing steps: 0.07219189181445064\n","Testing loss per 100 testing steps: 0.0703621315696047\n","Testing loss per 100 testing steps: 0.07196639539010949\n","Testing loss per 100 testing steps: 0.07364414107982625\n","Testing loss per 100 testing steps: 0.07414589978999339\n","Testing loss per 100 testing steps: 0.07586952873157751\n","Testing loss per 100 testing steps: 0.07576972461646606\n","Testing loss per 100 testing steps: 0.07546870687108173\n","Testing loss per 100 testing steps: 0.07432648756721276\n","Testing loss per 100 testing steps: 0.07529986604917493\n","Testing loss per 100 testing steps: 0.07583684368885837\n","Testing Loss: 0.07529412712506442\n","Testing Accuracy: 0.974997938369078\n","              precision    recall  f1-score   support\n","\n","         geo       0.85      0.90      0.87      2734\n","         gpe       0.97      0.95      0.96      1153\n","         org       0.73      0.72      0.72      1465\n","         per       0.77      0.76      0.77      1191\n","         tim       0.88      0.84      0.86      1546\n","\n","   micro avg       0.84      0.84      0.84      8089\n","   macro avg       0.84      0.83      0.84      8089\n","weighted avg       0.84      0.84      0.84      8089\n","\n","All files saved\n","Training epoch: 3\n","Training loss per 100 training steps: 0.05082157626748085\n","Training loss per 100 training steps: 0.05155951034932946\n","--- 7.79033088684082 seconds ---\n","Training loss per 100 training steps: 0.05241838484979061\n","--- 9.722928285598755 seconds ---\n","Training loss per 100 training steps: 0.0518739216695938\n","--- 7.933616399765015 seconds ---\n","Training loss per 100 training steps: 0.0550065415761793\n","--- 7.884936809539795 seconds ---\n","Training loss per 100 training steps: 0.05439520048985686\n","--- 8.272432088851929 seconds ---\n","Training loss per 100 training steps: 0.053313345295349375\n","--- 8.255295753479004 seconds ---\n","Training loss per 100 training steps: 0.05292295642139755\n","--- 7.832762956619263 seconds ---\n","Training loss per 100 training steps: 0.054230600717067215\n","--- 7.958991765975952 seconds ---\n","Training loss per 100 training steps: 0.054516602155304\n","--- 7.811850070953369 seconds ---\n","Training loss per 100 training steps: 0.05480696771576311\n","--- 7.91670298576355 seconds ---\n","Training loss per 100 training steps: 0.05568234530114135\n","--- 7.880704164505005 seconds ---\n","Training loss per 100 training steps: 0.05582623011586952\n","--- 7.875182151794434 seconds ---\n","Training loss per 100 training steps: 0.05603725260772932\n","--- 8.234559774398804 seconds ---\n","Training loss per 100 training steps: 0.05604385252972225\n","--- 8.274803161621094 seconds ---\n","Training loss per 100 training steps: 0.05637102152972175\n","--- 8.295518159866333 seconds ---\n","Training loss per 100 training steps: 0.05622618476431975\n","--- 8.229109048843384 seconds ---\n","Training loss per 100 training steps: 0.056477600517632426\n","--- 8.149948596954346 seconds ---\n","Training loss per 100 training steps: 0.056401407351922714\n","--- 8.258331298828125 seconds ---\n","Training loss per 100 training steps: 0.0563008588420557\n","--- 8.289560317993164 seconds ---\n","Training loss per 100 training steps: 0.05610663850795621\n","--- 8.534116506576538 seconds ---\n","Training loss per 100 training steps: 0.05640408399261411\n","--- 8.194138526916504 seconds ---\n","Training loss per 100 training steps: 0.05627497187793921\n","--- 8.233522176742554 seconds ---\n","Training loss per 100 training steps: 0.0565077235524503\n","--- 8.112409591674805 seconds ---\n","Training loss per 100 training steps: 0.056487167507260905\n","--- 8.015567541122437 seconds ---\n","Training loss per 100 training steps: 0.05657405534279538\n","--- 7.830140829086304 seconds ---\n","Training loss per 100 training steps: 0.05629293104589244\n","--- 8.016938209533691 seconds ---\n","Training loss per 100 training steps: 0.05610332143959477\n","--- 8.050360679626465 seconds ---\n","Training loss per 100 training steps: 0.05649611683846807\n","--- 7.768516302108765 seconds ---\n","Training loss per 100 training steps: 0.05635089770111179\n","--- 8.304736375808716 seconds ---\n","Training loss per 100 training steps: 0.0562724184226891\n","--- 8.255517959594727 seconds ---\n","Training loss per 100 training steps: 0.05657949860179618\n","--- 8.121007919311523 seconds ---\n","Training loss per 100 training steps: 0.056584366971820864\n","--- 7.822565317153931 seconds ---\n","Training loss per 100 training steps: 0.0563937978988134\n","--- 8.207323789596558 seconds ---\n","Training loss per 100 training steps: 0.055959874749083635\n","--- 7.90046501159668 seconds ---\n","Training loss per 100 training steps: 0.05582004197441664\n","--- 8.312484979629517 seconds ---\n","Training loss per 100 training steps: 0.055647189884117265\n","--- 8.209748029708862 seconds ---\n","Training loss per 100 training steps: 0.055608826432394655\n","--- 8.23925256729126 seconds ---\n","Training loss per 100 training steps: 0.05545115433069627\n","--- 8.265764713287354 seconds ---\n","Training loss per 100 training steps: 0.05517048642148567\n","--- 8.184985160827637 seconds ---\n","Training loss per 100 training steps: 0.055349729693214625\n","--- 7.854429244995117 seconds ---\n","Training loss per 100 training steps: 0.05565671606107328\n","--- 7.871897459030151 seconds ---\n","Training loss per 100 training steps: 0.055396947807329254\n","--- 7.986480236053467 seconds ---\n","Training loss per 100 training steps: 0.05550569277107114\n","--- 8.280503034591675 seconds ---\n","Training loss per 100 training steps: 0.05560385492853148\n","--- 7.7978715896606445 seconds ---\n","Training loss per 100 training steps: 0.05571450425456156\n","--- 7.845396041870117 seconds ---\n","Training loss per 100 training steps: 0.05571808045311003\n","--- 7.762658596038818 seconds ---\n","Training loss per 100 training steps: 0.05567627996786812\n","--- 7.900174856185913 seconds ---\n","Training loss per 100 training steps: 0.05558520492190686\n","--- 7.892313003540039 seconds ---\n","Training loss per 100 training steps: 0.055744288912617644\n","--- 8.355033874511719 seconds ---\n","Training loss per 100 training steps: 0.05586027477444927\n","--- 7.836906433105469 seconds ---\n","Training loss per 100 training steps: 0.055955606325771325\n","--- 7.897329807281494 seconds ---\n","Training loss per 100 training steps: 0.05614498106259474\n","--- 7.92545223236084 seconds ---\n","Training loss per 100 training steps: 0.05598062837703992\n","--- 8.100483894348145 seconds ---\n","Training loss per 100 training steps: 0.05599249184567296\n","--- 8.217482089996338 seconds ---\n","Training loss per 100 training steps: 0.055919286765093494\n","--- 8.253076314926147 seconds ---\n","Training loss per 100 training steps: 0.05587415929611288\n","--- 8.16783094406128 seconds ---\n","Training loss per 100 training steps: 0.05584001731976709\n","--- 8.384172439575195 seconds ---\n","Training loss per 100 training steps: 0.05580945173857793\n","--- 8.35446047782898 seconds ---\n","Training loss per 100 training steps: 0.05590929034473929\n","--- 7.9848103523254395 seconds ---\n","Training loss per 100 training steps: 0.055821735562119223\n","--- 7.9790565967559814 seconds ---\n","Training loss per 100 training steps: 0.055905084239163676\n","--- 8.129302501678467 seconds ---\n","Training loss per 100 training steps: 0.055834486575292164\n","--- 8.345689058303833 seconds ---\n","Training loss per 100 training steps: 0.055803024690312515\n","--- 8.440609693527222 seconds ---\n","Training loss per 100 training steps: 0.05575265652413127\n","--- 8.117250919342041 seconds ---\n","Training loss per 100 training steps: 0.0556372298068267\n","--- 7.877084255218506 seconds ---\n","Training loss per 100 training steps: 0.05554456152296852\n","--- 7.967609167098999 seconds ---\n","Training loss per 100 training steps: 0.05553272160089917\n","--- 8.35148811340332 seconds ---\n","Training loss per 100 training steps: 0.055467064238875693\n","--- 8.519121170043945 seconds ---\n","Training loss per 100 training steps: 0.055464120955027715\n","--- 8.80309534072876 seconds ---\n","Training loss per 100 training steps: 0.055486331863368\n","--- 7.861235618591309 seconds ---\n","Training loss per 100 training steps: 0.05544632168753403\n","--- 8.229408264160156 seconds ---\n","Training loss per 100 training steps: 0.055383815838234325\n","--- 8.090388536453247 seconds ---\n","Training loss per 100 training steps: 0.05552515603319309\n","--- 7.9843456745147705 seconds ---\n","Training loss per 100 training steps: 0.05546967886885411\n","--- 8.178284168243408 seconds ---\n","Training loss per 100 training steps: 0.055490241193623356\n","--- 8.305155038833618 seconds ---\n","Training loss per 100 training steps: 0.055563290084281354\n","--- 8.547944068908691 seconds ---\n","Training loss per 100 training steps: 0.055602857334441494\n","--- 8.490727424621582 seconds ---\n","Training loss per 100 training steps: 0.05566825339497582\n","--- 8.652130603790283 seconds ---\n","Training loss per 100 training steps: 0.05566849334874057\n","--- 8.586822271347046 seconds ---\n","Training loss per 100 training steps: 0.055637737719581554\n","--- 8.24681830406189 seconds ---\n","Training loss per 100 training steps: 0.05559907473025872\n","--- 8.280851125717163 seconds ---\n","Training loss per 100 training steps: 0.055624090127294544\n","--- 8.58145785331726 seconds ---\n","Training loss per 100 training steps: 0.05570880343890796\n","--- 8.673714399337769 seconds ---\n","Training loss per 100 training steps: 0.055671999299669\n","--- 8.384167671203613 seconds ---\n","Training loss per 100 training steps: 0.055700626873803054\n","--- 8.452538251876831 seconds ---\n","Training loss per 100 training steps: 0.05571804197659454\n","--- 8.448963642120361 seconds ---\n","Training loss per 100 training steps: 0.05567650756403978\n","--- 8.59801173210144 seconds ---\n","Training loss per 100 training steps: 0.05579588242887516\n","--- 8.406741619110107 seconds ---\n","Training loss per 100 training steps: 0.05588881342708501\n","--- 8.545212745666504 seconds ---\n","Training loss per 100 training steps: 0.05586165439930979\n","--- 8.317739963531494 seconds ---\n","Training loss per 100 training steps: 0.055864221566423546\n","--- 8.523929595947266 seconds ---\n","Training loss per 100 training steps: 0.055843480315841555\n","--- 8.152761220932007 seconds ---\n","Training loss per 100 training steps: 0.05582006685736933\n","--- 8.484761714935303 seconds ---\n","Training loss per 100 training steps: 0.05575804568506886\n","--- 9.166231870651245 seconds ---\n","Training loss per 100 training steps: 0.0557247591587333\n","--- 8.444553136825562 seconds ---\n","Training loss per 100 training steps: 0.05574669313331563\n","--- 8.124971151351929 seconds ---\n","Training loss per 100 training steps: 0.05570082220671437\n","--- 8.0484299659729 seconds ---\n","Training loss per 100 training steps: 0.05568016056357752\n","--- 8.396624088287354 seconds ---\n","Training loss per 100 training steps: 0.05564005920224061\n","--- 8.090900897979736 seconds ---\n","Training loss per 100 training steps: 0.055617671892921126\n","--- 8.036086082458496 seconds ---\n","Training loss per 100 training steps: 0.055528055980759126\n","--- 8.156445980072021 seconds ---\n","Training loss epoch: 0.05553231853331729\n","Training accuracy epoch: 0.9807515944076052\n","Validation loss per 100 evaluation steps: 0.006504626478999853\n","Validation loss per 100 evaluation steps: 0.0690286189120604\n","Validation loss per 100 evaluation steps: 0.07176087685977688\n","Validation loss per 100 evaluation steps: 0.07203149452571142\n","Validation loss per 100 evaluation steps: 0.07635402540514301\n","Validation loss per 100 evaluation steps: 0.07951772840393376\n","Validation loss per 100 evaluation steps: 0.08153683177490692\n","Validation loss per 100 evaluation steps: 0.08496209862376278\n","Validation loss per 100 evaluation steps: 0.08432355659174386\n","Validation Loss: 0.08404696921777761\n","Validation Accuracy: 0.9735777929259798\n","Testing loss per 100 testing steps: 0.17935559153556824\n","Testing loss per 100 testing steps: 0.08448155778116398\n","Testing loss per 100 testing steps: 0.08001478590415614\n","Testing loss per 100 testing steps: 0.07674248386916117\n","Testing loss per 100 testing steps: 0.07416434412615652\n","Testing loss per 100 testing steps: 0.07074315183270559\n","Testing loss per 100 testing steps: 0.07407250931418806\n","Testing loss per 100 testing steps: 0.07619606376935584\n","Testing loss per 100 testing steps: 0.07616231248196728\n","Testing loss per 100 testing steps: 0.07665346280856253\n","Testing loss per 100 testing steps: 0.07632712863727699\n","Testing loss per 100 testing steps: 0.07581298420368318\n","Testing loss per 100 testing steps: 0.07668586248323107\n","Testing loss per 100 testing steps: 0.07753315596710239\n","Testing loss per 100 testing steps: 0.0782053788732344\n","Testing loss per 100 testing steps: 0.07854060869045876\n","Testing loss per 100 testing steps: 0.07885960544612099\n","Testing loss per 100 testing steps: 0.07844188759866247\n","Testing Loss: 0.07925376603729276\n","Testing Accuracy: 0.9762166761870449\n","              precision    recall  f1-score   support\n","\n","         geo       0.87      0.89      0.88      2734\n","         gpe       0.97      0.95      0.96      1153\n","         org       0.75      0.75      0.75      1465\n","         per       0.76      0.81      0.78      1191\n","         tim       0.85      0.87      0.86      1546\n","\n","   micro avg       0.84      0.86      0.85      8089\n","   macro avg       0.84      0.85      0.85      8089\n","weighted avg       0.84      0.86      0.85      8089\n","\n","All files saved\n"]}]},{"cell_type":"markdown","metadata":{"id":"kv3PQZypdQ_t"},"source":["### 2) Evaluate the Model"]},{"cell_type":"code","metadata":{"id":"jFIfRvYXdQRv"},"source":["def valid(model, testing_loader):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(testing_loader):\n","            \n","            ids = batch['input_ids'].to(device, dtype = torch.long)\n","            mask = batch['attention_mask'].to(device, dtype = torch.long)\n","            labels = batch['labels'].to(device, dtype = torch.long)\n","            pos_ids = batch['pos_ids'].to(device, dtype = torch.long)\n","\n","\n","            outputs = model(input_ids=ids, attention_mask=mask, labels=labels, pos_ids=pos_ids)\n","\n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Testing loss per 100 testing steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","    \n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    print(f\"Testing Loss: {eval_loss}\")\n","    print(f\"Testing Accuracy: {eval_accuracy}\")\n","\n","    return eval_labels, eval_preds\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7ArcZ0wusBi","executionInfo":{"elapsed":545,"status":"ok","timestamp":1638932341301,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"},"user_tz":-480},"outputId":"fc851eda-d067-4907-a806-54ee1aee7e33"},"source":["len(testing_loader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1727"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"ZJN32jfGd3sQ"},"source":["labels, predictions = valid(model, testing_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"l-j7ltuGjWAB","executionInfo":{"elapsed":781,"status":"error","timestamp":1636932261962,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"},"user_tz":-480},"outputId":"f0dedf22-d290-4782-b8c9-79d43614808b"},"source":["New_NerDict = dict((v,k) for k,v in dict(Ner).items())\n","New_NerDict\n","labels_value = [[New_NerDict[i.item()] for i in labels]]\n","pred_value = [[New_NerDict[i.item()] for i in predictions]]\n","print(labels_value)\n","print(pred_value)\n","from seqeval.metrics import classification_reportsZZ\n","\n","print(classification_report(labels_value, pred_value))"],"execution_count":null,"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-7a3a6784d7b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNew_NerDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mNew_NerDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlabels_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNew_NerDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpred_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNew_NerDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"tLT4WtJ_MS3F"},"source":["### 3) Save Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oq6073uHMWsu","executionInfo":{"elapsed":1193,"status":"ok","timestamp":1636928816475,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"},"user_tz":-480},"outputId":"7040f0b1-77ab-4cdf-f562-1200ca6c3d51"},"source":["import os\n","\n","directory = \"./model\"\n","\n","if not os.path.exists(directory):\n","    os.makedirs(directory)\n","\n","# save vocabulary of the tokenizer\n","tokenizer.save_vocabulary(directory)\n","# save the model weights and its configuration file\n","model.save_pretrained(directory)\n","print('All files saved')"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["All files saved\n"]}]},{"cell_type":"code","metadata":{"id":"qT66WzNhmGTT"},"source":["#torch.save(model, 'model.pth')\n","\n","#torch.save(model.state_dict(), 'model_weights.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"101dcJ0w5rKK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639045707935,"user_tz":-480,"elapsed":18471,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"ec02463f-4f37-4e56-de56-951ea0f5cae6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]}]}