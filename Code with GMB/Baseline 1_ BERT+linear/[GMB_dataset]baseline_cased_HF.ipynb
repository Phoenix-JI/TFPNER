{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[GMB_dataset]baseline_cased_HF.ipynb","provenance":[],"collapsed_sections":["5BMn0-NQnkOV","IXOdIATZoNVQ","tLT4WtJ_MS3F"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8518887e4b0c433eb2950187b7d1ac98":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_662b7e21b0b041c594c2b8d33ac0d1ab","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d83347df8613415cbea39a45642fa856","IPY_MODEL_4b0c4e99f4b84aeab97375e1e25bd9ff","IPY_MODEL_aefd0b43072e49b1b2815f964385bf03"]}},"662b7e21b0b041c594c2b8d33ac0d1ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d83347df8613415cbea39a45642fa856":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e52e059a6c6e46399ec0044fe8bc7946","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_360451b1477e4e59853ec4da30fb24ae"}},"4b0c4e99f4b84aeab97375e1e25bd9ff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6a8566a6f1744d9d96dcddaf30acb4e5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2095f04270ae4c15993a5bc89b6b2e28"}},"aefd0b43072e49b1b2815f964385bf03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bdc662abc3a2459b8cc05a6173140867","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 208k/208k [00:00&lt;00:00, 2.57MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a5157491afa443a8a0cb1c1f24a79bd8"}},"e52e059a6c6e46399ec0044fe8bc7946":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"360451b1477e4e59853ec4da30fb24ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6a8566a6f1744d9d96dcddaf30acb4e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2095f04270ae4c15993a5bc89b6b2e28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bdc662abc3a2459b8cc05a6173140867":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a5157491afa443a8a0cb1c1f24a79bd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8be5f974a39f4ad3930920a73a95d888":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d2eab59d3c2a4de8bcfcf0d0df0dbf26","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6574b0888160408890caf00fb92518a0","IPY_MODEL_e71719fa2cfe43369a09d18bc0259c32","IPY_MODEL_b427fb72350e443d9017a98a0c36337c"]}},"d2eab59d3c2a4de8bcfcf0d0df0dbf26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6574b0888160408890caf00fb92518a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3ca692bfc4bc48299eae2e60af5d3e49","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4f1f73536dca4779b3cb5aeadb5ac45e"}},"e71719fa2cfe43369a09d18bc0259c32":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_52d077f8260249c182f16841b4260823","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":435797,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":435797,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e5c4d2ab42ae4a27bc33442678b42f02"}},"b427fb72350e443d9017a98a0c36337c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a1197ef718374ddd823010048ba9caf2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 426k/426k [00:00&lt;00:00, 3.38MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0052ba25980a4d9bb30cc1866edddd9a"}},"3ca692bfc4bc48299eae2e60af5d3e49":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4f1f73536dca4779b3cb5aeadb5ac45e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"52d077f8260249c182f16841b4260823":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e5c4d2ab42ae4a27bc33442678b42f02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a1197ef718374ddd823010048ba9caf2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0052ba25980a4d9bb30cc1866edddd9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e8001e81d7ab4bb687de3538ffc83a3b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e7aa45b984cb4b6fb5ab78573c60c234","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e0500070e34246f79242c91fd969e726","IPY_MODEL_3ca7fec8949243f385fccafefef17bb7","IPY_MODEL_08e40d6574224e38b841ec660f4a5525"]}},"e7aa45b984cb4b6fb5ab78573c60c234":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e0500070e34246f79242c91fd969e726":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b3f84bb515a84935be3f71b7d71631f2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d247920a14c14855b6a0cde0f7f60fbf"}},"3ca7fec8949243f385fccafefef17bb7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9804c60f83ed45f391b4f27ff442c4d6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":29,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":29,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cc542cf6b0fd41ccafc34fa725764b8d"}},"08e40d6574224e38b841ec660f4a5525":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_89c32c25b4904f8faeaf6588e4e718a6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29.0/29.0 [00:00&lt;00:00, 1.11kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dcd6df1d76a64c2ea456f923e4c3fbf2"}},"b3f84bb515a84935be3f71b7d71631f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d247920a14c14855b6a0cde0f7f60fbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9804c60f83ed45f391b4f27ff442c4d6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cc542cf6b0fd41ccafc34fa725764b8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"89c32c25b4904f8faeaf6588e4e718a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dcd6df1d76a64c2ea456f923e4c3fbf2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3dcb510759c44ca2bbfdba9aa29159f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ac2d955be78d47a896b31dd94a67b0d4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_95432c93ef9e488a9ea66a5e4c5bc634","IPY_MODEL_9c678a33c68543f7a74b1fb8657f791b","IPY_MODEL_53544de238ef4f8d9ad7fdb9cdc23ac5"]}},"ac2d955be78d47a896b31dd94a67b0d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"95432c93ef9e488a9ea66a5e4c5bc634":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ca8451ec87ad40e580e5f3b0024455c5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_703e81fef6c94e9d8b8c45e733723521"}},"9c678a33c68543f7a74b1fb8657f791b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a2612a0bf7234b23baef4ed5ad7f60a5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_48515c5cd2174f108f4b095ca28bd733"}},"53544de238ef4f8d9ad7fdb9cdc23ac5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1708c3782ff945e2bfe2c5f3706fd2f8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [00:00&lt;00:00, 23.5kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ab6ad3592ec44314804f484f0733d32f"}},"ca8451ec87ad40e580e5f3b0024455c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"703e81fef6c94e9d8b8c45e733723521":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a2612a0bf7234b23baef4ed5ad7f60a5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"48515c5cd2174f108f4b095ca28bd733":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1708c3782ff945e2bfe2c5f3706fd2f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ab6ad3592ec44314804f484f0733d32f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"444133d9dac546d3a60347b90d18d00d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7a0c2dc1765e44469fa0df9afdc0a367","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cfa3f4ec421d4c54a4d0ecbb3cee193f","IPY_MODEL_13d08b83c4684e2b9259ba8af7676f83","IPY_MODEL_fbd44ad9ad0147a7af4f3a5c143aa00f"]}},"7a0c2dc1765e44469fa0df9afdc0a367":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cfa3f4ec421d4c54a4d0ecbb3cee193f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ff0e074a98e84d528e401d79f994505e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_13ec89324cd64a9abd5d50b080f97ac1"}},"13d08b83c4684e2b9259ba8af7676f83":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9316c59b7a8c49248618f97a7f7d8986","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":435779157,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":435779157,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_89578b54ed504082aa3d61fdb6606b20"}},"fbd44ad9ad0147a7af4f3a5c143aa00f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bd6e53a5b5394d2d91a44f8c7acef462","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 416M/416M [00:10&lt;00:00, 16.6MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_94a64d10004e42d2ae7d7bf93d721b18"}},"ff0e074a98e84d528e401d79f994505e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"13ec89324cd64a9abd5d50b080f97ac1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9316c59b7a8c49248618f97a7f7d8986":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"89578b54ed504082aa3d61fdb6606b20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bd6e53a5b5394d2d91a44f8c7acef462":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"94a64d10004e42d2ae7d7bf93d721b18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DR6WnnwSiiPx"},"source":["## **1. Find the corresponding positive values for NER, POS, Chunk tags**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PflZFMcoyNNm","executionInfo":{"status":"ok","timestamp":1639183365977,"user_tz":-480,"elapsed":18771,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"a7a6da93-ef71-4516-d8e8-1e1ffe25ff63"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"wNF30bg7jgaO"},"source":["# **2. Data Preprocessing for BERT Model (Apply Hugging Face Data)**"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"9aEbqlC1M1Pq"}},{"cell_type":"markdown","metadata":{"id":"5BMn0-NQnkOV"},"source":["### (1) Hugging Face Dataset Conll2003 Exploration"]},{"cell_type":"code","metadata":{"id":"ZIPMjy9KBFAX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639183476925,"user_tz":-480,"elapsed":7612,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"1982a5be-ce88-4570-d6a8-cd62bc0cbc27"},"source":["!pip install datasets"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n","\u001b[K     |████████████████████████████████| 298 kB 5.1 MB/s \n","\u001b[?25hCollecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 67.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 66.3 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Collecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 654 kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 79.1 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 77.5 MB/s \n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n","\u001b[K     |████████████████████████████████| 192 kB 78.0 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 79.9 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, huggingface-hub, datasets\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 datasets-1.16.1 frozenlist-1.2.0 fsspec-2021.11.1 huggingface-hub-0.2.1 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2\n"]}]},{"cell_type":"code","metadata":{"id":"htUmCUrUBS0D"},"source":["from datasets import load_dataset\n","# dataset = load_dataset('conll2003')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IXOdIATZoNVQ"},"source":["### (2) Covert Data to BERT Input Style"]},{"cell_type":"code","metadata":{"id":"IsEmtyyeniqn","colab":{"base_uri":"https://localhost:8080/","height":940},"executionInfo":{"status":"ok","timestamp":1639183491729,"user_tz":-480,"elapsed":9126,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"ba3a8797-22c1-40ea-fc71-63160f47f559"},"source":["!pip install transformers seqeval[gpu]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 5.0 MB/s \n","\u001b[?25hCollecting seqeval[gpu]\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.6 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 74.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 75.5 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 71.7 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval[gpu]) (1.0.1)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.0.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=c70b26576999fea0c696f037a028de4b98726647c3f76aec4f3442813f14abf3\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: pyyaml, tokenizers, seqeval, sacremoses, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed pyyaml-6.0 sacremoses-0.0.46 seqeval-1.2.2 tokenizers-0.10.3 transformers-4.13.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["yaml"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"mUG0I8A-ndPo"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-3QG7H7OW4sT","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["8518887e4b0c433eb2950187b7d1ac98","662b7e21b0b041c594c2b8d33ac0d1ab","d83347df8613415cbea39a45642fa856","4b0c4e99f4b84aeab97375e1e25bd9ff","aefd0b43072e49b1b2815f964385bf03","e52e059a6c6e46399ec0044fe8bc7946","360451b1477e4e59853ec4da30fb24ae","6a8566a6f1744d9d96dcddaf30acb4e5","2095f04270ae4c15993a5bc89b6b2e28","bdc662abc3a2459b8cc05a6173140867","a5157491afa443a8a0cb1c1f24a79bd8","8be5f974a39f4ad3930920a73a95d888","d2eab59d3c2a4de8bcfcf0d0df0dbf26","6574b0888160408890caf00fb92518a0","e71719fa2cfe43369a09d18bc0259c32","b427fb72350e443d9017a98a0c36337c","3ca692bfc4bc48299eae2e60af5d3e49","4f1f73536dca4779b3cb5aeadb5ac45e","52d077f8260249c182f16841b4260823","e5c4d2ab42ae4a27bc33442678b42f02","a1197ef718374ddd823010048ba9caf2","0052ba25980a4d9bb30cc1866edddd9a","e8001e81d7ab4bb687de3538ffc83a3b","e7aa45b984cb4b6fb5ab78573c60c234","e0500070e34246f79242c91fd969e726","3ca7fec8949243f385fccafefef17bb7","08e40d6574224e38b841ec660f4a5525","b3f84bb515a84935be3f71b7d71631f2","d247920a14c14855b6a0cde0f7f60fbf","9804c60f83ed45f391b4f27ff442c4d6","cc542cf6b0fd41ccafc34fa725764b8d","89c32c25b4904f8faeaf6588e4e718a6","dcd6df1d76a64c2ea456f923e4c3fbf2","3dcb510759c44ca2bbfdba9aa29159f5","ac2d955be78d47a896b31dd94a67b0d4","95432c93ef9e488a9ea66a5e4c5bc634","9c678a33c68543f7a74b1fb8657f791b","53544de238ef4f8d9ad7fdb9cdc23ac5","ca8451ec87ad40e580e5f3b0024455c5","703e81fef6c94e9d8b8c45e733723521","a2612a0bf7234b23baef4ed5ad7f60a5","48515c5cd2174f108f4b095ca28bd733","1708c3782ff945e2bfe2c5f3706fd2f8","ab6ad3592ec44314804f484f0733d32f"]},"executionInfo":{"status":"ok","timestamp":1639183495532,"user_tz":-480,"elapsed":2020,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"467744d9-84ed-4b90-cc5b-6b30c891bb59"},"source":["MAX_LEN = 128     \n","TRAIN_BATCH_SIZE = 4\n","TEST_BATCH_SIZE = 2\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8518887e4b0c433eb2950187b7d1ac98","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8be5f974a39f4ad3930920a73a95d888","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8001e81d7ab4bb687de3538ffc83a3b","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3dcb510759c44ca2bbfdba9aa29159f5","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Pv2f46Mye6U","executionInfo":{"status":"ok","timestamp":1639183497218,"user_tz":-480,"elapsed":1692,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"a84bac7b-7b08-46e8-e5d4-b06698219e4b"},"source":["data = pd.read_csv(\"/content/drive/MyDrive/NLP/Final Project/New Dataset/Bank/ner_datasetreference_new.csv\", encoding='unicode_escape')\n","data.head()\n","\n","data.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sentence #      47959\n","Word          1048575\n","POS           1048575\n","Tag           1048575\n","dtype: int64"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUN2j6x8ye3Y","executionInfo":{"status":"ok","timestamp":1639183497219,"user_tz":-480,"elapsed":7,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"9638dcbf-fde8-409d-f719-9c76da80698d"},"source":["'''\n","step 2a: process NE tags and POS tags\n","'''\n","# NE \n","\"\"\"There are 8 category tags, each with a \"beginning\" and \"inside\" variant, and the \"outside\" tag. It is not really clear what these tags mean - \"geo\" probably stands for geographical entity, \"gpe\" for geopolitical entity, and so on. They do not seem to correspond with what the publisher says on Kaggle. Some tags seem to be underrepresented. Let's print them by frequency (highest to lowest): \"\"\"\n","\n","# tags = {}\n","# for tag, count in zip(frequencies_NE.index, frequencies_NE):\n","#     if tag != \"O\":\n","#         if tag[2:5] not in tags.keys():\n","#             tags[tag[2:5]] = count\n","#         else:\n","#             tags[tag[2:5]] += count\n","#     continue\n","\n","# print(sorted(tags.items(), key=lambda x: x[1], reverse=True))\n","\n","\"\"\"Let's remove \"art\", \"eve\" and \"nat\" named entities, as performance on them will probably be not comparable to the other named entities. \"\"\"\n","\n","entities_to_remove = [\"B-art\", \"I-art\", \"B-eve\", \"I-eve\", \"B-nat\", \"I-nat\"]\n","data = data[~data.Tag.isin(entities_to_remove)]\n","data.head()\n","data.count()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sentence #      47920\n","Word          1047063\n","POS           1047063\n","Tag           1047063\n","dtype: int64"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BL-8U-2Aye07","executionInfo":{"status":"ok","timestamp":1639183497820,"user_tz":-480,"elapsed":8,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"2ad461d8-7e92-4314-fb6a-58a776c76f50"},"source":["\"\"\"We create 2 dictionaries for NE: one that maps individual tags to indices, and one that maps indices to their individual tags. This is necessary in order to create the labels (as computers work with numbers = indices, rather than words = tags) - see further in this notebook.\"\"\"\n","\n","labels_to_ids = {k: v for v, k in enumerate(data.Tag.unique())}\n","ids_to_labels = {v: k for v, k in enumerate(data.Tag.unique())}\n","print(labels_to_ids)\n","print(ids_to_labels)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'I-per': 8, 'I-gpe': 9, 'I-tim': 10}\n","{0: 'O', 1: 'B-geo', 2: 'B-gpe', 3: 'B-per', 4: 'I-geo', 5: 'B-org', 6: 'I-org', 7: 'B-tim', 8: 'I-per', 9: 'I-gpe', 10: 'I-tim'}\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lb8mpgKVyeyO","executionInfo":{"status":"ok","timestamp":1639183497821,"user_tz":-480,"elapsed":7,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"6b3e9443-a680-4359-d211-e9c54b66d591"},"source":["# count NE tag\n","print(\"Number of NE tags: {}\".format(len(data.Tag.unique()))) # 17个\n","frequencies_NE = data.Tag.value_counts()\n","frequencies_NE\n","Ner_Tag = list(data.Tag.unique())\n","Ner_Number = [i for i in range(len(Ner_Tag))]\n","Ner = list(zip(Ner_Tag,Ner_Number))\n","print(Ner)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of NE tags: 11\n","[('O', 0), ('B-geo', 1), ('B-gpe', 2), ('B-per', 3), ('I-geo', 4), ('B-org', 5), ('I-org', 6), ('B-tim', 7), ('I-per', 8), ('I-gpe', 9), ('I-tim', 10)]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"gtIkj9n6yevj","executionInfo":{"status":"ok","timestamp":1639183497821,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"8d69ad40-fa45-4073-876c-885ec021b4c0"},"source":["# pandas has a very handy \"forward fill\" function to fill missing values based on the last upper non-nan value\n","data = data.fillna(method='ffill')\n","data.head()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>Thousands</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sentence: 1</td>\n","      <td>of</td>\n","      <td>IN</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sentence: 1</td>\n","      <td>demonstrators</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentence: 1</td>\n","      <td>have</td>\n","      <td>VBP</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sentence: 1</td>\n","      <td>marched</td>\n","      <td>VBN</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Sentence #           Word  POS Tag\n","0  Sentence: 1      Thousands  NNS   O\n","1  Sentence: 1             of   IN   O\n","2  Sentence: 1  demonstrators  NNS   O\n","3  Sentence: 1           have  VBP   O\n","4  Sentence: 1        marched  VBN   O"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"P2yH1GS8yejT","executionInfo":{"status":"ok","timestamp":1639183595033,"user_tz":-480,"elapsed":97217,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"8997b3f2-33b2-4d81-e6f0-bbb9866a8677"},"source":["# let's create a new column called \"sentence\" which groups the words by sentence \n","data['sentence'] = data[['Sentence #','Word','Tag', 'POS']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n","# let's also create a new column called \"word_labels\" which groups the tags by sentence \n","data['word_labels'] = data[['Sentence #','Word','Tag', 'POS']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n","data.head()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>Thousands</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","      <td>Thousands of demonstrators have marched throug...</td>\n","      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sentence: 1</td>\n","      <td>of</td>\n","      <td>IN</td>\n","      <td>O</td>\n","      <td>Thousands of demonstrators have marched throug...</td>\n","      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sentence: 1</td>\n","      <td>demonstrators</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","      <td>Thousands of demonstrators have marched throug...</td>\n","      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentence: 1</td>\n","      <td>have</td>\n","      <td>VBP</td>\n","      <td>O</td>\n","      <td>Thousands of demonstrators have marched throug...</td>\n","      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sentence: 1</td>\n","      <td>marched</td>\n","      <td>VBN</td>\n","      <td>O</td>\n","      <td>Thousands of demonstrators have marched throug...</td>\n","      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Sentence #  ...                                        word_labels\n","0  Sentence: 1  ...  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...\n","1  Sentence: 1  ...  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...\n","2  Sentence: 1  ...  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...\n","3  Sentence: 1  ...  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...\n","4  Sentence: 1  ...  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...\n","\n","[5 rows x 6 columns]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9xb50hv6y2wo","executionInfo":{"status":"ok","timestamp":1639183595703,"user_tz":-480,"elapsed":676,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"2ae747e2-911b-4e1a-dbea-c88fc0d63840"},"source":["\"\"\"Let's only keep the \"sentence\" and \"word_labels\" columns, and drop duplicates:\"\"\"\n","\n","data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n","data.head()\n","\n","len(data)\n","\"\"\"Let's verify that a random sentence and its corresponding tags are correct:\"\"\"\n","\n","print(data.iloc[41].sentence)\n","print(data.iloc[41].word_labels)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bedfordshire police said Tuesday that Omar Khayam was arrested in Bedford for breaching the conditions of his parole .\n","B-gpe,O,O,B-tim,O,B-per,I-per,O,O,O,B-geo,O,O,O,O,O,O,O,O\n"]}]},{"cell_type":"code","metadata":{"id":"rOEiVHh5y2tj"},"source":["train_df, validate_df, test_df = \\\n","              np.split(data.sample(frac=1, random_state=42), \n","                       [int(.85*len(data)), int(.925*len(data))])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgA6n9eBy2k7"},"source":["train_df = train_df.reset_index(drop=True)\n","validate_df = validate_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","data_combine_dict = {'train':train_df, 'validation':validate_df, 'test':test_df}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"blADhhnVBS44"},"source":["class Preprocess_Data(Dataset):\n","  def __init__(self, dataset, tokenizer, max_len): #usage -> train, validation, test\n","\n","        self.len = len(dataset)\n","        self.data = dataset\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","  def __getitem__(self, index):\n","\n","        # step 1: get the sentence and word labels \n","        sentence = self.data.sentence[index].strip().split()\n","        word_labels = self.data.word_labels[index].split(\",\")\n","\n","\n","        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n","        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n","        encoding = self.tokenizer(sentence,\n","                              is_split_into_words=True,\n","                              return_offsets_mapping=True,  #Set to True to return (char_start, char_end) for each token (default False)\n","                              padding='max_length', \n","                              truncation=True, \n","                              max_length=self.max_len)\n","        \n","        \n","        # step 3: create token labels only for first word pieces of each tokenized word\n","\n","        labels = [labels_to_ids[label] for label in word_labels]\n","\n","        # create an empty array of -100 of length max_length\n","        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n","        \n","        # set only labels whose first offset position is 0 and the second is not 0\n","        i = 0\n","        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n","          if mapping[0] == 0 and mapping[1] != 0:\n","            # overwrite label\n","            encoded_labels[idx] = labels[i]\n","            i += 1\n","\n","        # step 4: turn everything into PyTorch tensors\n","        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n","        item['labels'] = torch.as_tensor(encoded_labels)\n","        \n","        return item\n","\n","  def __len__(self):\n","        return self.len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQ54suiDDxHD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639183595704,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"bd39d92c-38b3-4800-b644-558cedad3531"},"source":["dataset = data_combine_dict\n","\n","training_set = Preprocess_Data(train_df, tokenizer, MAX_LEN)\n","validation_set = Preprocess_Data(validate_df, tokenizer, MAX_LEN)\n","testing_set = Preprocess_Data(test_df, tokenizer, MAX_LEN)\n","print(len(training_set),len(validation_set),len(testing_set))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["40435 3568 3568\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09bOO2Ddzzzj","executionInfo":{"status":"ok","timestamp":1639183595704,"user_tz":-480,"elapsed":7,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"bfc1a65d-1e65-4583-8942-975b205239fa"},"source":["for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"input_ids\"]), training_set[0][\"labels\"]):\n","  print('{0:10}  {1}'.format(token, label))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS]       -100\n","The         0\n","Tibetan     2\n","spiritual   0\n","leader      0\n","who         0\n","teaches     0\n","age         0\n","-           -100\n","old         -100\n","principles  0\n","of          0\n","peace       0\n","and         0\n","tolerance   0\n","has         0\n","gone        0\n","high        0\n","-           -100\n","tech        -100\n",",           0\n","joining     0\n","the         0\n","online      0\n","mess        0\n","##aging     -100\n","service     0\n",".           0\n","[SEP]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n","[PAD]       -100\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5pezZRhCzzwj","executionInfo":{"status":"ok","timestamp":1639183595705,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"d7d76bff-14ed-4976-8ec5-045e1a3e7da1"},"source":["# Define the Dataloader\n","training_loader = DataLoader(training_set, batch_size = TRAIN_BATCH_SIZE, shuffle=True,num_workers=0)\n","validation_loader = DataLoader(validation_set,batch_size = TRAIN_BATCH_SIZE, shuffle=True,num_workers=0)\n","testing_loader = DataLoader(testing_set,batch_size = TEST_BATCH_SIZE, shuffle=True,num_workers=0)\n","\n","print(len(training_loader),len(validation_loader),len(testing_loader))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10109 892 1784\n"]}]},{"cell_type":"code","metadata":{"id":"niW9J7uhIBxY"},"source":["# Define the Dataloader\n","training_loader = DataLoader(training_set, batch_size = TRAIN_BATCH_SIZE, shuffle=True,num_workers=0)\n","validation_loader = DataLoader(validation_set,batch_size = TRAIN_BATCH_SIZE, shuffle=True,num_workers=0)\n","testing_loader = DataLoader(testing_set,batch_size = TEST_BATCH_SIZE, shuffle=True,num_workers=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6X7rxuYMy6_z"},"source":[""]},{"cell_type":"code","metadata":{"id":"GlaCRzzmIBzz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639183595705,"user_tz":-480,"elapsed":5,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"48b7df10-b403-492a-b1de-ba550cd45aa1"},"source":["print(len(training_loader),len(validation_loader),len(testing_loader))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10109 892 1784\n"]}]},{"cell_type":"markdown","metadata":{"id":"6hF-6klfKomd"},"source":["# **3. Define the Model**"]},{"cell_type":"markdown","metadata":{"id":"IKXls9ALdeJt"},"source":["### 1) Train the Model"]},{"cell_type":"code","metadata":{"id":"ndNwQePxFFdY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639183596343,"user_tz":-480,"elapsed":643,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"fc8429a8-07e7-45a1-801f-6b8f49fc8009"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Dec 11 00:46:35 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0    42W / 400W |      0MiB / 40536MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6LvvmRHQMH2g","executionInfo":{"status":"ok","timestamp":1639183596343,"user_tz":-480,"elapsed":7,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"019bf6a8-a330-4c02-dedd-4a52b01b85c1"},"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","metadata":{"id":"VCEY_Fb2a7Hm"},"source":["# EPOCHS = 2\n","LEARNING_RATE = 1e-05\n","MAX_GRAD_NORM = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K8vCJCAFLWgK","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["444133d9dac546d3a60347b90d18d00d","7a0c2dc1765e44469fa0df9afdc0a367","cfa3f4ec421d4c54a4d0ecbb3cee193f","13d08b83c4684e2b9259ba8af7676f83","fbd44ad9ad0147a7af4f3a5c143aa00f","ff0e074a98e84d528e401d79f994505e","13ec89324cd64a9abd5d50b080f97ac1","9316c59b7a8c49248618f97a7f7d8986","89578b54ed504082aa3d61fdb6606b20","bd6e53a5b5394d2d91a44f8c7acef462","94a64d10004e42d2ae7d7bf93d721b18"]},"executionInfo":{"status":"ok","timestamp":1639183716190,"user_tz":-480,"elapsed":119851,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"c6f17575-5df9-4405-c385-290deac4000d"},"source":["# Define the model by just BertForTokenClassification\n","model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(Ner_Tag))\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"444133d9dac546d3a60347b90d18d00d","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",")"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"blREmCNMPgpY"},"source":["optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3800AVERAin"},"source":["import time \n","def train(epoch):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    # put model in training mode\n","    model.train()\n","    \n","    for idx, batch in enumerate(training_loader):\n","\n","        # if idx >200:\n","        #   break\n","        \n","        ids = batch['input_ids'].to(device, dtype = torch.long)\n","        mask = batch['attention_mask'].to(device, dtype = torch.long)\n","        labels = batch['labels'].to(device, dtype = torch.long)\n","\n","        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","        loss = outputs[0]\n","        tr_logits = outputs[1]\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 100==0:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","            if idx != 0:\n","              time_spent = time.time() - start_time\n","              print(\"--- %s seconds ---\" % (time_spent))\n","            start_time = time.time()\n","\n","           \n","        # compute training accuracy\n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","\n","        #print('flattened_targets', flattened_targets)\n","        #print('active_logits, ', active_logits)\n","        #print('flattened_predictions, ', flattened_predictions)\n","        # print('Logits 0, ', active_logits[0])\n","        # # print(\"real labels\",flattened_targets)\n","        # # print(\"real prediction\",flattened_predictions)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","        #print('active_accuracy ', active_accuracy)\n","\n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        #print('labels ', labels)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        #print('predictions ',predictions)\n","\n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","\n","        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","        tr_accuracy += tmp_tr_accuracy\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","    print(f\"Training loss epoch: {epoch_loss}\")\n","    print(f\"Training accuracy epoch: {tr_accuracy}\")\n","\n","    # --------------------------------------------------------------------------------------------------------------------\n","    # After the completion of each training epoch\n","    # measure our performance on validation set.\n","\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(validation_loader):\n","\n","            # if idx >200:\n","            #   break\n","            \n","            ids = batch['input_ids'].to(device, dtype = torch.long)\n","            mask = batch['attention_mask'].to(device, dtype = torch.long)\n","            labels = batch['labels'].to(device, dtype = torch.long)\n","\n","            \n","            outputs= model(input_ids=ids, attention_mask=mask, labels=labels)\n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    print(f\"Validation Loss: {eval_loss}\")\n","    print(f\"Validation Accuracy: {eval_accuracy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bYynGVa0aUYt","executionInfo":{"status":"ok","timestamp":1639112785259,"user_tz":-480,"elapsed":242,"user":{"displayName":"小奶狗喵喵喵","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgmHTG3dLKL5_0225w0WN8SnieInLh9QpUxa-Dd=s64","userId":"07360018702871445192"}},"outputId":"bc60f86f-ad12-4e4c-b9fc-b1032667056a"},"source":["len(training_loader),len(validation_loader),len(testing_loader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10109, 892, 1784)"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"6XTItahi_1kf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639183717633,"user_tz":-480,"elapsed":20,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"b7e8c427-36df-44c1-c56e-cea606020053"},"source":["from seqeval.metrics import classification_report\n","New_NerDict = dict((v,k) for k,v in dict(Ner).items())\n","New_NerDict"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'O',\n"," 1: 'B-geo',\n"," 2: 'B-gpe',\n"," 3: 'B-per',\n"," 4: 'I-geo',\n"," 5: 'B-org',\n"," 6: 'I-org',\n"," 7: 'B-tim',\n"," 8: 'I-per',\n"," 9: 'I-gpe',\n"," 10: 'I-tim'}"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["import os\n","import time"],"metadata":{"id":"-uLiqXs99z4m"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t3ehY9wnXJeC","outputId":"83093d4f-c6fa-4f71-82cf-bff5bb138a97","executionInfo":{"status":"ok","timestamp":1639185837752,"user_tz":-480,"elapsed":2065909,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}}},"source":["EPOCHS = 3\n","for epoch in range(EPOCHS):\n","\n","    directory = \"/content/drive/MyDrive/NLP/Final Project/New Dataset/Bank/Baseline/Cased/Model_\"+str(epoch+1)\n","\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train(epoch)\n","    labels, predictions = valid(model, testing_loader)\n","\n","    labels_value = [[New_NerDict[i.item()] for i in labels]]\n","    pred_value = [[New_NerDict[i.item()] for i in predictions]]\n","\n","    print(classification_report(labels_value, pred_value))\n","\n","    tokenizer.save_vocabulary(directory)\n","    # save the model weights and its configuration file\n","    model.save_pretrained(directory)\n","    print('All files saved')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training epoch: 1\n","Training loss per 100 training steps: 2.4574403762817383\n","Training loss per 100 training steps: 0.8305661631102609\n","--- 6.259000062942505 seconds ---\n","Training loss per 100 training steps: 0.5829148917871328\n","--- 6.2004570960998535 seconds ---\n","Training loss per 100 training steps: 0.46191439074634316\n","--- 6.196728467941284 seconds ---\n","Training loss per 100 training steps: 0.39226172004647536\n","--- 6.3908796310424805 seconds ---\n","Training loss per 100 training steps: 0.3509406074546947\n","--- 6.208604097366333 seconds ---\n","Training loss per 100 training steps: 0.3137690831989223\n","--- 6.217293739318848 seconds ---\n","Training loss per 100 training steps: 0.29061334599181476\n","--- 6.201436519622803 seconds ---\n","Training loss per 100 training steps: 0.2725380587109019\n","--- 6.1972815990448 seconds ---\n","Training loss per 100 training steps: 0.25721586787541884\n","--- 6.222629547119141 seconds ---\n","Training loss per 100 training steps: 0.24495543834132763\n","--- 6.386406898498535 seconds ---\n","Training loss per 100 training steps: 0.2355710494817978\n","--- 6.188171625137329 seconds ---\n","Training loss per 100 training steps: 0.2274771844756888\n","--- 6.221814870834351 seconds ---\n","Training loss per 100 training steps: 0.22179204294387161\n","--- 6.258622884750366 seconds ---\n","Training loss per 100 training steps: 0.21523260575124886\n","--- 6.228551149368286 seconds ---\n","Training loss per 100 training steps: 0.2090210049057306\n","--- 6.256489038467407 seconds ---\n","Training loss per 100 training steps: 0.20403022048945454\n","--- 6.208234548568726 seconds ---\n","Training loss per 100 training steps: 0.19858148382160307\n","--- 6.251797199249268 seconds ---\n","Training loss per 100 training steps: 0.19418877991795983\n","--- 6.439971685409546 seconds ---\n","Training loss per 100 training steps: 0.1904487444171393\n","--- 6.27103590965271 seconds ---\n","Training loss per 100 training steps: 0.18653372386946337\n","--- 6.224559307098389 seconds ---\n","Training loss per 100 training steps: 0.18287166987091652\n","--- 6.267646074295044 seconds ---\n","Training loss per 100 training steps: 0.1789714980776655\n","--- 6.212487697601318 seconds ---\n","Training loss per 100 training steps: 0.17623115065891087\n","--- 6.2494285106658936 seconds ---\n","Training loss per 100 training steps: 0.17389308900390246\n","--- 6.245040416717529 seconds ---\n","Training loss per 100 training steps: 0.1720339348665854\n","--- 6.285341024398804 seconds ---\n","Training loss per 100 training steps: 0.16995150728328623\n","--- 6.247684717178345 seconds ---\n","Training loss per 100 training steps: 0.16752445372299943\n","--- 6.249883413314819 seconds ---\n","Training loss per 100 training steps: 0.1657281515792578\n","--- 6.503085136413574 seconds ---\n","Training loss per 100 training steps: 0.1638682248137531\n","--- 6.28097939491272 seconds ---\n","Training loss per 100 training steps: 0.1621035699969932\n","--- 6.22054123878479 seconds ---\n","Training loss per 100 training steps: 0.16015214497459707\n","--- 6.2559754848480225 seconds ---\n","Training loss per 100 training steps: 0.1581142504286892\n","--- 6.257285118103027 seconds ---\n","Training loss per 100 training steps: 0.156370252759579\n","--- 6.2585742473602295 seconds ---\n","Training loss per 100 training steps: 0.15440894962854326\n","--- 6.256937265396118 seconds ---\n","Training loss per 100 training steps: 0.1527642725995268\n","--- 6.294774532318115 seconds ---\n","Training loss per 100 training steps: 0.15157953384558012\n","--- 6.266570806503296 seconds ---\n","Training loss per 100 training steps: 0.1502826053718577\n","--- 6.300862550735474 seconds ---\n","Training loss per 100 training steps: 0.14870516078155713\n","--- 6.2973644733428955 seconds ---\n","Training loss per 100 training steps: 0.14810327178820937\n","--- 6.302611827850342 seconds ---\n","Training loss per 100 training steps: 0.1468654958879779\n","--- 6.60581111907959 seconds ---\n","Training loss per 100 training steps: 0.14572282185007823\n","--- 6.294858455657959 seconds ---\n","Training loss per 100 training steps: 0.14451059621148113\n","--- 6.2936365604400635 seconds ---\n","Training loss per 100 training steps: 0.1434279348503595\n","--- 6.285024166107178 seconds ---\n","Training loss per 100 training steps: 0.14251245676293411\n","--- 6.285911321640015 seconds ---\n","Training loss per 100 training steps: 0.1416759246587474\n","--- 6.26717472076416 seconds ---\n","Training loss per 100 training steps: 0.14097245211708057\n","--- 6.2852699756622314 seconds ---\n","Training loss per 100 training steps: 0.1402387008214032\n","--- 6.311411380767822 seconds ---\n","Training loss per 100 training steps: 0.13943090380039094\n","--- 6.3141021728515625 seconds ---\n","Training loss per 100 training steps: 0.13860023226817947\n","--- 6.2973644733428955 seconds ---\n","Training loss per 100 training steps: 0.13791358231926637\n","--- 6.294963359832764 seconds ---\n","Training loss per 100 training steps: 0.13727171515442146\n","--- 6.290143251419067 seconds ---\n","Training loss per 100 training steps: 0.13656182787441098\n","--- 6.309703826904297 seconds ---\n","Training loss per 100 training steps: 0.13591948307845428\n","--- 6.310975790023804 seconds ---\n","Training loss per 100 training steps: 0.1353592469894607\n","--- 6.291024208068848 seconds ---\n","Training loss per 100 training steps: 0.13471814889411365\n","--- 6.678004026412964 seconds ---\n","Training loss per 100 training steps: 0.13386666413390444\n","--- 6.307217359542847 seconds ---\n","Training loss per 100 training steps: 0.13321687647880848\n","--- 6.262955665588379 seconds ---\n","Training loss per 100 training steps: 0.13265485369120605\n","--- 6.301101446151733 seconds ---\n","Training loss per 100 training steps: 0.13222905433935167\n","--- 6.283254384994507 seconds ---\n","Training loss per 100 training steps: 0.13135974810185694\n","--- 6.2786970138549805 seconds ---\n","Training loss per 100 training steps: 0.13077025051871782\n","--- 6.251454830169678 seconds ---\n","Training loss per 100 training steps: 0.13030798596062013\n","--- 6.267401695251465 seconds ---\n","Training loss per 100 training steps: 0.1296364564615689\n","--- 6.256743907928467 seconds ---\n","Training loss per 100 training steps: 0.129228916136144\n","--- 6.265001058578491 seconds ---\n","Training loss per 100 training steps: 0.1285875726569396\n","--- 6.263884544372559 seconds ---\n","Training loss per 100 training steps: 0.12808306594379598\n","--- 6.298494577407837 seconds ---\n","Training loss per 100 training steps: 0.1276517013302284\n","--- 6.277144193649292 seconds ---\n","Training loss per 100 training steps: 0.12721868060586952\n","--- 6.279247760772705 seconds ---\n","Training loss per 100 training steps: 0.12694122379891257\n","--- 6.285089015960693 seconds ---\n","Training loss per 100 training steps: 0.12632936469723607\n","--- 6.273848295211792 seconds ---\n","Training loss per 100 training steps: 0.12577588693046982\n","--- 6.284143447875977 seconds ---\n","Training loss per 100 training steps: 0.1252081562781465\n","--- 6.289682626724243 seconds ---\n","Training loss per 100 training steps: 0.1247827831005403\n","--- 6.736236572265625 seconds ---\n","Training loss per 100 training steps: 0.12429555779727203\n","--- 6.285123348236084 seconds ---\n","Training loss per 100 training steps: 0.12370373076733159\n","--- 6.281554222106934 seconds ---\n","Training loss per 100 training steps: 0.12334670290083148\n","--- 6.299819231033325 seconds ---\n","Training loss per 100 training steps: 0.12304516509391851\n","--- 6.314134120941162 seconds ---\n","Training loss per 100 training steps: 0.12261046791691795\n","--- 6.309344053268433 seconds ---\n","Training loss per 100 training steps: 0.122334505948972\n","--- 6.27852201461792 seconds ---\n","Training loss per 100 training steps: 0.12200436470188321\n","--- 6.27393651008606 seconds ---\n","Training loss per 100 training steps: 0.12151517554206992\n","--- 6.284597873687744 seconds ---\n","Training loss per 100 training steps: 0.1211855907403486\n","--- 6.286240577697754 seconds ---\n","Training loss per 100 training steps: 0.1208843337034557\n","--- 6.304242849349976 seconds ---\n","Training loss per 100 training steps: 0.12051266584840716\n","--- 6.280568838119507 seconds ---\n","Training loss per 100 training steps: 0.12019310223261417\n","--- 6.295818567276001 seconds ---\n","Training loss per 100 training steps: 0.11980111730209964\n","--- 6.31802225112915 seconds ---\n","Training loss per 100 training steps: 0.11942482699088444\n","--- 6.262683391571045 seconds ---\n","Training loss per 100 training steps: 0.11903718873865025\n","--- 6.37901759147644 seconds ---\n","Training loss per 100 training steps: 0.11866355592817074\n","--- 6.3187243938446045 seconds ---\n","Training loss per 100 training steps: 0.11837953537574944\n","--- 6.368984937667847 seconds ---\n","Training loss per 100 training steps: 0.11794116464488595\n","--- 6.384821176528931 seconds ---\n","Training loss per 100 training steps: 0.11767729321205678\n","--- 6.310678958892822 seconds ---\n","Training loss per 100 training steps: 0.11748109472542202\n","--- 6.3233630657196045 seconds ---\n","Training loss per 100 training steps: 0.1170892241231409\n","--- 6.277553558349609 seconds ---\n","Training loss per 100 training steps: 0.11672466554514702\n","--- 6.33650016784668 seconds ---\n","Training loss per 100 training steps: 0.11640596788737917\n","--- 6.866677284240723 seconds ---\n","Training loss per 100 training steps: 0.11611952425926435\n","--- 6.324599266052246 seconds ---\n","Training loss per 100 training steps: 0.11572182057208787\n","--- 6.289690256118774 seconds ---\n","Training loss per 100 training steps: 0.11540596484535522\n","--- 6.314634084701538 seconds ---\n","Training loss per 100 training steps: 0.11513217510159235\n","--- 6.33261775970459 seconds ---\n","Training loss per 100 training steps: 0.1148786850220922\n","--- 6.357493162155151 seconds ---\n","Training loss epoch: 0.11485987902605736\n","Training accuracy epoch: 0.9654872556432957\n","Validation loss per 100 evaluation steps: 0.17959095537662506\n","Validation loss per 100 evaluation steps: 0.09167466237265064\n","Validation loss per 100 evaluation steps: 0.08305585853736941\n","Validation loss per 100 evaluation steps: 0.08370736398133898\n","Validation loss per 100 evaluation steps: 0.08610467216815158\n","Validation loss per 100 evaluation steps: 0.08413861493371844\n","Validation loss per 100 evaluation steps: 0.08415582424034357\n","Validation loss per 100 evaluation steps: 0.08459801487975054\n","Validation loss per 100 evaluation steps: 0.08574759636522525\n","Testing Loss: 0.08529246697552614\n","Testing Accuracy: 0.9721365663598137\n","Validation loss per 100 evaluation steps: 0.05042073503136635\n","Validation loss per 100 evaluation steps: 0.06333364193840067\n","--- 1.5556073188781738 seconds ---\n","Validation loss per 100 evaluation steps: 0.07321663048360595\n","--- 1.5624005794525146 seconds ---\n","Validation loss per 100 evaluation steps: 0.0789690883753079\n","--- 1.555647373199463 seconds ---\n","Validation loss per 100 evaluation steps: 0.07641578046403587\n","--- 1.5314624309539795 seconds ---\n","Validation loss per 100 evaluation steps: 0.07635497080835506\n","--- 1.5509026050567627 seconds ---\n","Validation loss per 100 evaluation steps: 0.07721746878816094\n","--- 1.5323805809020996 seconds ---\n","Validation loss per 100 evaluation steps: 0.07773371069390367\n","--- 1.5331206321716309 seconds ---\n","Validation loss per 100 evaluation steps: 0.07985852043844822\n","--- 1.5353493690490723 seconds ---\n","Validation loss per 100 evaluation steps: 0.08423298539797608\n","--- 1.5536839962005615 seconds ---\n","Validation loss per 100 evaluation steps: 0.08448694174621306\n","--- 1.5413024425506592 seconds ---\n","Validation loss per 100 evaluation steps: 0.08544087108729334\n","--- 1.5396032333374023 seconds ---\n","Validation loss per 100 evaluation steps: 0.08440446950916825\n","--- 1.5299491882324219 seconds ---\n","Validation loss per 100 evaluation steps: 0.08407548086989503\n","--- 1.5583038330078125 seconds ---\n","Validation loss per 100 evaluation steps: 0.08559113310568123\n","--- 1.5549912452697754 seconds ---\n","Validation loss per 100 evaluation steps: 0.08569795012353437\n","--- 1.547375202178955 seconds ---\n","Validation loss per 100 evaluation steps: 0.0857780393508244\n","--- 1.5516836643218994 seconds ---\n","Validation loss per 100 evaluation steps: 0.08562338745199184\n","--- 1.5548992156982422 seconds ---\n","Validation Loss: 0.08545259895252644\n","Validation Accuracy: 0.9727208902234619\n","              precision    recall  f1-score   support\n","\n","         geo       0.87      0.90      0.88      2831\n","         gpe       0.95      0.95      0.95      1167\n","         org       0.70      0.69      0.70      1469\n","         per       0.75      0.78      0.76      1218\n","         tim       0.84      0.87      0.85      1526\n","\n","   micro avg       0.83      0.85      0.84      8211\n","   macro avg       0.82      0.84      0.83      8211\n","weighted avg       0.83      0.85      0.84      8211\n","\n","All files saved\n","Training epoch: 2\n","Training loss per 100 training steps: 0.071438267827034\n","Training loss per 100 training steps: 0.08584542460693508\n","--- 6.269492864608765 seconds ---\n","Training loss per 100 training steps: 0.0788221041618647\n","--- 6.604212522506714 seconds ---\n","Training loss per 100 training steps: 0.07645237217100331\n","--- 6.3364503383636475 seconds ---\n","Training loss per 100 training steps: 0.07243313722862883\n","--- 6.321956396102905 seconds ---\n","Training loss per 100 training steps: 0.07325582860521936\n","--- 6.280219554901123 seconds ---\n","Training loss per 100 training steps: 0.07415434786915848\n","--- 6.509946823120117 seconds ---\n","Training loss per 100 training steps: 0.07485384283937413\n","--- 6.301427841186523 seconds ---\n","Training loss per 100 training steps: 0.07428186826106188\n","--- 6.315840244293213 seconds ---\n","Training loss per 100 training steps: 0.07313224312021903\n","--- 6.332991361618042 seconds ---\n","Training loss per 100 training steps: 0.0743659165871379\n","--- 6.347531080245972 seconds ---\n","Training loss per 100 training steps: 0.07423332552469973\n","--- 6.340939283370972 seconds ---\n","Training loss per 100 training steps: 0.0744654730294438\n","--- 6.300546169281006 seconds ---\n","Training loss per 100 training steps: 0.07321528927772038\n","--- 6.356035947799683 seconds ---\n","Training loss per 100 training steps: 0.07274884458326819\n","--- 6.330324172973633 seconds ---\n","Training loss per 100 training steps: 0.07299365911326886\n","--- 6.594024658203125 seconds ---\n","Training loss per 100 training steps: 0.07213502624150665\n","--- 6.307057857513428 seconds ---\n","Training loss per 100 training steps: 0.07218207729934359\n","--- 6.300981521606445 seconds ---\n","Training loss per 100 training steps: 0.07288629667334949\n","--- 6.319981336593628 seconds ---\n","Training loss per 100 training steps: 0.0735930643522811\n","--- 6.305107116699219 seconds ---\n","Training loss per 100 training steps: 0.07380926605248973\n","--- 6.304201126098633 seconds ---\n","Training loss per 100 training steps: 0.07422902696688496\n","--- 6.303467273712158 seconds ---\n","Training loss per 100 training steps: 0.07443411695022074\n","--- 6.330211162567139 seconds ---\n","Training loss per 100 training steps: 0.07426507188981563\n","--- 6.325716018676758 seconds ---\n","Training loss per 100 training steps: 0.07406210756471523\n","--- 6.332090377807617 seconds ---\n","Training loss per 100 training steps: 0.07408499559658006\n","--- 6.337826490402222 seconds ---\n","Training loss per 100 training steps: 0.07418329875917765\n","--- 6.6147260665893555 seconds ---\n","Training loss per 100 training steps: 0.0742474393511592\n","--- 6.3118391036987305 seconds ---\n","Training loss per 100 training steps: 0.07405752174376125\n","--- 6.293521165847778 seconds ---\n","Training loss per 100 training steps: 0.07383707765957745\n","--- 6.311903238296509 seconds ---\n","Training loss per 100 training steps: 0.07376241278147729\n","--- 6.292647123336792 seconds ---\n","Training loss per 100 training steps: 0.07382642341392152\n","--- 6.3422346115112305 seconds ---\n","Training loss per 100 training steps: 0.07386755428956554\n","--- 6.306530475616455 seconds ---\n","Training loss per 100 training steps: 0.07398816534817917\n","--- 6.333586692810059 seconds ---\n","Training loss per 100 training steps: 0.07405114641157962\n","--- 6.3344926834106445 seconds ---\n","Training loss per 100 training steps: 0.073850298831175\n","--- 6.344678163528442 seconds ---\n","Training loss per 100 training steps: 0.0739864127037776\n","--- 6.356598138809204 seconds ---\n","Training loss per 100 training steps: 0.07435020426263414\n","--- 6.351531744003296 seconds ---\n","Training loss per 100 training steps: 0.07420337774209712\n","--- 6.338317394256592 seconds ---\n","Training loss per 100 training steps: 0.07439357084162528\n","--- 6.327998399734497 seconds ---\n","Training loss per 100 training steps: 0.07443989992980475\n","--- 6.712962865829468 seconds ---\n","Training loss per 100 training steps: 0.07457116595757898\n","--- 6.287473678588867 seconds ---\n","Training loss per 100 training steps: 0.07441942380147509\n","--- 6.305127859115601 seconds ---\n","Training loss per 100 training steps: 0.07439596536743136\n","--- 6.299504041671753 seconds ---\n","Training loss per 100 training steps: 0.07443505771557357\n","--- 6.350322246551514 seconds ---\n","Training loss per 100 training steps: 0.074326328191531\n","--- 6.32517409324646 seconds ---\n","Training loss per 100 training steps: 0.07437675737702701\n","--- 6.332342147827148 seconds ---\n","Training loss per 100 training steps: 0.07423504495135148\n","--- 6.3354785442352295 seconds ---\n","Training loss per 100 training steps: 0.07425676826864665\n","--- 6.364551067352295 seconds ---\n","Training loss per 100 training steps: 0.07448865061645993\n","--- 6.36432147026062 seconds ---\n","Training loss per 100 training steps: 0.07467157812759112\n","--- 6.393539190292358 seconds ---\n","Training loss per 100 training steps: 0.07457461066213787\n","--- 6.401469469070435 seconds ---\n","Training loss per 100 training steps: 0.07474830626090274\n","--- 6.3139801025390625 seconds ---\n","Training loss per 100 training steps: 0.0745815722892292\n","--- 6.295766830444336 seconds ---\n","Training loss per 100 training steps: 0.07431067922056932\n","--- 6.314945936203003 seconds ---\n","Training loss per 100 training steps: 0.07443745602566092\n","--- 6.311415195465088 seconds ---\n","Training loss per 100 training steps: 0.07463909667983408\n","--- 6.773326873779297 seconds ---\n","Training loss per 100 training steps: 0.07485040388660404\n","--- 6.310239315032959 seconds ---\n","Training loss per 100 training steps: 0.07471868281518515\n","--- 6.343079328536987 seconds ---\n","Training loss per 100 training steps: 0.07464820258084663\n","--- 6.317484378814697 seconds ---\n","Training loss per 100 training steps: 0.0745859722470737\n","--- 6.319533348083496 seconds ---\n","Training loss per 100 training steps: 0.07445371858181851\n","--- 6.341003179550171 seconds ---\n","Training loss per 100 training steps: 0.0743038671465963\n","--- 6.341387748718262 seconds ---\n","Training loss per 100 training steps: 0.07420304407383181\n","--- 6.35584020614624 seconds ---\n","Training loss per 100 training steps: 0.07425234263555618\n","--- 6.350106239318848 seconds ---\n","Training loss per 100 training steps: 0.07431819323311985\n","--- 6.279749393463135 seconds ---\n","Training loss per 100 training steps: 0.07447711692393884\n","--- 6.360143661499023 seconds ---\n","Training loss per 100 training steps: 0.07440018997057038\n","--- 6.374082803726196 seconds ---\n","Training loss per 100 training steps: 0.07432535332643044\n","--- 6.318309545516968 seconds ---\n","Training loss per 100 training steps: 0.07436994024097447\n","--- 6.308898448944092 seconds ---\n","Training loss per 100 training steps: 0.07421079216508172\n","--- 6.32135009765625 seconds ---\n","Training loss per 100 training steps: 0.07426753696480183\n","--- 6.3207175731658936 seconds ---\n","Training loss per 100 training steps: 0.07419611013510737\n","--- 6.343530893325806 seconds ---\n","Training loss per 100 training steps: 0.074159220325785\n","--- 6.291026592254639 seconds ---\n","Training loss per 100 training steps: 0.07420234407230575\n","--- 6.349137306213379 seconds ---\n","Training loss per 100 training steps: 0.07414767906927769\n","--- 6.288367748260498 seconds ---\n","Training loss per 100 training steps: 0.07401758260798041\n","--- 6.31776237487793 seconds ---\n","Training loss per 100 training steps: 0.07390240031062095\n","--- 6.869672775268555 seconds ---\n","Training loss per 100 training steps: 0.07389176635413801\n","--- 6.323330640792847 seconds ---\n","Training loss per 100 training steps: 0.07384663198733446\n","--- 6.3390138149261475 seconds ---\n","Training loss per 100 training steps: 0.07411314902438024\n","--- 6.33119010925293 seconds ---\n","Training loss per 100 training steps: 0.07404671076567905\n","--- 6.312992095947266 seconds ---\n","Training loss per 100 training steps: 0.07408850456947656\n","--- 6.3287317752838135 seconds ---\n","Training loss per 100 training steps: 0.07390857047697932\n","--- 6.3350749015808105 seconds ---\n","Training loss per 100 training steps: 0.07381559855322929\n","--- 6.314055442810059 seconds ---\n","Training loss per 100 training steps: 0.07385125809879178\n","--- 6.336653470993042 seconds ---\n","Training loss per 100 training steps: 0.07373729706253015\n","--- 6.347506046295166 seconds ---\n","Training loss per 100 training steps: 0.0737126159999763\n","--- 6.348062992095947 seconds ---\n","Training loss per 100 training steps: 0.07364443926256149\n","--- 6.347331762313843 seconds ---\n","Training loss per 100 training steps: 0.07357849443690462\n","--- 6.355204820632935 seconds ---\n","Training loss per 100 training steps: 0.07358038313313357\n","--- 6.356558084487915 seconds ---\n","Training loss per 100 training steps: 0.07356441811942036\n","--- 6.335448503494263 seconds ---\n","Training loss per 100 training steps: 0.07357393044777694\n","--- 6.35272216796875 seconds ---\n","Training loss per 100 training steps: 0.07347890678628508\n","--- 6.335076093673706 seconds ---\n","Training loss per 100 training steps: 0.07350612127079446\n","--- 6.363950729370117 seconds ---\n","Training loss per 100 training steps: 0.0735955983464872\n","--- 6.341639280319214 seconds ---\n","Training loss per 100 training steps: 0.0734992992718777\n","--- 6.333427667617798 seconds ---\n","Training loss per 100 training steps: 0.07351846989997178\n","--- 6.331651210784912 seconds ---\n","Training loss per 100 training steps: 0.07343010872481409\n","--- 6.337066888809204 seconds ---\n","Training loss per 100 training steps: 0.07343400626545014\n","--- 6.3385844230651855 seconds ---\n","Training loss per 100 training steps: 0.07328684655328721\n","--- 6.362731695175171 seconds ---\n","Training loss per 100 training steps: 0.07325669632967832\n","--- 6.4033942222595215 seconds ---\n","Training loss epoch: 0.0732681681985558\n","Training accuracy epoch: 0.9757215663049131\n","Validation loss per 100 evaluation steps: 0.07516659796237946\n","Validation loss per 100 evaluation steps: 0.07703705826695861\n","Validation loss per 100 evaluation steps: 0.08471647725007335\n","Validation loss per 100 evaluation steps: 0.08371720934384178\n","Validation loss per 100 evaluation steps: 0.08446441462990407\n","Validation loss per 100 evaluation steps: 0.08107225506990764\n","Validation loss per 100 evaluation steps: 0.08009282775418314\n","Validation loss per 100 evaluation steps: 0.07887659531064928\n","Validation loss per 100 evaluation steps: 0.07840431433324146\n","Testing Loss: 0.0799481308618982\n","Testing Accuracy: 0.9742500485817611\n","Validation loss per 100 evaluation steps: 0.0013633646303787827\n","Validation loss per 100 evaluation steps: 0.07998552810000763\n","--- 1.5688095092773438 seconds ---\n","Validation loss per 100 evaluation steps: 0.0803976107886432\n","--- 1.577544927597046 seconds ---\n","Validation loss per 100 evaluation steps: 0.07745922093080816\n","--- 1.5746231079101562 seconds ---\n","Validation loss per 100 evaluation steps: 0.08207390583235867\n","--- 1.5705211162567139 seconds ---\n","Validation loss per 100 evaluation steps: 0.07905931728067274\n","--- 1.569793701171875 seconds ---\n","Validation loss per 100 evaluation steps: 0.07711707234306044\n","--- 1.557521104812622 seconds ---\n","Validation loss per 100 evaluation steps: 0.07656443289676505\n","--- 1.5508384704589844 seconds ---\n","Validation loss per 100 evaluation steps: 0.07830630219809002\n","--- 1.5881328582763672 seconds ---\n","Validation loss per 100 evaluation steps: 0.07855052306764188\n","--- 1.5827147960662842 seconds ---\n","Validation loss per 100 evaluation steps: 0.07871770452355896\n","--- 1.5820972919464111 seconds ---\n","Validation loss per 100 evaluation steps: 0.07903474307340937\n","--- 1.5691168308258057 seconds ---\n","Validation loss per 100 evaluation steps: 0.07863431179987392\n","--- 1.5759632587432861 seconds ---\n","Validation loss per 100 evaluation steps: 0.07892323122268094\n","--- 1.5646436214447021 seconds ---\n","Validation loss per 100 evaluation steps: 0.07724241574321684\n","--- 1.5822722911834717 seconds ---\n","Validation loss per 100 evaluation steps: 0.07656465601132299\n","--- 1.5728280544281006 seconds ---\n","Validation loss per 100 evaluation steps: 0.07637204126014552\n","--- 1.5669975280761719 seconds ---\n","Validation loss per 100 evaluation steps: 0.07710681802099535\n","--- 1.5827996730804443 seconds ---\n","Validation Loss: 0.07813026045924323\n","Validation Accuracy: 0.975097008830297\n","              precision    recall  f1-score   support\n","\n","         geo       0.85      0.93      0.89      2831\n","         gpe       0.96      0.96      0.96      1167\n","         org       0.75      0.68      0.71      1469\n","         per       0.77      0.78      0.78      1218\n","         tim       0.87      0.86      0.86      1526\n","\n","   micro avg       0.84      0.85      0.85      8211\n","   macro avg       0.84      0.84      0.84      8211\n","weighted avg       0.84      0.85      0.85      8211\n","\n","All files saved\n","Training epoch: 3\n","Training loss per 100 training steps: 0.08912606537342072\n","Training loss per 100 training steps: 0.057090441755129784\n","--- 6.369898796081543 seconds ---\n","Training loss per 100 training steps: 0.05172785003067212\n","--- 6.6223084926605225 seconds ---\n","Training loss per 100 training steps: 0.05149840661850557\n","--- 6.370450735092163 seconds ---\n","Training loss per 100 training steps: 0.051436559809255704\n","--- 6.339052438735962 seconds ---\n","Training loss per 100 training steps: 0.05284795541332072\n","--- 6.318365573883057 seconds ---\n","Training loss per 100 training steps: 0.05353137760529854\n","--- 6.344557762145996 seconds ---\n","Training loss per 100 training steps: 0.053373677777410415\n","--- 6.315680742263794 seconds ---\n","Training loss per 100 training steps: 0.05258506011790026\n","--- 6.346494197845459 seconds ---\n","Training loss per 100 training steps: 0.05313307462939367\n","--- 6.296556234359741 seconds ---\n","Training loss per 100 training steps: 0.05364784017160696\n","--- 6.317712068557739 seconds ---\n","Training loss per 100 training steps: 0.054383306707946846\n","--- 6.3230180740356445 seconds ---\n","Training loss per 100 training steps: 0.054635847383551955\n","--- 6.35528302192688 seconds ---\n","Training loss per 100 training steps: 0.05437647699624773\n","--- 6.328325510025024 seconds ---\n","Training loss per 100 training steps: 0.05368485388502317\n","--- 6.332276344299316 seconds ---\n","Training loss per 100 training steps: 0.05420649740063951\n","--- 6.338458061218262 seconds ---\n","Training loss per 100 training steps: 0.05446014238678083\n","--- 6.290583372116089 seconds ---\n","Training loss per 100 training steps: 0.05518080313919758\n","--- 6.344129323959351 seconds ---\n","Training loss per 100 training steps: 0.0553622119110395\n","--- 6.569999933242798 seconds ---\n","Training loss per 100 training steps: 0.05532348967715773\n","--- 6.308467149734497 seconds ---\n","Training loss per 100 training steps: 0.05535942969650098\n","--- 6.335424900054932 seconds ---\n","Training loss per 100 training steps: 0.05493220562212112\n","--- 6.324643850326538 seconds ---\n","Training loss per 100 training steps: 0.05542061992212235\n","--- 6.3357765674591064 seconds ---\n","Training loss per 100 training steps: 0.055164586063128865\n","--- 6.336032152175903 seconds ---\n","Training loss per 100 training steps: 0.0547591752184386\n","--- 6.323965549468994 seconds ---\n","Training loss per 100 training steps: 0.05479579910411403\n","--- 6.302689552307129 seconds ---\n","Training loss per 100 training steps: 0.055043957603047824\n","--- 6.315763235092163 seconds ---\n","Training loss per 100 training steps: 0.055079974917704756\n","--- 6.347281455993652 seconds ---\n","Training loss per 100 training steps: 0.05532721090636318\n","--- 6.336765289306641 seconds ---\n","Training loss per 100 training steps: 0.05527697606324031\n","--- 6.345302104949951 seconds ---\n","Training loss per 100 training steps: 0.05537698953554065\n","--- 6.669217348098755 seconds ---\n","Training loss per 100 training steps: 0.055432119827540496\n","--- 6.342684268951416 seconds ---\n","Training loss per 100 training steps: 0.05549191417732188\n","--- 6.3395469188690186 seconds ---\n","Training loss per 100 training steps: 0.05556950177106514\n","--- 6.353011131286621 seconds ---\n","Training loss per 100 training steps: 0.055664275842047875\n","--- 6.307807207107544 seconds ---\n","Training loss per 100 training steps: 0.0557057253766976\n","--- 6.325222969055176 seconds ---\n","Training loss per 100 training steps: 0.05559810559013217\n","--- 6.320103406906128 seconds ---\n","Training loss per 100 training steps: 0.055631379397164865\n","--- 6.340440988540649 seconds ---\n","Training loss per 100 training steps: 0.05544718373508367\n","--- 6.314453125 seconds ---\n","Training loss per 100 training steps: 0.05531492429813244\n","--- 6.337778329849243 seconds ---\n","Training loss per 100 training steps: 0.055480725222884754\n","--- 6.238774299621582 seconds ---\n","Training loss per 100 training steps: 0.0555170806830168\n","--- 6.318600654602051 seconds ---\n","Training loss per 100 training steps: 0.05546327961497479\n","--- 6.369656324386597 seconds ---\n","Training loss per 100 training steps: 0.05546567332113438\n","--- 6.331348419189453 seconds ---\n","Training loss per 100 training steps: 0.055518450093594435\n","--- 6.712525367736816 seconds ---\n","Training loss per 100 training steps: 0.05586912625520375\n","--- 6.334959030151367 seconds ---\n","Training loss per 100 training steps: 0.05582021906451415\n","--- 6.305788278579712 seconds ---\n","Training loss per 100 training steps: 0.05566670163794603\n","--- 6.328666687011719 seconds ---\n","Training loss per 100 training steps: 0.05572554347705417\n","--- 6.339608907699585 seconds ---\n","Training loss per 100 training steps: 0.05583516646048688\n","--- 6.329934120178223 seconds ---\n","Training loss per 100 training steps: 0.055795610729257226\n","--- 6.350221872329712 seconds ---\n","Training loss per 100 training steps: 0.05560602084007234\n","--- 6.361058473587036 seconds ---\n","Training loss per 100 training steps: 0.0556774353965446\n","--- 6.3809309005737305 seconds ---\n","Training loss per 100 training steps: 0.055820125166274145\n","--- 6.321631908416748 seconds ---\n","Training loss per 100 training steps: 0.05587792726734052\n","--- 6.338629961013794 seconds ---\n","Training loss per 100 training steps: 0.05583607850613789\n","--- 6.330927610397339 seconds ---\n","Training loss per 100 training steps: 0.055706377156693465\n","--- 6.322462797164917 seconds ---\n","Training loss per 100 training steps: 0.05569214573055233\n","--- 6.326143503189087 seconds ---\n","Training loss per 100 training steps: 0.055739442597388104\n","--- 6.289935111999512 seconds ---\n","Training loss per 100 training steps: 0.05580584821163036\n","--- 6.309385776519775 seconds ---\n","Training loss per 100 training steps: 0.055745303597820914\n","--- 6.3219945430755615 seconds ---\n","Training loss per 100 training steps: 0.05583639882161934\n","--- 6.305803298950195 seconds ---\n","Training loss per 100 training steps: 0.05579761940371795\n","--- 6.3220086097717285 seconds ---\n","Training loss per 100 training steps: 0.05585187855155196\n","--- 6.778449535369873 seconds ---\n","Training loss per 100 training steps: 0.05596798387704894\n","--- 6.322582483291626 seconds ---\n","Training loss per 100 training steps: 0.055977739227791055\n","--- 6.3086631298065186 seconds ---\n","Training loss per 100 training steps: 0.056036961424267326\n","--- 6.313566207885742 seconds ---\n","Training loss per 100 training steps: 0.056016077453909456\n","--- 6.330214977264404 seconds ---\n","Training loss per 100 training steps: 0.056078633946982696\n","--- 6.331749200820923 seconds ---\n","Training loss per 100 training steps: 0.0560979258736715\n","--- 6.346495866775513 seconds ---\n","Training loss per 100 training steps: 0.05614384194849606\n","--- 6.315035581588745 seconds ---\n","Training loss per 100 training steps: 0.0561318024584358\n","--- 6.313512086868286 seconds ---\n","Training loss per 100 training steps: 0.056247954278786774\n","--- 6.343313455581665 seconds ---\n","Training loss per 100 training steps: 0.05629698669240621\n","--- 6.338683605194092 seconds ---\n","Training loss per 100 training steps: 0.056386053637084745\n","--- 6.354059457778931 seconds ---\n","Training loss per 100 training steps: 0.05628354019336609\n","--- 6.344071626663208 seconds ---\n","Training loss per 100 training steps: 0.05625801285075257\n","--- 6.356224536895752 seconds ---\n","Training loss per 100 training steps: 0.056141903654538965\n","--- 6.31295108795166 seconds ---\n","Training loss per 100 training steps: 0.056156000742890566\n","--- 6.32105827331543 seconds ---\n","Training loss per 100 training steps: 0.056258757348753664\n","--- 6.3528547286987305 seconds ---\n","Training loss per 100 training steps: 0.056290500627611734\n","--- 6.320296049118042 seconds ---\n","Training loss per 100 training steps: 0.05633714471485632\n","--- 6.338577032089233 seconds ---\n","Training loss per 100 training steps: 0.05634280735780949\n","--- 6.343584299087524 seconds ---\n","Training loss per 100 training steps: 0.056293475008032806\n","--- 6.338592052459717 seconds ---\n","Training loss per 100 training steps: 0.05630101099139291\n","--- 6.324492454528809 seconds ---\n","Training loss per 100 training steps: 0.056321938790609594\n","--- 6.879449844360352 seconds ---\n","Training loss per 100 training steps: 0.056373528265766214\n","--- 6.3254005908966064 seconds ---\n","Training loss per 100 training steps: 0.05651403321973038\n","--- 6.28989052772522 seconds ---\n","Training loss per 100 training steps: 0.05647528519977733\n","--- 6.335646629333496 seconds ---\n","Training loss per 100 training steps: 0.05637328267814027\n","--- 6.319534063339233 seconds ---\n","Training loss per 100 training steps: 0.056382765835540506\n","--- 6.330249786376953 seconds ---\n","Training loss per 100 training steps: 0.056400198371729895\n","--- 6.339188098907471 seconds ---\n","Training loss per 100 training steps: 0.05642498524827423\n","--- 6.311281204223633 seconds ---\n","Training loss per 100 training steps: 0.056407845315444766\n","--- 6.340177297592163 seconds ---\n","Training loss per 100 training steps: 0.056400698115145066\n","--- 6.314781665802002 seconds ---\n","Training loss per 100 training steps: 0.05636829541281811\n","--- 6.328960418701172 seconds ---\n","Training loss per 100 training steps: 0.05633005382007919\n","--- 6.351663827896118 seconds ---\n","Training loss per 100 training steps: 0.05642111144000847\n","--- 6.326438903808594 seconds ---\n","Training loss per 100 training steps: 0.0565348735141071\n","--- 6.372272253036499 seconds ---\n","Training loss per 100 training steps: 0.056598021872610066\n","--- 6.326996326446533 seconds ---\n","Training loss per 100 training steps: 0.05660073628841403\n","--- 6.338118553161621 seconds ---\n","Training loss per 100 training steps: 0.05660573063898371\n","--- 6.345242977142334 seconds ---\n","Training loss epoch: 0.05657808617255541\n","Training accuracy epoch: 0.9806027766713399\n","Validation loss per 100 evaluation steps: 0.15594398975372314\n","Validation loss per 100 evaluation steps: 0.08706760164615375\n","Validation loss per 100 evaluation steps: 0.0908913814977613\n","Validation loss per 100 evaluation steps: 0.08955221764156997\n","Validation loss per 100 evaluation steps: 0.09004721216444608\n","Validation loss per 100 evaluation steps: 0.08893783454457484\n","Validation loss per 100 evaluation steps: 0.08807546316679744\n","Validation loss per 100 evaluation steps: 0.08724012492018979\n","Validation loss per 100 evaluation steps: 0.08666611901874224\n","Testing Loss: 0.08557414909887179\n","Testing Accuracy: 0.9743211690243325\n","Validation loss per 100 evaluation steps: 0.01706467755138874\n","Validation loss per 100 evaluation steps: 0.07084325925946063\n","--- 1.573655366897583 seconds ---\n","Validation loss per 100 evaluation steps: 0.07932124155797907\n","--- 1.566525936126709 seconds ---\n","Validation loss per 100 evaluation steps: 0.08415443188378322\n","--- 1.5671813488006592 seconds ---\n","Validation loss per 100 evaluation steps: 0.08236712624627608\n","--- 1.560295820236206 seconds ---\n","Validation loss per 100 evaluation steps: 0.0840563942082138\n","--- 1.5797028541564941 seconds ---\n","Validation loss per 100 evaluation steps: 0.08331969981136747\n","--- 1.561429500579834 seconds ---\n","Validation loss per 100 evaluation steps: 0.08233728528979083\n","--- 1.7950503826141357 seconds ---\n","Validation loss per 100 evaluation steps: 0.08027317947131081\n","--- 1.5736191272735596 seconds ---\n","Validation loss per 100 evaluation steps: 0.08168485227643728\n","--- 1.55702805519104 seconds ---\n","Validation loss per 100 evaluation steps: 0.08287868669113985\n","--- 1.5536189079284668 seconds ---\n","Validation loss per 100 evaluation steps: 0.08238742470207609\n","--- 1.5506329536437988 seconds ---\n","Validation loss per 100 evaluation steps: 0.08161348109372298\n","--- 1.56951904296875 seconds ---\n","Validation loss per 100 evaluation steps: 0.08178834158089718\n","--- 1.573990821838379 seconds ---\n","Validation loss per 100 evaluation steps: 0.08114580007089695\n","--- 1.5841240882873535 seconds ---\n","Validation loss per 100 evaluation steps: 0.0806583162085395\n","--- 1.5653090476989746 seconds ---\n","Validation loss per 100 evaluation steps: 0.08001342874930964\n","--- 1.5682017803192139 seconds ---\n","Validation loss per 100 evaluation steps: 0.08096822164032888\n","--- 1.5591249465942383 seconds ---\n","Validation Loss: 0.08067753592039614\n","Validation Accuracy: 0.9760409125535804\n","              precision    recall  f1-score   support\n","\n","         geo       0.86      0.92      0.89      2831\n","         gpe       0.96      0.96      0.96      1167\n","         org       0.75      0.68      0.72      1469\n","         per       0.79      0.81      0.80      1218\n","         tim       0.87      0.85      0.86      1526\n","\n","   micro avg       0.85      0.86      0.85      8211\n","   macro avg       0.85      0.85      0.85      8211\n","weighted avg       0.85      0.86      0.85      8211\n","\n","All files saved\n"]}]},{"cell_type":"markdown","metadata":{"id":"kv3PQZypdQ_t"},"source":["### 2) Evaluate the Model"]},{"cell_type":"code","metadata":{"id":"jFIfRvYXdQRv"},"source":["def valid(model, testing_loader):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(testing_loader):\n","            \n","            ids = batch['input_ids'].to(device, dtype = torch.long)\n","            mask = batch['attention_mask'].to(device, dtype = torch.long)\n","            labels = batch['labels'].to(device, dtype = torch.long)\n","\n","        #             outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","        # loss = outputs[0]\n","        # tr_logits = outputs[1]\n","        # tr_loss += loss.item()\n","            \n","            outputs= model(input_ids=ids, attention_mask=mask, labels=labels)\n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","                if idx!=0:\n","                  print(\"--- %s seconds ---\" % (time.time() - start_time))\n","                start_time = time.time()\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","\n","    \n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    print(f\"Testing Loss: {eval_loss}\")\n","    print(f\"Testing Accuracy: {eval_accuracy}\")\n","\n","    return eval_labels, eval_preds\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7ArcZ0wusBi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638343306209,"user_tz":-480,"elapsed":344,"user":{"displayName":"haoqing tang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05592153470326549422"}},"outputId":"d462e988-5759-49d0-8ad9-631bcfa7e646"},"source":["len(testing_loader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1727"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"d6eqElS_5v-e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638343569786,"user_tz":-480,"elapsed":4616,"user":{"displayName":"haoqing tang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05592153470326549422"}},"outputId":"68ac71be-ca69-4f7f-fdcb-6db6a8b60abb"},"source":["model.load_state_dict(torch.load('/content/drive/MyDrive/pytorch_model.bin'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"ZJN32jfGd3sQ","colab":{"base_uri":"https://localhost:8080/","height":470},"executionInfo":{"status":"error","timestamp":1638343744526,"user_tz":-480,"elapsed":171897,"user":{"displayName":"haoqing tang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05592153470326549422"}},"outputId":"13c56eaa-3e33-4602-f0d0-d4d21e96b7da"},"source":["labels, predictions = valid(model, testing_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation loss per 100 evaluation steps: 3.179767608642578\n","Validation loss per 100 evaluation steps: 0.1595110519685999\n","Validation loss per 100 evaluation steps: 0.12362539394546082\n","Validation loss per 100 evaluation steps: 0.11389398336419418\n","Validation loss per 100 evaluation steps: 0.11897770675265595\n","Validation loss per 100 evaluation steps: 0.11645176817837057\n","Validation loss per 100 evaluation steps: 0.11470924197494117\n","Validation loss per 100 evaluation steps: 0.1201461836735611\n","Validation loss per 100 evaluation steps: 0.11642353929331893\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-a12e3d1b973f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-29-0ac8f1aed52b>\u001b[0m in \u001b[0;36mvalid\u001b[0;34m(model, testing_loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-cd65f930e798>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# step 1: get the sentence and word labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mword_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ner_tags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m         return self._getitem(\n\u001b[0;32m-> 1843\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1844\u001b[0m         )\n\u001b[1;32m   1845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[1;32m   1834\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m         formatted_output = format_table(\n\u001b[0;32m-> 1836\u001b[0;31m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1837\u001b[0m         )\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mformatted_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0mpython_formatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mextract_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pylist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"l-j7ltuGjWAB"},"source":["New_NerDict = dict((v,k) for k,v in dict(Ner).items())\n","New_NerDict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WUkwANjxkP2q"},"source":["labels_value = [[New_NerDict[i.item()] for i in labels]]\n","pred_value = [[New_NerDict[i.item()] for i in predictions]]\n","print(labels_value)\n","print(pred_value)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CdHNOEf5d7Sf"},"source":["from seqeval.metrics import classification_report\n","\n","print(classification_report(labels_value, pred_value))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qyW2sy3fMsko","executionInfo":{"status":"ok","timestamp":1638030047562,"user_tz":-480,"elapsed":3943,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"d99e2f43-eb5d-45f5-aaa3-596ea3bad8b5"},"source":["pip install seqeval"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 34.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.1)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=104e7548622562892f5d00dea1c4bb4e75e5274c8dc1b4f317572613f23894b0\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"]}]},{"cell_type":"code","metadata":{"id":"Qpkk2kR_MfIQ"},"source":["from datasets import load_metric\n","metric = load_metric(\"seqeval\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eVUlv9g9MxDL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638030057605,"user_tz":-480,"elapsed":407,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"489ea880-9ee5-4383-9051-bf331902baef"},"source":["metric"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Metric(name: \"seqeval\", features: {'predictions': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence')}, usage: \"\"\"\n","Produces labelling scores along with its sufficient statistics\n","from a source against one or more references.\n","\n","Args:\n","    predictions: List of List of predicted labels (Estimated targets as returned by a tagger)\n","    references: List of List of reference labels (Ground truth (correct) target values)\n","    suffix: True if the IOB prefix is after type, False otherwise. default: False\n","    scheme: Specify target tagging scheme. Should be one of [\"IOB1\", \"IOB2\", \"IOE1\", \"IOE2\", \"IOBES\", \"BILOU\"].\n","        default: None\n","    mode: Whether to count correct entity labels with incorrect I/B tags as true positives or not.\n","        If you want to only count exact matches, pass mode=\"strict\". default: None.\n","    sample_weight: Array-like of shape (n_samples,), weights for individual samples. default: None\n","    zero_division: Which value to substitute as a metric value when encountering zero division. Should be on of 0, 1,\n","        \"warn\". \"warn\" acts as 0, but the warning is raised.\n","\n","Returns:\n","    'scores': dict. Summary of the scores for overall and per type\n","        Overall:\n","            'accuracy': accuracy,\n","            'precision': precision,\n","            'recall': recall,\n","            'f1': F1 score, also known as balanced F-score or F-measure,\n","        Per type:\n","            'precision': precision,\n","            'recall': recall,\n","            'f1': F1 score, also known as balanced F-score or F-measure\n","Examples:\n","\n","    >>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n","    >>> references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n","    >>> seqeval = datasets.load_metric(\"seqeval\")\n","    >>> results = seqeval.compute(predictions=predictions, references=references)\n","    >>> print(list(results.keys()))\n","    ['MISC', 'PER', 'overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']\n","    >>> print(results[\"overall_f1\"])\n","    0.5\n","    >>> print(results[\"PER\"][\"f1\"])\n","    1.0\n","\"\"\", stored examples: 0)"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"tLT4WtJ_MS3F"},"source":["### 3) Save Model"]},{"cell_type":"code","metadata":{"id":"oq6073uHMWsu"},"source":["import os\n","\n","directory = \"./model\"\n","\n","if not os.path.exists(directory):\n","    os.makedirs(directory)\n","\n","# save vocabulary of the tokenizer\n","tokenizer.save_vocabulary(directory)\n","# save the model weights and its configuration file\n","model.save_pretrained(directory)\n","print('All files saved')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qT66WzNhmGTT"},"source":["#torch.save(model, 'model.pth')\n","\n","#torch.save(model.state_dict(), 'model_weights.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"101dcJ0w5rKK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638343453370,"user_tz":-480,"elapsed":25108,"user":{"displayName":"haoqing tang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05592153470326549422"}},"outputId":"c499c8ed-b40a-4fdb-9411-b4f0fa881231"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]}]}