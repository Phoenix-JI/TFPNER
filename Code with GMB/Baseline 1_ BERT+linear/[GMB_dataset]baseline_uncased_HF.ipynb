{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[GMB_dataset]baseline_uncased_HF.ipynb","provenance":[],"collapsed_sections":["tLT4WtJ_MS3F"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ac1771350ccb40feab36976e56c9c249":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_850cf3b9ebc546e1baaeae53767e6a14","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cc7e8bec725244a29297db295040ff08","IPY_MODEL_77655d80ec6e4332af541511c26106cb","IPY_MODEL_e0d6c31343ff4c868f7313b03ef8abe8"]}},"850cf3b9ebc546e1baaeae53767e6a14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cc7e8bec725244a29297db295040ff08":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5b131977b13346c5bf6be093b7842703","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8c4293f1abe94140bb0a9c50e7a83d91"}},"77655d80ec6e4332af541511c26106cb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_bf6c0fb6df35410cbddc817def3619b8","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_13b4561d654944d3b97ed0f5aeee5d77"}},"e0d6c31343ff4c868f7313b03ef8abe8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1c627991fb5249999037f94a93b6ed8d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 420M/420M [00:09&lt;00:00, 42.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b518a6c9df544ca59c5785d744f2f1c6"}},"5b131977b13346c5bf6be093b7842703":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8c4293f1abe94140bb0a9c50e7a83d91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bf6c0fb6df35410cbddc817def3619b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"13b4561d654944d3b97ed0f5aeee5d77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1c627991fb5249999037f94a93b6ed8d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b518a6c9df544ca59c5785d744f2f1c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"430caccb6b73417ba048c11f0ce07b42":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fcec3d18224a48e4bfd7ff6f476a3ba3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_478cbae64d264442828d5bc9b4eba7c0","IPY_MODEL_bde293d366394e98a081a825fcff6084","IPY_MODEL_09316820a3b1469388c6b4c4565b46d4"]}},"fcec3d18224a48e4bfd7ff6f476a3ba3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"478cbae64d264442828d5bc9b4eba7c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_53e144cc44da474e92886e46a5c6c469","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_afc3d800e4414db08065b6da46f59c0a"}},"bde293d366394e98a081a825fcff6084":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4110d551f0b34177b5af62b8af5069bb","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a617d462aa814439a0681cd8263b2988"}},"09316820a3b1469388c6b4c4565b46d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a88e30b961574014849435dfc56ce163","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 226k/226k [00:00&lt;00:00, 524kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bf6a61517d024b95ac7abaa134d086ec"}},"53e144cc44da474e92886e46a5c6c469":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"afc3d800e4414db08065b6da46f59c0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4110d551f0b34177b5af62b8af5069bb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a617d462aa814439a0681cd8263b2988":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a88e30b961574014849435dfc56ce163":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bf6a61517d024b95ac7abaa134d086ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3eb455de62414ac3a0a570796a22ab17":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_706a086e7d334a4eb584fea2f7c03141","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2ab0c9db33b64058809f4a51b1440999","IPY_MODEL_8770ef63ff304701b48e64287c8a6350","IPY_MODEL_fb401fed5e0e47c8a89614d2915122a6"]}},"706a086e7d334a4eb584fea2f7c03141":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2ab0c9db33b64058809f4a51b1440999":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_de34313857364705b73af4f9223537bc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_056698554a7540e9b5b77030d4367842"}},"8770ef63ff304701b48e64287c8a6350":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c230f4871bd34ca2b8fc6ae9cbc734bf","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5ca2d041e4e54f5ea4af5ed410140af4"}},"fb401fed5e0e47c8a89614d2915122a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_99166965ac0c4f4fbda5a01bca62dece","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 455k/455k [00:00&lt;00:00, 1.41MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f4f2bfd9467f4235ab0cc9b55a85faa5"}},"de34313857364705b73af4f9223537bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"056698554a7540e9b5b77030d4367842":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c230f4871bd34ca2b8fc6ae9cbc734bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5ca2d041e4e54f5ea4af5ed410140af4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"99166965ac0c4f4fbda5a01bca62dece":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f4f2bfd9467f4235ab0cc9b55a85faa5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"12d8b1f3458f436e8cc33a882125a075":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4422ccc68d2b45a2a36d9ef885c6983a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_da0baa4b66e349abb15f9eafc718cccb","IPY_MODEL_ffc36b876cd147239ed0197c94dfe656","IPY_MODEL_93d5a87746e7475781a0e3669fda0b7a"]}},"4422ccc68d2b45a2a36d9ef885c6983a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"da0baa4b66e349abb15f9eafc718cccb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9fe0f5f07a17428f9f609b40cdf3ab50","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a62914a2da304ce1b976ee46d5b6a0a7"}},"ffc36b876cd147239ed0197c94dfe656":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fee548bdc05547d684f48e9b13f8f57f","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d460d99b1e6b4c6ea52cc67ee9126f59"}},"93d5a87746e7475781a0e3669fda0b7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a471d79f65dd495c9c3f7a2fc3ffdfbd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 193B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f08583359f674c01a47af315516162a3"}},"9fe0f5f07a17428f9f609b40cdf3ab50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a62914a2da304ce1b976ee46d5b6a0a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fee548bdc05547d684f48e9b13f8f57f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d460d99b1e6b4c6ea52cc67ee9126f59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a471d79f65dd495c9c3f7a2fc3ffdfbd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f08583359f674c01a47af315516162a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0ea476dc4ee748df9bee75167f1523ba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_15b9270e6cd64278b5a9c3b7fde9eb4b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_33edb3f09d994232847446bcd9dc87b5","IPY_MODEL_c17757c66338437c9495eb5a7a4d5729","IPY_MODEL_930e4e2505da4106b1fe3874dbc64a9b"]}},"15b9270e6cd64278b5a9c3b7fde9eb4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"33edb3f09d994232847446bcd9dc87b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_aff0b6a569f64ee49aa8cdfc007fba37","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8aaf8cd5b228420bbdd34d808976978c"}},"c17757c66338437c9495eb5a7a4d5729":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_26910b21a594434ba74384065457f04b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_97476934727d485da3628282ae63b3ca"}},"930e4e2505da4106b1fe3874dbc64a9b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_97f7778d988d466fa8ee7f4535c7c4a4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [00:00&lt;00:00, 15.4kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c45fb62d936a4eadaabad11652d506c7"}},"aff0b6a569f64ee49aa8cdfc007fba37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8aaf8cd5b228420bbdd34d808976978c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"26910b21a594434ba74384065457f04b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"97476934727d485da3628282ae63b3ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"97f7778d988d466fa8ee7f4535c7c4a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c45fb62d936a4eadaabad11652d506c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DR6WnnwSiiPx"},"source":["## **1. Find the corresponding positive values for NER, POS, Chunk tags**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PflZFMcoyNNm","executionInfo":{"status":"ok","timestamp":1639654020215,"user_tz":-480,"elapsed":20206,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"ab9c6b59-d5b3-47f1-9941-a58554e93848"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"wNF30bg7jgaO"},"source":["# **2. Data Preprocessing for BERT Model (Apply Hugging Face Data)**"]},{"cell_type":"markdown","metadata":{"id":"5BMn0-NQnkOV"},"source":["### (1) Hugging Face Dataset Conll2003 Exploration"]},{"cell_type":"code","metadata":{"id":"ZIPMjy9KBFAX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639653917181,"user_tz":-480,"elapsed":8925,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"cb547330-e83d-43b7-f734-ccd2f2e2af5e"},"source":["!pip install datasets"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n","\u001b[K     |████████████████████████████████| 298 kB 15.9 MB/s \n","\u001b[?25hCollecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 70.0 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 59.7 MB/s \n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 71.3 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Collecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 657 kB/s \n","\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n","\u001b[K     |████████████████████████████████| 192 kB 66.8 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 67.0 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 64.8 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, huggingface-hub, datasets\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 datasets-1.16.1 frozenlist-1.2.0 fsspec-2021.11.1 huggingface-hub-0.2.1 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2\n"]}]},{"cell_type":"code","metadata":{"id":"htUmCUrUBS0D"},"source":["from datasets import load_dataset\n","# dataset = load_dataset('conll2003')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IXOdIATZoNVQ"},"source":["### (2) Covert Data to BERT Input Style"]},{"cell_type":"code","metadata":{"id":"IsEmtyyeniqn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639653977827,"user_tz":-480,"elapsed":9277,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"98d113ac-1687-417c-ad44-33da00eb8a9c"},"source":["!pip install transformers seqeval[gpu]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\n","\u001b[K     |████████████████████████████████| 3.4 MB 12.8 MB/s \n","\u001b[?25hCollecting seqeval[gpu]\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 41.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 57.5 MB/s \n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 48.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval[gpu]) (1.0.1)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.0.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=81b6c97cdc73463a1f544905ca89a534042062283d72189d95577a86fcf319da\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: pyyaml, tokenizers, seqeval, sacremoses, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed pyyaml-6.0 sacremoses-0.0.46 seqeval-1.2.2 tokenizers-0.10.3 transformers-4.14.1\n"]}]},{"cell_type":"code","metadata":{"id":"mUG0I8A-ndPo"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-3QG7H7OW4sT","colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["430caccb6b73417ba048c11f0ce07b42","fcec3d18224a48e4bfd7ff6f476a3ba3","478cbae64d264442828d5bc9b4eba7c0","bde293d366394e98a081a825fcff6084","09316820a3b1469388c6b4c4565b46d4","53e144cc44da474e92886e46a5c6c469","afc3d800e4414db08065b6da46f59c0a","4110d551f0b34177b5af62b8af5069bb","a617d462aa814439a0681cd8263b2988","a88e30b961574014849435dfc56ce163","bf6a61517d024b95ac7abaa134d086ec","3eb455de62414ac3a0a570796a22ab17","706a086e7d334a4eb584fea2f7c03141","2ab0c9db33b64058809f4a51b1440999","8770ef63ff304701b48e64287c8a6350","fb401fed5e0e47c8a89614d2915122a6","de34313857364705b73af4f9223537bc","056698554a7540e9b5b77030d4367842","c230f4871bd34ca2b8fc6ae9cbc734bf","5ca2d041e4e54f5ea4af5ed410140af4","99166965ac0c4f4fbda5a01bca62dece","f4f2bfd9467f4235ab0cc9b55a85faa5","12d8b1f3458f436e8cc33a882125a075","4422ccc68d2b45a2a36d9ef885c6983a","da0baa4b66e349abb15f9eafc718cccb","ffc36b876cd147239ed0197c94dfe656","93d5a87746e7475781a0e3669fda0b7a","9fe0f5f07a17428f9f609b40cdf3ab50","a62914a2da304ce1b976ee46d5b6a0a7","fee548bdc05547d684f48e9b13f8f57f","d460d99b1e6b4c6ea52cc67ee9126f59","a471d79f65dd495c9c3f7a2fc3ffdfbd","f08583359f674c01a47af315516162a3","0ea476dc4ee748df9bee75167f1523ba","15b9270e6cd64278b5a9c3b7fde9eb4b","33edb3f09d994232847446bcd9dc87b5","c17757c66338437c9495eb5a7a4d5729","930e4e2505da4106b1fe3874dbc64a9b","aff0b6a569f64ee49aa8cdfc007fba37","8aaf8cd5b228420bbdd34d808976978c","26910b21a594434ba74384065457f04b","97476934727d485da3628282ae63b3ca","97f7778d988d466fa8ee7f4535c7c4a4","c45fb62d936a4eadaabad11652d506c7"],"height":145},"executionInfo":{"status":"ok","timestamp":1639653997242,"user_tz":-480,"elapsed":5674,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"b6511e34-3ada-41b8-f05a-9e1e139b4c58"},"source":["MAX_LEN = 128     \n","TRAIN_BATCH_SIZE = 4\n","TEST_BATCH_SIZE = 2\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"430caccb6b73417ba048c11f0ce07b42","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3eb455de62414ac3a0a570796a22ab17","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12d8b1f3458f436e8cc33a882125a075","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ea476dc4ee748df9bee75167f1523ba","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Pv2f46Mye6U","executionInfo":{"status":"ok","timestamp":1639654025560,"user_tz":-480,"elapsed":5352,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"4e30e3a1-6ad2-4ed2-bb8d-a5c546034c80"},"source":["data = pd.read_csv(\"/content/drive/MyDrive/NLP/Final Project/New Dataset/Bank/ner_datasetreference_new.csv\", encoding='unicode_escape')\n","data.head()\n","\n","data.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sentence #      47959\n","Word          1048575\n","POS           1048575\n","Tag           1048575\n","dtype: int64"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUN2j6x8ye3Y","executionInfo":{"status":"ok","timestamp":1639654026114,"user_tz":-480,"elapsed":560,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"0a0b8311-6021-4aa6-ad1b-043adf601a92"},"source":["'''\n","step 2a: process NE tags and POS tags\n","'''\n","# NE \n","\"\"\"There are 8 category tags, each with a \"beginning\" and \"inside\" variant, and the \"outside\" tag. It is not really clear what these tags mean - \"geo\" probably stands for geographical entity, \"gpe\" for geopolitical entity, and so on. They do not seem to correspond with what the publisher says on Kaggle. Some tags seem to be underrepresented. Let's print them by frequency (highest to lowest): \"\"\"\n","\n","# tags = {}\n","# for tag, count in zip(frequencies_NE.index, frequencies_NE):\n","#     if tag != \"O\":\n","#         if tag[2:5] not in tags.keys():\n","#             tags[tag[2:5]] = count\n","#         else:\n","#             tags[tag[2:5]] += count\n","#     continue\n","\n","# print(sorted(tags.items(), key=lambda x: x[1], reverse=True))\n","\n","\"\"\"Let's remove \"art\", \"eve\" and \"nat\" named entities, as performance on them will probably be not comparable to the other named entities. \"\"\"\n","\n","entities_to_remove = [\"B-art\", \"I-art\", \"B-eve\", \"I-eve\", \"B-nat\", \"I-nat\"]\n","data = data[~data.Tag.isin(entities_to_remove)]\n","data.head()\n","data.count()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sentence #      47920\n","Word          1047063\n","POS           1047063\n","Tag           1047063\n","dtype: int64"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BL-8U-2Aye07","executionInfo":{"status":"ok","timestamp":1639654026115,"user_tz":-480,"elapsed":7,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"8ce22997-6aff-4a58-c69e-ee9e0cce55d6"},"source":["\"\"\"We create 2 dictionaries for NE: one that maps individual tags to indices, and one that maps indices to their individual tags. This is necessary in order to create the labels (as computers work with numbers = indices, rather than words = tags) - see further in this notebook.\"\"\"\n","\n","labels_to_ids = {k: v for v, k in enumerate(data.Tag.unique())}\n","ids_to_labels = {v: k for v, k in enumerate(data.Tag.unique())}\n","print(labels_to_ids)\n","print(ids_to_labels)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'I-per': 8, 'I-gpe': 9, 'I-tim': 10}\n","{0: 'O', 1: 'B-geo', 2: 'B-gpe', 3: 'B-per', 4: 'I-geo', 5: 'B-org', 6: 'I-org', 7: 'B-tim', 8: 'I-per', 9: 'I-gpe', 10: 'I-tim'}\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lb8mpgKVyeyO","executionInfo":{"status":"ok","timestamp":1639654026629,"user_tz":-480,"elapsed":518,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"06fb3707-15f9-410f-a326-1d2548dd9743"},"source":["# count NE tag\n","print(\"Number of NE tags: {}\".format(len(data.Tag.unique()))) # 17个\n","frequencies_NE = data.Tag.value_counts()\n","frequencies_NE\n","Ner_Tag = list(data.Tag.unique())\n","Ner_Number = [i for i in range(len(Ner_Tag))]\n","Ner = list(zip(Ner_Tag,Ner_Number))\n","print(Ner)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of NE tags: 11\n","[('O', 0), ('B-geo', 1), ('B-gpe', 2), ('B-per', 3), ('I-geo', 4), ('B-org', 5), ('I-org', 6), ('B-tim', 7), ('I-per', 8), ('I-gpe', 9), ('I-tim', 10)]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"gtIkj9n6yevj","executionInfo":{"status":"ok","timestamp":1639654027142,"user_tz":-480,"elapsed":4,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"3f2f1c7d-fdf2-439d-8f10-e5f9c96e4f35"},"source":["# pandas has a very handy \"forward fill\" function to fill missing values based on the last upper non-nan value\n","data = data.fillna(method='ffill')\n","data.head()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>Thousands</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sentence: 1</td>\n","      <td>of</td>\n","      <td>IN</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sentence: 1</td>\n","      <td>demonstrators</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentence: 1</td>\n","      <td>have</td>\n","      <td>VBP</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sentence: 1</td>\n","      <td>marched</td>\n","      <td>VBN</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Sentence #           Word  POS Tag\n","0  Sentence: 1      Thousands  NNS   O\n","1  Sentence: 1             of   IN   O\n","2  Sentence: 1  demonstrators  NNS   O\n","3  Sentence: 1           have  VBP   O\n","4  Sentence: 1        marched  VBN   O"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"P2yH1GS8yejT","colab":{"base_uri":"https://localhost:8080/","height":310},"executionInfo":{"status":"ok","timestamp":1639654124181,"user_tz":-480,"elapsed":96659,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"dd348c1b-0abc-46b6-efb7-63d6b5f4409d"},"source":["# let's create a new column called \"sentence\" which groups the words by sentence \n","data['sentence'] = data[['Sentence #','Word','Tag', 'POS']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n","# let's also create a new column called \"word_labels\" which groups the tags by sentence \n","data['word_labels'] = data[['Sentence #','Word','Tag', 'POS']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n","data.head()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>Thousands</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","      <td>Thousands of demonstrators have marched throug...</td>\n","      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sentence: 1</td>\n","      <td>of</td>\n","      <td>IN</td>\n","      <td>O</td>\n","      <td>Thousands of demonstrators have marched throug...</td>\n","      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sentence: 1</td>\n","      <td>demonstrators</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","      <td>Thousands of demonstrators have marched throug...</td>\n","      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentence: 1</td>\n","      <td>have</td>\n","      <td>VBP</td>\n","      <td>O</td>\n","      <td>Thousands of demonstrators have marched throug...</td>\n","      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sentence: 1</td>\n","      <td>marched</td>\n","      <td>VBN</td>\n","      <td>O</td>\n","      <td>Thousands of demonstrators have marched throug...</td>\n","      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Sentence #  ...                                        word_labels\n","0  Sentence: 1  ...  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...\n","1  Sentence: 1  ...  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...\n","2  Sentence: 1  ...  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...\n","3  Sentence: 1  ...  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...\n","4  Sentence: 1  ...  O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...\n","\n","[5 rows x 6 columns]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"9xb50hv6y2wo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639654124689,"user_tz":-480,"elapsed":516,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"3c61db4b-62c9-4174-ba7c-ff772d40430b"},"source":["\"\"\"Let's only keep the \"sentence\" and \"word_labels\" columns, and drop duplicates:\"\"\"\n","\n","data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n","data.head()\n","\n","len(data)\n","\"\"\"Let's verify that a random sentence and its corresponding tags are correct:\"\"\"\n","\n","print(data.iloc[41].sentence)\n","print(data.iloc[41].word_labels)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bedfordshire police said Tuesday that Omar Khayam was arrested in Bedford for breaching the conditions of his parole .\n","B-gpe,O,O,B-tim,O,B-per,I-per,O,O,O,B-geo,O,O,O,O,O,O,O,O\n"]}]},{"cell_type":"code","metadata":{"id":"rOEiVHh5y2tj"},"source":["train_df, validate_df, test_df = \\\n","              np.split(data.sample(frac=1, random_state=42), \n","                       [int(.85*len(data)), int(.925*len(data))])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgA6n9eBy2k7"},"source":["train_df = train_df.reset_index(drop=True)\n","validate_df = validate_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","data_combine_dict = {'train':train_df, 'validation':validate_df, 'test':test_df}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"blADhhnVBS44"},"source":["class Preprocess_Data(Dataset):\n","  def __init__(self, dataset, tokenizer, max_len): #usage -> train, validation, test\n","\n","        self.len = len(dataset)\n","        self.data = dataset\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","  def __getitem__(self, index):\n","\n","        # step 1: get the sentence and word labels \n","        sentence = self.data.sentence[index].strip().split()\n","        word_labels = self.data.word_labels[index].split(\",\")\n","\n","\n","        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n","        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n","        encoding = self.tokenizer(sentence,\n","                              is_split_into_words=True,\n","                              return_offsets_mapping=True,  #Set to True to return (char_start, char_end) for each token (default False)\n","                              padding='max_length', \n","                              truncation=True, \n","                              max_length=self.max_len)\n","        \n","        \n","        # step 3: create token labels only for first word pieces of each tokenized word\n","\n","        labels = [labels_to_ids[label] for label in word_labels]\n","\n","        # create an empty array of -100 of length max_length\n","        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n","        \n","        # set only labels whose first offset position is 0 and the second is not 0\n","        i = 0\n","        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n","          if mapping[0] == 0 and mapping[1] != 0:\n","            # overwrite label\n","            encoded_labels[idx] = labels[i]\n","            i += 1\n","\n","        # step 4: turn everything into PyTorch tensors\n","        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n","        item['labels'] = torch.as_tensor(encoded_labels)\n","        \n","        return item\n","\n","  def __len__(self):\n","        return self.len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQ54suiDDxHD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639654124690,"user_tz":-480,"elapsed":5,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"7d6c0a03-fec6-4f07-f87c-9452f10ae304"},"source":["dataset = data_combine_dict\n","\n","training_set = Preprocess_Data(train_df, tokenizer, MAX_LEN)\n","validation_set = Preprocess_Data(validate_df, tokenizer, MAX_LEN)\n","testing_set = Preprocess_Data(test_df, tokenizer, MAX_LEN)\n","print(len(training_set),len(validation_set),len(testing_set))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["40435 3568 3568\n"]}]},{"cell_type":"code","metadata":{"id":"09bOO2Ddzzzj"},"source":["for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"input_ids\"]), training_set[0][\"labels\"]):\n","  print('{0:10}  {1}'.format(token, label))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5pezZRhCzzwj"},"source":["# Define the Dataloader\n","training_loader = DataLoader(training_set, batch_size = TRAIN_BATCH_SIZE, shuffle=True,num_workers=0)\n","validation_loader = DataLoader(validation_set,batch_size = TRAIN_BATCH_SIZE, shuffle=True,num_workers=0)\n","testing_loader = DataLoader(testing_set,batch_size = TEST_BATCH_SIZE, shuffle=True,num_workers=0)\n","\n","print(len(training_loader),len(validation_loader),len(testing_loader))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"niW9J7uhIBxY"},"source":["# Define the Dataloader\n","training_loader = DataLoader(training_set, batch_size = TRAIN_BATCH_SIZE, shuffle=True,num_workers=0)\n","validation_loader = DataLoader(validation_set,batch_size = TRAIN_BATCH_SIZE, shuffle=True,num_workers=0)\n","testing_loader = DataLoader(testing_set,batch_size = TEST_BATCH_SIZE, shuffle=True,num_workers=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6X7rxuYMy6_z"},"source":[""]},{"cell_type":"code","metadata":{"id":"GlaCRzzmIBzz"},"source":["print(len(training_loader),len(validation_loader),len(testing_loader))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6hF-6klfKomd"},"source":["# **3. Define the Model**"]},{"cell_type":"markdown","metadata":{"id":"IKXls9ALdeJt"},"source":["### 1) Train the Model"]},{"cell_type":"code","metadata":{"id":"ndNwQePxFFdY"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6LvvmRHQMH2g","executionInfo":{"status":"ok","timestamp":1639183111224,"user_tz":-480,"elapsed":437,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"70142ce5-9a5d-4488-8609-22a8bc167aba"},"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","metadata":{"id":"VCEY_Fb2a7Hm"},"source":["# EPOCHS = 2\n","LEARNING_RATE = 1e-05\n","MAX_GRAD_NORM = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(Ner_Tag)"],"metadata":{"id":"IgLxs44ub4ay"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K8vCJCAFLWgK","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ac1771350ccb40feab36976e56c9c249","850cf3b9ebc546e1baaeae53767e6a14","cc7e8bec725244a29297db295040ff08","77655d80ec6e4332af541511c26106cb","e0d6c31343ff4c868f7313b03ef8abe8","5b131977b13346c5bf6be093b7842703","8c4293f1abe94140bb0a9c50e7a83d91","bf6c0fb6df35410cbddc817def3619b8","13b4561d654944d3b97ed0f5aeee5d77","1c627991fb5249999037f94a93b6ed8d","b518a6c9df544ca59c5785d744f2f1c6"]},"executionInfo":{"status":"ok","timestamp":1639183138941,"user_tz":-480,"elapsed":25899,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"98eeac30-5491-4e50-fff9-ca1889444ef2"},"source":["# Define the model by just BertForTokenClassification\n","model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(Ner_Tag))\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac1771350ccb40feab36976e56c9c249","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",")"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"blREmCNMPgpY"},"source":["optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3800AVERAin"},"source":["def train(epoch):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    # put model in training mode\n","    model.train()\n","    \n","    for idx, batch in enumerate(training_loader):\n","\n","        # if idx >200:\n","        #   break\n","        \n","        ids = batch['input_ids'].to(device, dtype = torch.long)\n","        mask = batch['attention_mask'].to(device, dtype = torch.long)\n","        labels = batch['labels'].to(device, dtype = torch.long)\n","\n","        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","        loss = outputs[0]\n","        tr_logits = outputs[1]\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 100==0:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","            # Step_loss.append(loss_step)\n","            if idx!=0:\n","              time_spent = time.time() - start_time\n","              # Train_time.append(time_spent)\n","              print(\"--- %s seconds ---\" % (time_spent))\n","            start_time = time.time() \n","        # compute training accuracy\n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","\n","        #print('flattened_targets', flattened_targets)\n","        #print('active_logits, ', active_logits)\n","        #print('flattened_predictions, ', flattened_predictions)\n","        # print('Logits 0, ', active_logits[0])\n","        # # print(\"real labels\",flattened_targets)\n","        # # print(\"real prediction\",flattened_predictions)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","        #print('active_accuracy ', active_accuracy)\n","\n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        #print('labels ', labels)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        #print('predictions ',predictions)\n","\n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","\n","        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","        tr_accuracy += tmp_tr_accuracy\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","    print(f\"Training loss epoch: {epoch_loss}\")\n","    print(f\"Training accuracy epoch: {tr_accuracy}\")\n","\n","    # --------------------------------------------------------------------------------------------------------------------\n","    # After the completion of each training epoch\n","    # measure our performance on validation set.\n","\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(validation_loader):\n","\n","            # if idx >200:\n","            #   break\n","            \n","            ids = batch['input_ids'].to(device, dtype = torch.long)\n","            mask = batch['attention_mask'].to(device, dtype = torch.long)\n","            labels = batch['labels'].to(device, dtype = torch.long)\n","            #print(\"real labels\",labels)\n","            \n","            outputs= model(input_ids=ids, attention_mask=mask, labels=labels)\n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    print(f\"Validation Loss: {eval_loss}\")\n","    print(f\"Validation Accuracy: {eval_accuracy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bYynGVa0aUYt","executionInfo":{"status":"ok","timestamp":1639183152976,"user_tz":-480,"elapsed":454,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"9cc0c019-d779-445a-ede0-05cbcbcff6ef"},"source":["len(training_loader),len(validation_loader),len(testing_loader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10109, 892, 1784)"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["import os\n","import time\n","from seqeval.metrics import classification_report"],"metadata":{"id":"zy2Tuws3SN0O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["New_NerDict = dict((v,k) for k,v in dict(Ner).items())\n","New_NerDict"],"metadata":{"id":"4XGkIJkOTquz"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t3ehY9wnXJeC","executionInfo":{"status":"ok","timestamp":1639185092987,"user_tz":-480,"elapsed":1902072,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"a7987291-806d-406b-9dd2-76d3b188c0b7"},"source":["EPOCHS = 3\n","for epoch in range(EPOCHS):\n","\n","    directory = \"/content/drive/MyDrive/NLP/Final Project/New Dataset/Bank/Baseline/Uncased/Model_\"+str(epoch+1)\n","\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train(epoch)\n","    labels, predictions = valid(model, testing_loader)\n","\n","    labels_value = [[New_NerDict[i.item()] for i in labels]]\n","    pred_value = [[New_NerDict[i.item()] for i in predictions]]\n","\n","    print(classification_report(labels_value, pred_value))\n","\n","    tokenizer.save_vocabulary(directory)\n","    # save the model weights and its configuration file\n","    model.save_pretrained(directory)\n","    print('All files saved')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training epoch: 1\n","Training loss per 100 training steps: 2.558194875717163\n","Training loss per 100 training steps: 0.9366362330937149\n","--- 5.9279704093933105 seconds ---\n","Training loss per 100 training steps: 0.6701212523514358\n","--- 5.915601968765259 seconds ---\n","Training loss per 100 training steps: 0.5432393161759425\n","--- 6.0934977531433105 seconds ---\n","Training loss per 100 training steps: 0.46587976292147304\n","--- 5.986647605895996 seconds ---\n","Training loss per 100 training steps: 0.4171758894762117\n","--- 6.056638240814209 seconds ---\n","Training loss per 100 training steps: 0.3808844457977078\n","--- 5.963814735412598 seconds ---\n","Training loss per 100 training steps: 0.3503934023250824\n","--- 5.837019443511963 seconds ---\n","Training loss per 100 training steps: 0.3292685566439797\n","--- 5.878058433532715 seconds ---\n","Training loss per 100 training steps: 0.3102899889971288\n","--- 5.7992284297943115 seconds ---\n","Training loss per 100 training steps: 0.29548316603930264\n","--- 5.953798770904541 seconds ---\n","Training loss per 100 training steps: 0.28381724766608696\n","--- 5.79200291633606 seconds ---\n","Training loss per 100 training steps: 0.2723115058135021\n","--- 5.80164909362793 seconds ---\n","Training loss per 100 training steps: 0.2614083993488452\n","--- 5.799376726150513 seconds ---\n","Training loss per 100 training steps: 0.253612813803711\n","--- 5.761203050613403 seconds ---\n","Training loss per 100 training steps: 0.24517939076991596\n","--- 5.818794012069702 seconds ---\n","Training loss per 100 training steps: 0.2379791289236371\n","--- 5.875178098678589 seconds ---\n","Training loss per 100 training steps: 0.23264113774272377\n","--- 6.0820159912109375 seconds ---\n","Training loss per 100 training steps: 0.22735287203582244\n","--- 5.883950710296631 seconds ---\n","Training loss per 100 training steps: 0.22301126579004\n","--- 5.908105373382568 seconds ---\n","Training loss per 100 training steps: 0.2187526081816695\n","--- 6.039660453796387 seconds ---\n","Training loss per 100 training steps: 0.21576135101923627\n","--- 6.079967737197876 seconds ---\n","Training loss per 100 training steps: 0.21248578264427517\n","--- 6.140283823013306 seconds ---\n","Training loss per 100 training steps: 0.20912540766302462\n","--- 6.092533588409424 seconds ---\n","Training loss per 100 training steps: 0.20521663686183778\n","--- 6.07651686668396 seconds ---\n","Training loss per 100 training steps: 0.20208821168876157\n","--- 6.0743467807769775 seconds ---\n","Training loss per 100 training steps: 0.1989881264307937\n","--- 6.0606849193573 seconds ---\n","Training loss per 100 training steps: 0.19636285288851765\n","--- 6.287580966949463 seconds ---\n","Training loss per 100 training steps: 0.19389637406288338\n","--- 5.95147442817688 seconds ---\n","Training loss per 100 training steps: 0.1915286975042424\n","--- 5.925990343093872 seconds ---\n","Training loss per 100 training steps: 0.1895170219339398\n","--- 5.92133903503418 seconds ---\n","Training loss per 100 training steps: 0.18756464991228392\n","--- 5.907818555831909 seconds ---\n","Training loss per 100 training steps: 0.18576837552191153\n","--- 5.891334295272827 seconds ---\n","Training loss per 100 training steps: 0.18353633860394727\n","--- 5.911478519439697 seconds ---\n","Training loss per 100 training steps: 0.18168079481733718\n","--- 5.931310415267944 seconds ---\n","Training loss per 100 training steps: 0.17957724861708096\n","--- 5.942243814468384 seconds ---\n","Training loss per 100 training steps: 0.1779450484132769\n","--- 5.943365573883057 seconds ---\n","Training loss per 100 training steps: 0.17614968093220318\n","--- 5.901345491409302 seconds ---\n","Training loss per 100 training steps: 0.17485819066655023\n","--- 6.247116327285767 seconds ---\n","Training loss per 100 training steps: 0.1734484701888718\n","--- 5.912688255310059 seconds ---\n","Training loss per 100 training steps: 0.17208471008079135\n","--- 5.8006908893585205 seconds ---\n","Training loss per 100 training steps: 0.17075445356067875\n","--- 5.801955699920654 seconds ---\n","Training loss per 100 training steps: 0.16938270431372052\n","--- 5.878935098648071 seconds ---\n","Training loss per 100 training steps: 0.1680413332781697\n","--- 5.842066287994385 seconds ---\n","Training loss per 100 training steps: 0.166955537800347\n","--- 5.872425556182861 seconds ---\n","Training loss per 100 training steps: 0.16610947624782618\n","--- 5.867025852203369 seconds ---\n","Training loss per 100 training steps: 0.16508283754912406\n","--- 5.827046871185303 seconds ---\n","Training loss per 100 training steps: 0.16395616151758075\n","--- 5.860136032104492 seconds ---\n","Training loss per 100 training steps: 0.16320424804840217\n","--- 5.788414239883423 seconds ---\n","Training loss per 100 training steps: 0.16200588498410587\n","--- 5.870107412338257 seconds ---\n","Training loss per 100 training steps: 0.16083808669211372\n","--- 5.815033197402954 seconds ---\n","Training loss per 100 training steps: 0.15990691577430619\n","--- 5.840414762496948 seconds ---\n","Training loss per 100 training steps: 0.1591442297438179\n","--- 5.835892915725708 seconds ---\n","Training loss per 100 training steps: 0.15811062871254583\n","--- 6.2431511878967285 seconds ---\n","Training loss per 100 training steps: 0.15773362746794423\n","--- 5.890821218490601 seconds ---\n","Training loss per 100 training steps: 0.15700489487370753\n","--- 5.871860504150391 seconds ---\n","Training loss per 100 training steps: 0.156112411983882\n","--- 5.840718030929565 seconds ---\n","Training loss per 100 training steps: 0.15516852879631052\n","--- 5.839941501617432 seconds ---\n","Training loss per 100 training steps: 0.1543497116597889\n","--- 5.834460973739624 seconds ---\n","Training loss per 100 training steps: 0.15372107026877505\n","--- 5.8174943923950195 seconds ---\n","Training loss per 100 training steps: 0.15270227610380424\n","--- 5.88462495803833 seconds ---\n","Training loss per 100 training steps: 0.15201590834676634\n","--- 5.839891672134399 seconds ---\n","Training loss per 100 training steps: 0.15146260945702164\n","--- 5.862324476242065 seconds ---\n","Training loss per 100 training steps: 0.15074206729982448\n","--- 5.841222524642944 seconds ---\n","Training loss per 100 training steps: 0.15023930518470144\n","--- 5.8686182498931885 seconds ---\n","Training loss per 100 training steps: 0.1494603742251515\n","--- 5.866867303848267 seconds ---\n","Training loss per 100 training steps: 0.14889923317120235\n","--- 5.88918924331665 seconds ---\n","Training loss per 100 training steps: 0.14828506216514603\n","--- 5.927403688430786 seconds ---\n","Training loss per 100 training steps: 0.14764917584553724\n","--- 5.86474084854126 seconds ---\n","Training loss per 100 training steps: 0.14704117463453792\n","--- 5.8448405265808105 seconds ---\n","Training loss per 100 training steps: 0.14640207925508839\n","--- 5.859720945358276 seconds ---\n","Training loss per 100 training steps: 0.14590015960724478\n","--- 6.345837831497192 seconds ---\n","Training loss per 100 training steps: 0.1454819581569385\n","--- 5.886844635009766 seconds ---\n","Training loss per 100 training steps: 0.1449879305210627\n","--- 5.879808664321899 seconds ---\n","Training loss per 100 training steps: 0.14461360171807164\n","--- 5.859878778457642 seconds ---\n","Training loss per 100 training steps: 0.14418273883870306\n","--- 5.873566627502441 seconds ---\n","Training loss per 100 training steps: 0.14369561812574091\n","--- 5.832818984985352 seconds ---\n","Training loss per 100 training steps: 0.14317430540856235\n","--- 5.819452524185181 seconds ---\n","Training loss per 100 training steps: 0.14265677639096222\n","--- 5.886655807495117 seconds ---\n","Training loss per 100 training steps: 0.14215648181404006\n","--- 5.894983768463135 seconds ---\n","Training loss per 100 training steps: 0.14175122335646348\n","--- 5.847922325134277 seconds ---\n","Training loss per 100 training steps: 0.14131598731297623\n","--- 5.854051113128662 seconds ---\n","Training loss per 100 training steps: 0.14111032448702482\n","--- 5.836843729019165 seconds ---\n","Training loss per 100 training steps: 0.14063861561018737\n","--- 5.923847675323486 seconds ---\n","Training loss per 100 training steps: 0.14031318375743948\n","--- 5.94073748588562 seconds ---\n","Training loss per 100 training steps: 0.13991591271892798\n","--- 5.99251127243042 seconds ---\n","Training loss per 100 training steps: 0.1394970927395949\n","--- 5.929313898086548 seconds ---\n","Training loss per 100 training steps: 0.13906368065165226\n","--- 5.910569190979004 seconds ---\n","Training loss per 100 training steps: 0.1387528272596006\n","--- 5.906483888626099 seconds ---\n","Training loss per 100 training steps: 0.13830333182798477\n","--- 5.974497079849243 seconds ---\n","Training loss per 100 training steps: 0.13789103736746786\n","--- 6.002806186676025 seconds ---\n","Training loss per 100 training steps: 0.1373658253699093\n","--- 6.010820627212524 seconds ---\n","Training loss per 100 training steps: 0.1369404226538533\n","--- 5.970771312713623 seconds ---\n","Training loss per 100 training steps: 0.13658170047677323\n","--- 6.590674877166748 seconds ---\n","Training loss per 100 training steps: 0.13615498692166278\n","--- 6.280809164047241 seconds ---\n","Training loss per 100 training steps: 0.13592555412609272\n","--- 6.127298831939697 seconds ---\n","Training loss per 100 training steps: 0.13565174409572656\n","--- 6.158412218093872 seconds ---\n","Training loss per 100 training steps: 0.13549127579250406\n","--- 6.14018702507019 seconds ---\n","Training loss per 100 training steps: 0.1351933857524064\n","--- 6.3352134227752686 seconds ---\n","Training loss per 100 training steps: 0.13491919469601432\n","--- 6.204327583312988 seconds ---\n","Training loss per 100 training steps: 0.1346790523840942\n","--- 6.096083164215088 seconds ---\n","Training loss per 100 training steps: 0.13454248669010338\n","--- 6.0439369678497314 seconds ---\n","Training loss epoch: 0.13448227559448853\n","Training accuracy epoch: 0.9605529123610728\n","Validation loss per 100 evaluation steps: 0.0010752191301435232\n","Validation loss per 100 evaluation steps: 0.099472700051115\n","Validation loss per 100 evaluation steps: 0.10165270534228181\n","Validation loss per 100 evaluation steps: 0.10175132413396296\n","Validation loss per 100 evaluation steps: 0.09992007976206266\n","Validation loss per 100 evaluation steps: 0.09990639345717622\n","Validation loss per 100 evaluation steps: 0.10085375277215487\n","Validation loss per 100 evaluation steps: 0.09990931230865567\n","Validation loss per 100 evaluation steps: 0.09977274046837005\n","Validation Loss: 0.09941780872811613\n","Validation Accuracy: 0.9684087593390572\n","Testing loss per 100 evaluation steps: 0.0014043650589883327\n","Testing loss per 100 evaluation steps: 0.089158744096386\n","Testing loss per 100 evaluation steps: 0.09230348963627247\n","Testing loss per 100 evaluation steps: 0.09407899322331863\n","Testing loss per 100 evaluation steps: 0.09791952693695406\n","Testing loss per 100 evaluation steps: 0.0985146942904264\n","Testing loss per 100 evaluation steps: 0.09533489117696288\n","Testing loss per 100 evaluation steps: 0.09710053325453942\n","Testing loss per 100 evaluation steps: 0.09759003091719425\n","Testing loss per 100 evaluation steps: 0.09690064859557336\n","Testing loss per 100 evaluation steps: 0.0965250811571311\n","Testing loss per 100 evaluation steps: 0.09503339039948001\n","Testing loss per 100 evaluation steps: 0.09624263920994418\n","Testing loss per 100 evaluation steps: 0.09680392147107357\n","Testing loss per 100 evaluation steps: 0.09744342380108745\n","Testing loss per 100 evaluation steps: 0.09753370323064994\n","Testing loss per 100 evaluation steps: 0.09716281812456405\n","Testing loss per 100 evaluation steps: 0.09688088232179806\n","Testing Loss: 0.0967778391838729\n","Testing Accuracy: 0.969137025515533\n","              precision    recall  f1-score   support\n","\n","         geo       0.85      0.90      0.87      2831\n","         gpe       0.96      0.95      0.95      1167\n","         org       0.66      0.62      0.64      1469\n","         per       0.73      0.74      0.74      1218\n","         tim       0.86      0.83      0.84      1526\n","\n","   micro avg       0.81      0.82      0.82      8211\n","   macro avg       0.81      0.81      0.81      8211\n","weighted avg       0.81      0.82      0.82      8211\n","\n","All files saved\n","Training epoch: 2\n","Training loss per 100 training steps: 0.047096703201532364\n","Training loss per 100 training steps: 0.08011632098004885\n","--- 5.941465854644775 seconds ---\n","Training loss per 100 training steps: 0.08549283108840794\n","--- 7.1756978034973145 seconds ---\n","Training loss per 100 training steps: 0.08733057094802864\n","--- 7.038224220275879 seconds ---\n","Training loss per 100 training steps: 0.0886074507526561\n","--- 6.0471673011779785 seconds ---\n","Training loss per 100 training steps: 0.0873570703978496\n","--- 5.86299204826355 seconds ---\n","Training loss per 100 training steps: 0.08533528611256111\n","--- 5.901551246643066 seconds ---\n","Training loss per 100 training steps: 0.08600289442745196\n","--- 5.880265474319458 seconds ---\n","Training loss per 100 training steps: 0.08636208741777994\n","--- 5.879399299621582 seconds ---\n","Training loss per 100 training steps: 0.08674675079484312\n","--- 5.8137736320495605 seconds ---\n","Training loss per 100 training steps: 0.08595662173227994\n","--- 6.063674211502075 seconds ---\n","Training loss per 100 training steps: 0.08716906049034197\n","--- 5.860511302947998 seconds ---\n","Training loss per 100 training steps: 0.08758582456423579\n","--- 5.776414632797241 seconds ---\n","Training loss per 100 training steps: 0.08776740593403337\n","--- 5.989657163619995 seconds ---\n","Training loss per 100 training steps: 0.0873219507788278\n","--- 6.011282682418823 seconds ---\n","Training loss per 100 training steps: 0.08735360112398004\n","--- 5.998891115188599 seconds ---\n","Training loss per 100 training steps: 0.08729018551820446\n","--- 5.808941125869751 seconds ---\n","Training loss per 100 training steps: 0.08826472644372758\n","--- 5.872239351272583 seconds ---\n","Training loss per 100 training steps: 0.0885155353649131\n","--- 5.850645065307617 seconds ---\n","Training loss per 100 training steps: 0.08872438028213744\n","--- 5.873321294784546 seconds ---\n","Training loss per 100 training steps: 0.08851098413216131\n","--- 6.252334356307983 seconds ---\n","Training loss per 100 training steps: 0.08824774220145651\n","--- 6.017950057983398 seconds ---\n","Training loss per 100 training steps: 0.08807629611466278\n","--- 5.930005311965942 seconds ---\n","Training loss per 100 training steps: 0.0883750573171182\n","--- 5.8705055713653564 seconds ---\n","Training loss per 100 training steps: 0.08802258634070141\n","--- 5.908740758895874 seconds ---\n","Training loss per 100 training steps: 0.08783439855507967\n","--- 6.097517728805542 seconds ---\n","Training loss per 100 training steps: 0.0881283819304492\n","--- 5.884096622467041 seconds ---\n","Training loss per 100 training steps: 0.08806342631345701\n","--- 5.885599374771118 seconds ---\n","Training loss per 100 training steps: 0.08790194891419452\n","--- 5.850778341293335 seconds ---\n","Training loss per 100 training steps: 0.08755099013733804\n","--- 5.843982696533203 seconds ---\n","Training loss per 100 training steps: 0.08743535666832045\n","--- 5.9012391567230225 seconds ---\n","Training loss per 100 training steps: 0.08760632606363179\n","--- 5.903465032577515 seconds ---\n","Training loss per 100 training steps: 0.08726886685045744\n","--- 5.739204406738281 seconds ---\n","Training loss per 100 training steps: 0.08744272610085309\n","--- 6.130909442901611 seconds ---\n","Training loss per 100 training steps: 0.08772979980645856\n","--- 5.741645812988281 seconds ---\n","Training loss per 100 training steps: 0.08743526262513947\n","--- 5.687786102294922 seconds ---\n","Training loss per 100 training steps: 0.08757201131671576\n","--- 5.712775945663452 seconds ---\n","Training loss per 100 training steps: 0.08750706322936447\n","--- 5.707848072052002 seconds ---\n","Training loss per 100 training steps: 0.08756372491220589\n","--- 5.759730339050293 seconds ---\n","Training loss per 100 training steps: 0.08750183108959002\n","--- 5.766353130340576 seconds ---\n","Training loss per 100 training steps: 0.08717347368318308\n","--- 5.75527024269104 seconds ---\n","Training loss per 100 training steps: 0.08711801979029826\n","--- 5.781423807144165 seconds ---\n","Training loss per 100 training steps: 0.08662493182968621\n","--- 5.796241283416748 seconds ---\n","Training loss per 100 training steps: 0.08657778835302932\n","--- 5.750591516494751 seconds ---\n","Training loss per 100 training steps: 0.08652317429340084\n","--- 5.774254083633423 seconds ---\n","Training loss per 100 training steps: 0.08665606077152439\n","--- 5.69513201713562 seconds ---\n","Training loss per 100 training steps: 0.08648451179456956\n","--- 5.693754434585571 seconds ---\n","Training loss per 100 training steps: 0.08644128273588611\n","--- 5.709227561950684 seconds ---\n","Training loss per 100 training steps: 0.08621663310666605\n","--- 5.729501485824585 seconds ---\n","Training loss per 100 training steps: 0.08638910672211135\n","--- 6.1688807010650635 seconds ---\n","Training loss per 100 training steps: 0.08645226528246173\n","--- 5.717377185821533 seconds ---\n","Training loss per 100 training steps: 0.08635396663893462\n","--- 5.724524736404419 seconds ---\n","Training loss per 100 training steps: 0.08619182964598779\n","--- 5.668402910232544 seconds ---\n","Training loss per 100 training steps: 0.0862152002789117\n","--- 5.773200273513794 seconds ---\n","Training loss per 100 training steps: 0.08600964393159173\n","--- 5.820220708847046 seconds ---\n","Training loss per 100 training steps: 0.08580905576775345\n","--- 5.847593069076538 seconds ---\n","Training loss per 100 training steps: 0.08574561598607042\n","--- 5.737902641296387 seconds ---\n","Training loss per 100 training steps: 0.0859556853834063\n","--- 5.732317209243774 seconds ---\n","Training loss per 100 training steps: 0.08591779845493716\n","--- 5.755441188812256 seconds ---\n","Training loss per 100 training steps: 0.08586482460511692\n","--- 5.8020195960998535 seconds ---\n","Training loss per 100 training steps: 0.08582805435949109\n","--- 5.712162733078003 seconds ---\n","Training loss per 100 training steps: 0.08561115034419245\n","--- 5.708808898925781 seconds ---\n","Training loss per 100 training steps: 0.08564767864978605\n","--- 5.728911638259888 seconds ---\n","Training loss per 100 training steps: 0.08559667113358431\n","--- 5.735861301422119 seconds ---\n","Training loss per 100 training steps: 0.08553304544788531\n","--- 5.7695300579071045 seconds ---\n","Training loss per 100 training steps: 0.08560588530373242\n","--- 5.844037055969238 seconds ---\n","Training loss per 100 training steps: 0.08546002785397425\n","--- 5.759466886520386 seconds ---\n","Training loss per 100 training steps: 0.08547565240643383\n","--- 5.720766544342041 seconds ---\n","Training loss per 100 training steps: 0.08548696854438946\n","--- 6.249067783355713 seconds ---\n","Training loss per 100 training steps: 0.08565047714920854\n","--- 5.731566667556763 seconds ---\n","Training loss per 100 training steps: 0.08544137997907868\n","--- 5.753646612167358 seconds ---\n","Training loss per 100 training steps: 0.08554448902816089\n","--- 5.815869331359863 seconds ---\n","Training loss per 100 training steps: 0.08548496089336188\n","--- 5.742181062698364 seconds ---\n","Training loss per 100 training steps: 0.0854440076031137\n","--- 5.725148916244507 seconds ---\n","Training loss per 100 training steps: 0.08549667420302698\n","--- 5.656411647796631 seconds ---\n","Training loss per 100 training steps: 0.0854986374499835\n","--- 5.729076623916626 seconds ---\n","Training loss per 100 training steps: 0.08543391716606176\n","--- 5.717008352279663 seconds ---\n","Training loss per 100 training steps: 0.08540694826762855\n","--- 5.70957088470459 seconds ---\n","Training loss per 100 training steps: 0.08537762575051781\n","--- 5.690754413604736 seconds ---\n","Training loss per 100 training steps: 0.0853034419953332\n","--- 5.807648420333862 seconds ---\n","Training loss per 100 training steps: 0.08502238474237976\n","--- 5.78959321975708 seconds ---\n","Training loss per 100 training steps: 0.08491764969474021\n","--- 5.777944087982178 seconds ---\n","Training loss per 100 training steps: 0.08480924771243793\n","--- 5.790574073791504 seconds ---\n","Training loss per 100 training steps: 0.0847652473345329\n","--- 5.81034517288208 seconds ---\n","Training loss per 100 training steps: 0.08477082977397689\n","--- 5.770262718200684 seconds ---\n","Training loss per 100 training steps: 0.08493077095229797\n","--- 5.757639646530151 seconds ---\n","Training loss per 100 training steps: 0.08496977685857278\n","--- 5.845183610916138 seconds ---\n","Training loss per 100 training steps: 0.0850442835815057\n","--- 5.792082071304321 seconds ---\n","Training loss per 100 training steps: 0.08496099817969087\n","--- 5.790396690368652 seconds ---\n","Training loss per 100 training steps: 0.08496959513993697\n","--- 5.776339292526245 seconds ---\n","Training loss per 100 training steps: 0.08500263559672848\n","--- 5.829071998596191 seconds ---\n","Training loss per 100 training steps: 0.08496238104082555\n","--- 6.3470189571380615 seconds ---\n","Training loss per 100 training steps: 0.08499162035133535\n","--- 5.740005731582642 seconds ---\n","Training loss per 100 training steps: 0.0849240186272004\n","--- 5.8235204219818115 seconds ---\n","Training loss per 100 training steps: 0.08495881085466411\n","--- 5.780406713485718 seconds ---\n","Training loss per 100 training steps: 0.08500616303756699\n","--- 5.6324241161346436 seconds ---\n","Training loss per 100 training steps: 0.08491252018028547\n","--- 5.706045866012573 seconds ---\n","Training loss per 100 training steps: 0.08492687331239707\n","--- 5.701329708099365 seconds ---\n","Training loss per 100 training steps: 0.08497373441955053\n","--- 5.704728603363037 seconds ---\n","Training loss per 100 training steps: 0.08489051528074823\n","--- 5.702866315841675 seconds ---\n","Training loss per 100 training steps: 0.08494982948482065\n","--- 5.6920530796051025 seconds ---\n","Training loss per 100 training steps: 0.08485197354780909\n","--- 5.711080074310303 seconds ---\n","Training loss epoch: 0.08490278431264164\n","Training accuracy epoch: 0.9722970063689257\n","Validation loss per 100 evaluation steps: 0.015825431793928146\n","Validation loss per 100 evaluation steps: 0.10076664593693165\n","Validation loss per 100 evaluation steps: 0.09133072282750604\n","Validation loss per 100 evaluation steps: 0.09242694511850275\n","Validation loss per 100 evaluation steps: 0.09222836472776874\n","Validation loss per 100 evaluation steps: 0.0935448126675944\n","Validation loss per 100 evaluation steps: 0.09460672657214687\n","Validation loss per 100 evaluation steps: 0.09378432676866358\n","Validation loss per 100 evaluation steps: 0.09444763657439863\n","Validation Loss: 0.09475610589387107\n","Validation Accuracy: 0.9689424406478707\n","Testing loss per 100 evaluation steps: 0.0030314181931316853\n","Testing loss per 100 evaluation steps: 0.09044495239173232\n","Testing loss per 100 evaluation steps: 0.09723407113058748\n","Testing loss per 100 evaluation steps: 0.09522719828138206\n","Testing loss per 100 evaluation steps: 0.09318333102513625\n","Testing loss per 100 evaluation steps: 0.09149842710781331\n","Testing loss per 100 evaluation steps: 0.08850607099204275\n","Testing loss per 100 evaluation steps: 0.09251761149854025\n","Testing loss per 100 evaluation steps: 0.09197037989042145\n","Testing loss per 100 evaluation steps: 0.09323022471281253\n","Testing loss per 100 evaluation steps: 0.09255412318793628\n","Testing loss per 100 evaluation steps: 0.09172634090571494\n","Testing loss per 100 evaluation steps: 0.09273860878269538\n","Testing loss per 100 evaluation steps: 0.09279315290813604\n","Testing loss per 100 evaluation steps: 0.092810184561943\n","Testing loss per 100 evaluation steps: 0.09343743155419884\n","Testing loss per 100 evaluation steps: 0.09203164265571145\n","Testing loss per 100 evaluation steps: 0.09156020219124976\n","Testing Loss: 0.09184133017080942\n","Testing Accuracy: 0.970627422908612\n","              precision    recall  f1-score   support\n","\n","         geo       0.83      0.92      0.87      2831\n","         gpe       0.97      0.95      0.96      1167\n","         org       0.70      0.61      0.66      1469\n","         per       0.72      0.80      0.76      1218\n","         tim       0.87      0.85      0.86      1526\n","\n","   micro avg       0.82      0.84      0.83      8211\n","   macro avg       0.82      0.83      0.82      8211\n","weighted avg       0.82      0.84      0.83      8211\n","\n","All files saved\n","Training epoch: 3\n","Training loss per 100 training steps: 0.020915962755680084\n","Training loss per 100 training steps: 0.06233549683290248\n","--- 5.736490726470947 seconds ---\n","Training loss per 100 training steps: 0.06681070740181785\n","--- 6.46525239944458 seconds ---\n","Training loss per 100 training steps: 0.06525044605844466\n","--- 6.513580799102783 seconds ---\n","Training loss per 100 training steps: 0.06408218065383577\n","--- 5.772036790847778 seconds ---\n","Training loss per 100 training steps: 0.06466963678526835\n","--- 5.751331329345703 seconds ---\n","Training loss per 100 training steps: 0.062898057443206\n","--- 5.715415000915527 seconds ---\n","Training loss per 100 training steps: 0.06325364794473186\n","--- 5.732687473297119 seconds ---\n","Training loss per 100 training steps: 0.06371886149594111\n","--- 5.762342691421509 seconds ---\n","Training loss per 100 training steps: 0.06505379462500953\n","--- 5.832285642623901 seconds ---\n","Training loss per 100 training steps: 0.06530476066927714\n","--- 6.001699686050415 seconds ---\n","Training loss per 100 training steps: 0.06548188347305992\n","--- 5.718056678771973 seconds ---\n","Training loss per 100 training steps: 0.06490250703284314\n","--- 5.718535661697388 seconds ---\n","Training loss per 100 training steps: 0.06483152368061923\n","--- 5.7680394649505615 seconds ---\n","Training loss per 100 training steps: 0.06483239090117658\n","--- 5.746274471282959 seconds ---\n","Training loss per 100 training steps: 0.06488576406207516\n","--- 5.727717638015747 seconds ---\n","Training loss per 100 training steps: 0.06485869084747882\n","--- 5.750117063522339 seconds ---\n","Training loss per 100 training steps: 0.06487415473924132\n","--- 5.816271781921387 seconds ---\n","Training loss per 100 training steps: 0.06462589180424899\n","--- 5.730010271072388 seconds ---\n","Training loss per 100 training steps: 0.06451367175730809\n","--- 5.85319972038269 seconds ---\n","Training loss per 100 training steps: 0.06449507934327618\n","--- 6.099614143371582 seconds ---\n","Training loss per 100 training steps: 0.06453382592053003\n","--- 5.741843938827515 seconds ---\n","Training loss per 100 training steps: 0.06464160033636294\n","--- 5.704746961593628 seconds ---\n","Training loss per 100 training steps: 0.06447887339969272\n","--- 5.767246723175049 seconds ---\n","Training loss per 100 training steps: 0.06419192733609809\n","--- 5.796298027038574 seconds ---\n","Training loss per 100 training steps: 0.06423392239896372\n","--- 5.75830078125 seconds ---\n","Training loss per 100 training steps: 0.06439644095845912\n","--- 5.767596244812012 seconds ---\n","Training loss per 100 training steps: 0.06464964589216765\n","--- 5.715887546539307 seconds ---\n","Training loss per 100 training steps: 0.0646844566752839\n","--- 5.72826361656189 seconds ---\n","Training loss per 100 training steps: 0.06449608293825095\n","--- 5.6837263107299805 seconds ---\n","Training loss per 100 training steps: 0.06467280375503907\n","--- 5.804920434951782 seconds ---\n","Training loss per 100 training steps: 0.06470451848527561\n","--- 5.751227855682373 seconds ---\n","Training loss per 100 training steps: 0.06464604297567193\n","--- 6.126498222351074 seconds ---\n","Training loss per 100 training steps: 0.06489031919428899\n","--- 5.732163429260254 seconds ---\n","Training loss per 100 training steps: 0.06467028428311501\n","--- 5.734818696975708 seconds ---\n","Training loss per 100 training steps: 0.06483812914761462\n","--- 5.729344129562378 seconds ---\n","Training loss per 100 training steps: 0.06494162468349356\n","--- 5.756899118423462 seconds ---\n","Training loss per 100 training steps: 0.06487830345214181\n","--- 5.7137229442596436 seconds ---\n","Training loss per 100 training steps: 0.06512349502176178\n","--- 5.659639120101929 seconds ---\n","Training loss per 100 training steps: 0.06520506572159325\n","--- 5.6482579708099365 seconds ---\n","Training loss per 100 training steps: 0.06500226864114785\n","--- 5.593841791152954 seconds ---\n","Training loss per 100 training steps: 0.06518264505031854\n","--- 5.799243927001953 seconds ---\n","Training loss per 100 training steps: 0.06505358305512571\n","--- 5.66401219367981 seconds ---\n","Training loss per 100 training steps: 0.06506827246218469\n","--- 5.6466405391693115 seconds ---\n","Training loss per 100 training steps: 0.06516606492560349\n","--- 5.692127466201782 seconds ---\n","Training loss per 100 training steps: 0.06481861365152262\n","--- 5.683210611343384 seconds ---\n","Training loss per 100 training steps: 0.06495607412869757\n","--- 5.719524145126343 seconds ---\n","Training loss per 100 training steps: 0.06490204449199785\n","--- 5.757596731185913 seconds ---\n","Training loss per 100 training steps: 0.06506604627124325\n","--- 6.205451250076294 seconds ---\n","Training loss per 100 training steps: 0.0651978215266669\n","--- 5.665950059890747 seconds ---\n","Training loss per 100 training steps: 0.06508926180322233\n","--- 5.75091290473938 seconds ---\n","Training loss per 100 training steps: 0.06517799925856939\n","--- 5.772306442260742 seconds ---\n","Training loss per 100 training steps: 0.0652986720944881\n","--- 5.828184604644775 seconds ---\n","Training loss per 100 training steps: 0.06531691440018797\n","--- 5.776806592941284 seconds ---\n","Training loss per 100 training steps: 0.065174082469139\n","--- 5.755778074264526 seconds ---\n","Training loss per 100 training steps: 0.06536068586878138\n","--- 5.770519733428955 seconds ---\n","Training loss per 100 training steps: 0.06540254098785775\n","--- 5.726625919342041 seconds ---\n","Training loss per 100 training steps: 0.06567464371813528\n","--- 5.68558931350708 seconds ---\n","Training loss per 100 training steps: 0.06555325330380236\n","--- 5.62889552116394 seconds ---\n","Training loss per 100 training steps: 0.065599859852871\n","--- 5.638697624206543 seconds ---\n","Training loss per 100 training steps: 0.06549547171409172\n","--- 5.709754228591919 seconds ---\n","Training loss per 100 training steps: 0.06558665974596543\n","--- 5.6879494190216064 seconds ---\n","Training loss per 100 training steps: 0.06556954403786198\n","--- 5.700182199478149 seconds ---\n","Training loss per 100 training steps: 0.06539322474023869\n","--- 5.663318395614624 seconds ---\n","Training loss per 100 training steps: 0.06547972632481593\n","--- 5.6605212688446045 seconds ---\n","Training loss per 100 training steps: 0.06544656986925988\n","--- 5.807992219924927 seconds ---\n","Training loss per 100 training steps: 0.06539876043143901\n","--- 6.2636191844940186 seconds ---\n","Training loss per 100 training steps: 0.06538382568808798\n","--- 5.729705333709717 seconds ---\n","Training loss per 100 training steps: 0.06534501992904161\n","--- 5.700283050537109 seconds ---\n","Training loss per 100 training steps: 0.06537460589291534\n","--- 5.718397617340088 seconds ---\n","Training loss per 100 training steps: 0.065363775071816\n","--- 5.757607698440552 seconds ---\n","Training loss per 100 training steps: 0.06539334880805825\n","--- 5.770922899246216 seconds ---\n","Training loss per 100 training steps: 0.06534516501593493\n","--- 5.783032655715942 seconds ---\n","Training loss per 100 training steps: 0.06543212519875813\n","--- 5.722572088241577 seconds ---\n","Training loss per 100 training steps: 0.06544618152569885\n","--- 5.6660614013671875 seconds ---\n","Training loss per 100 training steps: 0.06541821451607675\n","--- 5.730267524719238 seconds ---\n","Training loss per 100 training steps: 0.06546503459013885\n","--- 5.792499542236328 seconds ---\n","Training loss per 100 training steps: 0.06555774211487406\n","--- 5.681366205215454 seconds ---\n","Training loss per 100 training steps: 0.06556926739190715\n","--- 5.705472946166992 seconds ---\n","Training loss per 100 training steps: 0.06552050658655657\n","--- 5.707304239273071 seconds ---\n","Training loss per 100 training steps: 0.06549999743584703\n","--- 5.781037330627441 seconds ---\n","Training loss per 100 training steps: 0.06547326538043989\n","--- 5.699794769287109 seconds ---\n","Training loss per 100 training steps: 0.06551304431776811\n","--- 5.765876054763794 seconds ---\n","Training loss per 100 training steps: 0.06552317365588171\n","--- 5.916677236557007 seconds ---\n","Training loss per 100 training steps: 0.06560564968989704\n","--- 5.87484073638916 seconds ---\n","Training loss per 100 training steps: 0.06564060667343569\n","--- 5.888001441955566 seconds ---\n","Training loss per 100 training steps: 0.06575506106341697\n","--- 5.780307292938232 seconds ---\n","Training loss per 100 training steps: 0.06573696903699346\n","--- 5.8938398361206055 seconds ---\n","Training loss per 100 training steps: 0.06578463827503331\n","--- 5.908297538757324 seconds ---\n","Training loss per 100 training steps: 0.06571104953500466\n","--- 5.826662302017212 seconds ---\n","Training loss per 100 training steps: 0.0657161659443764\n","--- 6.459653377532959 seconds ---\n","Training loss per 100 training steps: 0.06571999002948152\n","--- 5.832038164138794 seconds ---\n","Training loss per 100 training steps: 0.06565017797201249\n","--- 5.8975841999053955 seconds ---\n","Training loss per 100 training steps: 0.06561503846743236\n","--- 6.0617287158966064 seconds ---\n","Training loss per 100 training steps: 0.06562368822290829\n","--- 5.855510950088501 seconds ---\n","Training loss per 100 training steps: 0.06560488460756184\n","--- 5.781527757644653 seconds ---\n","Training loss per 100 training steps: 0.06574855794738704\n","--- 5.6992058753967285 seconds ---\n","Training loss per 100 training steps: 0.06586087253979497\n","--- 5.6535325050354 seconds ---\n","Training loss per 100 training steps: 0.06579347900243591\n","--- 5.694304943084717 seconds ---\n","Training loss per 100 training steps: 0.06588142303857833\n","--- 5.776069164276123 seconds ---\n","Training loss per 100 training steps: 0.06595127092318157\n","--- 5.737550735473633 seconds ---\n","Training loss per 100 training steps: 0.0659356577923418\n","--- 5.749567270278931 seconds ---\n","Training loss epoch: 0.06590369950842977\n","Training accuracy epoch: 0.9775560433027182\n","Validation loss per 100 evaluation steps: 0.2986200451850891\n","Validation loss per 100 evaluation steps: 0.11284562110172419\n","Validation loss per 100 evaluation steps: 0.10352622923155695\n","Validation loss per 100 evaluation steps: 0.09891131776452693\n","Validation loss per 100 evaluation steps: 0.09851970292586877\n","Validation loss per 100 evaluation steps: 0.09665326574157845\n","Validation loss per 100 evaluation steps: 0.09691628770494622\n","Validation loss per 100 evaluation steps: 0.09650709587033904\n","Validation loss per 100 evaluation steps: 0.09621377101958818\n","Validation Loss: 0.09715675070063935\n","Validation Accuracy: 0.9697350514614156\n","Testing loss per 100 evaluation steps: 0.06324759870767593\n","Testing loss per 100 evaluation steps: 0.10561997989537816\n","Testing loss per 100 evaluation steps: 0.107955535462842\n","Testing loss per 100 evaluation steps: 0.09977169002048447\n","Testing loss per 100 evaluation steps: 0.09914512352433963\n","Testing loss per 100 evaluation steps: 0.09831374043378836\n","Testing loss per 100 evaluation steps: 0.09604404172002468\n","Testing loss per 100 evaluation steps: 0.09702769940261982\n","Testing loss per 100 evaluation steps: 0.09660228372111637\n","Testing loss per 100 evaluation steps: 0.09508631762242833\n","Testing loss per 100 evaluation steps: 0.09363356098007383\n","Testing loss per 100 evaluation steps: 0.09506759349768437\n","Testing loss per 100 evaluation steps: 0.09476201229143508\n","Testing loss per 100 evaluation steps: 0.09342106018815681\n","Testing loss per 100 evaluation steps: 0.09300192537527069\n","Testing loss per 100 evaluation steps: 0.09347041351219759\n","Testing loss per 100 evaluation steps: 0.09331653637646826\n","Testing loss per 100 evaluation steps: 0.09246848960300005\n","Testing Loss: 0.09291010106797395\n","Testing Accuracy: 0.9717586547688308\n","              precision    recall  f1-score   support\n","\n","         geo       0.85      0.91      0.88      2831\n","         gpe       0.97      0.96      0.96      1167\n","         org       0.70      0.64      0.67      1469\n","         per       0.75      0.81      0.78      1218\n","         tim       0.86      0.86      0.86      1526\n","\n","   micro avg       0.83      0.84      0.84      8211\n","   macro avg       0.83      0.84      0.83      8211\n","weighted avg       0.83      0.84      0.83      8211\n","\n","All files saved\n"]}]},{"cell_type":"markdown","metadata":{"id":"kv3PQZypdQ_t"},"source":["### 2) Evaluate the Model"]},{"cell_type":"code","metadata":{"id":"jFIfRvYXdQRv"},"source":["def valid(model, testing_loader):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(testing_loader):\n","            \n","            ids = batch['input_ids'].to(device, dtype = torch.long)\n","            mask = batch['attention_mask'].to(device, dtype = torch.long)\n","            labels = batch['labels'].to(device, dtype = torch.long)\n","\n","        #             outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","        # loss = outputs[0]\n","        # tr_logits = outputs[1]\n","        # tr_loss += loss.item()\n","            \n","            outputs= model(input_ids=ids, attention_mask=mask, labels=labels)\n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Testing loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    print(f\"Testing Loss: {eval_loss}\")\n","    print(f\"Testing Accuracy: {eval_accuracy}\")\n","\n","    return eval_labels, eval_preds\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7ArcZ0wusBi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638343306209,"user_tz":-480,"elapsed":344,"user":{"displayName":"haoqing tang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05592153470326549422"}},"outputId":"d462e988-5759-49d0-8ad9-631bcfa7e646"},"source":["len(testing_loader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1727"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"d6eqElS_5v-e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638343569786,"user_tz":-480,"elapsed":4616,"user":{"displayName":"haoqing tang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05592153470326549422"}},"outputId":"68ac71be-ca69-4f7f-fdcb-6db6a8b60abb"},"source":["model.load_state_dict(torch.load('/content/drive/MyDrive/pytorch_model.bin'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"ZJN32jfGd3sQ","colab":{"base_uri":"https://localhost:8080/","height":470},"executionInfo":{"status":"error","timestamp":1638343744526,"user_tz":-480,"elapsed":171897,"user":{"displayName":"haoqing tang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05592153470326549422"}},"outputId":"13c56eaa-3e33-4602-f0d0-d4d21e96b7da"},"source":["labels, predictions = valid(model, testing_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation loss per 100 evaluation steps: 3.179767608642578\n","Validation loss per 100 evaluation steps: 0.1595110519685999\n","Validation loss per 100 evaluation steps: 0.12362539394546082\n","Validation loss per 100 evaluation steps: 0.11389398336419418\n","Validation loss per 100 evaluation steps: 0.11897770675265595\n","Validation loss per 100 evaluation steps: 0.11645176817837057\n","Validation loss per 100 evaluation steps: 0.11470924197494117\n","Validation loss per 100 evaluation steps: 0.1201461836735611\n","Validation loss per 100 evaluation steps: 0.11642353929331893\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-a12e3d1b973f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-29-0ac8f1aed52b>\u001b[0m in \u001b[0;36mvalid\u001b[0;34m(model, testing_loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-cd65f930e798>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# step 1: get the sentence and word labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mword_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ner_tags'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m         return self._getitem(\n\u001b[0;32m-> 1843\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1844\u001b[0m         )\n\u001b[1;32m   1845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[1;32m   1834\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m         formatted_output = format_table(\n\u001b[0;32m-> 1836\u001b[0;31m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1837\u001b[0m         )\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mformatted_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0mpython_formatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mextract_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pylist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"l-j7ltuGjWAB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639138788166,"user_tz":-480,"elapsed":316,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"0c3b23a4-7911-41bb-cde3-c6cbfef7f62a"},"source":["New_NerDict = dict((v,k) for k,v in dict(Ner).items())\n","New_NerDict"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'O',\n"," 1: 'B-geo',\n"," 2: 'B-gpe',\n"," 3: 'B-per',\n"," 4: 'I-geo',\n"," 5: 'B-org',\n"," 6: 'I-org',\n"," 7: 'B-tim',\n"," 8: 'I-per',\n"," 9: 'I-gpe',\n"," 10: 'I-tim'}"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"WUkwANjxkP2q"},"source":["labels_value = [[New_NerDict[i.item()] for i in labels]]\n","pred_value = [[New_NerDict[i.item()] for i in predictions]]\n","print(labels_value)\n","print(pred_value)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CdHNOEf5d7Sf","colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"status":"error","timestamp":1639138792110,"user_tz":-480,"elapsed":396,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"306bcd25-8868-4601-d672-a12e4c4fdda6"},"source":["from seqeval.metrics import classification_report\n","\n","print(classification_report(labels_value, pred_value))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-3ba35dc5bf0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mseqeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'labels_value' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qyW2sy3fMsko","executionInfo":{"status":"ok","timestamp":1638030047562,"user_tz":-480,"elapsed":3943,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"d99e2f43-eb5d-45f5-aaa3-596ea3bad8b5"},"source":["pip install seqeval"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 34.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.1)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=104e7548622562892f5d00dea1c4bb4e75e5274c8dc1b4f317572613f23894b0\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"]}]},{"cell_type":"code","metadata":{"id":"Qpkk2kR_MfIQ"},"source":["from datasets import load_metric\n","metric = load_metric(\"seqeval\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eVUlv9g9MxDL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638030057605,"user_tz":-480,"elapsed":407,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"489ea880-9ee5-4383-9051-bf331902baef"},"source":["metric"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Metric(name: \"seqeval\", features: {'predictions': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence')}, usage: \"\"\"\n","Produces labelling scores along with its sufficient statistics\n","from a source against one or more references.\n","\n","Args:\n","    predictions: List of List of predicted labels (Estimated targets as returned by a tagger)\n","    references: List of List of reference labels (Ground truth (correct) target values)\n","    suffix: True if the IOB prefix is after type, False otherwise. default: False\n","    scheme: Specify target tagging scheme. Should be one of [\"IOB1\", \"IOB2\", \"IOE1\", \"IOE2\", \"IOBES\", \"BILOU\"].\n","        default: None\n","    mode: Whether to count correct entity labels with incorrect I/B tags as true positives or not.\n","        If you want to only count exact matches, pass mode=\"strict\". default: None.\n","    sample_weight: Array-like of shape (n_samples,), weights for individual samples. default: None\n","    zero_division: Which value to substitute as a metric value when encountering zero division. Should be on of 0, 1,\n","        \"warn\". \"warn\" acts as 0, but the warning is raised.\n","\n","Returns:\n","    'scores': dict. Summary of the scores for overall and per type\n","        Overall:\n","            'accuracy': accuracy,\n","            'precision': precision,\n","            'recall': recall,\n","            'f1': F1 score, also known as balanced F-score or F-measure,\n","        Per type:\n","            'precision': precision,\n","            'recall': recall,\n","            'f1': F1 score, also known as balanced F-score or F-measure\n","Examples:\n","\n","    >>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n","    >>> references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n","    >>> seqeval = datasets.load_metric(\"seqeval\")\n","    >>> results = seqeval.compute(predictions=predictions, references=references)\n","    >>> print(list(results.keys()))\n","    ['MISC', 'PER', 'overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']\n","    >>> print(results[\"overall_f1\"])\n","    0.5\n","    >>> print(results[\"PER\"][\"f1\"])\n","    1.0\n","\"\"\", stored examples: 0)"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"tLT4WtJ_MS3F"},"source":["### 3) Save Model"]},{"cell_type":"code","metadata":{"id":"oq6073uHMWsu"},"source":["import os\n","\n","directory = \"./model\"\n","\n","if not os.path.exists(directory):\n","    os.makedirs(directory)\n","\n","# save vocabulary of the tokenizer\n","tokenizer.save_vocabulary(directory)\n","# save the model weights and its configuration file\n","model.save_pretrained(directory)\n","print('All files saved')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qT66WzNhmGTT"},"source":["#torch.save(model, 'model.pth')\n","\n","#torch.save(model.state_dict(), 'model_weights.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"101dcJ0w5rKK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638343453370,"user_tz":-480,"elapsed":25108,"user":{"displayName":"haoqing tang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05592153470326549422"}},"outputId":"c499c8ed-b40a-4fdb-9411-b4f0fa881231"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]}]}