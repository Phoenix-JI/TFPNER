{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[GMB_dataset]embedding_Cased.ipynb","provenance":[],"collapsed_sections":["5BMn0-NQnkOV","IXOdIATZoNVQ","9Txi4qKmD5tf","tLT4WtJ_MS3F"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"09226399d28c47759132106cb5b75762":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_85ddc09b05ab46dcb166ab52ca15f109","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fc920f5abc464c4994a018b556f94735","IPY_MODEL_95c30ee5bc2d44f9b661282827e79769","IPY_MODEL_0017799d4f6b4daf991b8a33eb3eff73"]}},"85ddc09b05ab46dcb166ab52ca15f109":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fc920f5abc464c4994a018b556f94735":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0a634ba09874468e98fa39f837e484cf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bdb836aba4fc4c1f9ac9df6fcc48af7e"}},"95c30ee5bc2d44f9b661282827e79769":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_198bfbc94ce64b09b8dc80f1465f0eae","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ef00d8da292f46c99b24da445fcae75d"}},"0017799d4f6b4daf991b8a33eb3eff73":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c78d5d2303c744409fe23163c8b328ad","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 208k/208k [00:00&lt;00:00, 1.63MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4bdc72bb3eb84d66b3ad7927c1cced57"}},"0a634ba09874468e98fa39f837e484cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bdb836aba4fc4c1f9ac9df6fcc48af7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"198bfbc94ce64b09b8dc80f1465f0eae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ef00d8da292f46c99b24da445fcae75d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c78d5d2303c744409fe23163c8b328ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4bdc72bb3eb84d66b3ad7927c1cced57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6a65f646154b487f9f37d158e78e87f8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7ba074cd9a2a4885842619035413b69d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f5633d0e4a53446ca97185d9d99eff4f","IPY_MODEL_25ab885cbae048ba849d84980601f583","IPY_MODEL_df6c621d10064eb987ec81716719a2cf"]}},"7ba074cd9a2a4885842619035413b69d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f5633d0e4a53446ca97185d9d99eff4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_32af295a42444f0894b1b7c7fc74165e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_520ba8bc47234c2cb66dd6f71e27ad15"}},"25ab885cbae048ba849d84980601f583":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_836ae04f9f50489180864b1fce0517c8","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":435797,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":435797,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_367022b86cdb4b67830c5b52ead270e0"}},"df6c621d10064eb987ec81716719a2cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9b697e71c20943ce94493c0082468d3d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 426k/426k [00:00&lt;00:00, 1.85MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e008015dc23a4ee4bec0fa28a7749911"}},"32af295a42444f0894b1b7c7fc74165e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"520ba8bc47234c2cb66dd6f71e27ad15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"836ae04f9f50489180864b1fce0517c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"367022b86cdb4b67830c5b52ead270e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9b697e71c20943ce94493c0082468d3d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e008015dc23a4ee4bec0fa28a7749911":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"57e16465fbf8476ab52497f83703257f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bbd5bb495f2e4dd18fbc10bab60c36a7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cda984007d8549858507c4bc0d0e1ac1","IPY_MODEL_0a0a745b33394d18bad7c931f9bc7010","IPY_MODEL_59605b30aed54430b9be64e73b8c427c"]}},"bbd5bb495f2e4dd18fbc10bab60c36a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cda984007d8549858507c4bc0d0e1ac1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8e948d386a894749b189adc399bba200","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_74ef22932ae546fea40a0882c3792b70"}},"0a0a745b33394d18bad7c931f9bc7010":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a526bf414e344de4a045f5adaec5e362","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":29,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":29,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1c4a90a477bf46ef8c3771a174115d8a"}},"59605b30aed54430b9be64e73b8c427c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0ee1beaea6fd4fb88c958d9a9cdbff56","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29.0/29.0 [00:00&lt;00:00, 1.18kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a3efc5d22dae48e8b76b5fdf9e994ae0"}},"8e948d386a894749b189adc399bba200":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"74ef22932ae546fea40a0882c3792b70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a526bf414e344de4a045f5adaec5e362":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1c4a90a477bf46ef8c3771a174115d8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0ee1beaea6fd4fb88c958d9a9cdbff56":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a3efc5d22dae48e8b76b5fdf9e994ae0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5af03ddec5af41d5ad6fd4a77a1a4d94":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e079d9515d1148bdbaecf14e6095155a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_46bc899d1c47413f8e67909fa77fcd5e","IPY_MODEL_75f3d38dfd65445c82ed33f823057e26","IPY_MODEL_fc6b5bdb129e4afa8366c8972ad09879"]}},"e079d9515d1148bdbaecf14e6095155a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"46bc899d1c47413f8e67909fa77fcd5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5ad6be31b76a4a90b2b54ff03903ae54","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2ac52d35a39c4cd5983321d9ec9b8892"}},"75f3d38dfd65445c82ed33f823057e26":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8912a0981d9a4e028e88627c39435941","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7d702a31ac8c4a679899551e6ea9b7df"}},"fc6b5bdb129e4afa8366c8972ad09879":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_721f92683d8e4c61bf85a3ddb44cb2d8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [00:00&lt;00:00, 20.9kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5fab8235f9ba4cddb017bf735cd7671c"}},"5ad6be31b76a4a90b2b54ff03903ae54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2ac52d35a39c4cd5983321d9ec9b8892":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8912a0981d9a4e028e88627c39435941":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7d702a31ac8c4a679899551e6ea9b7df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"721f92683d8e4c61bf85a3ddb44cb2d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5fab8235f9ba4cddb017bf735cd7671c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DR6WnnwSiiPx"},"source":["## **1. Find the corresponding positive values for NER, POS, Chunk tags**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2289ypdGa2gW","executionInfo":{"status":"ok","timestamp":1639191406036,"user_tz":-480,"elapsed":3141,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"4a2f2fe0-b6f5-49b1-ebbe-43676ea0f05c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"wNF30bg7jgaO"},"source":["# **2. Data Preprocessing for BERT Model (Apply Hugging Face Data)**"]},{"cell_type":"markdown","metadata":{"id":"5BMn0-NQnkOV"},"source":["### (1) Hugging Face Dataset Conll2003 Exploration"]},{"cell_type":"code","metadata":{"id":"ZIPMjy9KBFAX"},"source":["!pip install datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"htUmCUrUBS0D"},"source":["from datasets import load_dataset\n","# dataset = load_dataset('conll2003')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IXOdIATZoNVQ"},"source":["### (2) Covert Data to BERT Input Style"]},{"cell_type":"code","metadata":{"id":"IsEmtyyeniqn"},"source":["!pip install transformers seqeval[gpu]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mUG0I8A-ndPo"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizerFast, BertConfig #BertForTokenClassification"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-3QG7H7OW4sT","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["09226399d28c47759132106cb5b75762","85ddc09b05ab46dcb166ab52ca15f109","fc920f5abc464c4994a018b556f94735","95c30ee5bc2d44f9b661282827e79769","0017799d4f6b4daf991b8a33eb3eff73","0a634ba09874468e98fa39f837e484cf","bdb836aba4fc4c1f9ac9df6fcc48af7e","198bfbc94ce64b09b8dc80f1465f0eae","ef00d8da292f46c99b24da445fcae75d","c78d5d2303c744409fe23163c8b328ad","4bdc72bb3eb84d66b3ad7927c1cced57","6a65f646154b487f9f37d158e78e87f8","7ba074cd9a2a4885842619035413b69d","f5633d0e4a53446ca97185d9d99eff4f","25ab885cbae048ba849d84980601f583","df6c621d10064eb987ec81716719a2cf","32af295a42444f0894b1b7c7fc74165e","520ba8bc47234c2cb66dd6f71e27ad15","836ae04f9f50489180864b1fce0517c8","367022b86cdb4b67830c5b52ead270e0","9b697e71c20943ce94493c0082468d3d","e008015dc23a4ee4bec0fa28a7749911","57e16465fbf8476ab52497f83703257f","bbd5bb495f2e4dd18fbc10bab60c36a7","cda984007d8549858507c4bc0d0e1ac1","0a0a745b33394d18bad7c931f9bc7010","59605b30aed54430b9be64e73b8c427c","8e948d386a894749b189adc399bba200","74ef22932ae546fea40a0882c3792b70","a526bf414e344de4a045f5adaec5e362","1c4a90a477bf46ef8c3771a174115d8a","0ee1beaea6fd4fb88c958d9a9cdbff56","a3efc5d22dae48e8b76b5fdf9e994ae0","5af03ddec5af41d5ad6fd4a77a1a4d94","e079d9515d1148bdbaecf14e6095155a","46bc899d1c47413f8e67909fa77fcd5e","75f3d38dfd65445c82ed33f823057e26","fc6b5bdb129e4afa8366c8972ad09879","5ad6be31b76a4a90b2b54ff03903ae54","2ac52d35a39c4cd5983321d9ec9b8892","8912a0981d9a4e028e88627c39435941","7d702a31ac8c4a679899551e6ea9b7df","721f92683d8e4c61bf85a3ddb44cb2d8","5fab8235f9ba4cddb017bf735cd7671c"]},"executionInfo":{"status":"ok","timestamp":1639191028532,"user_tz":-480,"elapsed":2078,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"01bce3ef-6dad-4584-cde5-cf34f30dc789"},"source":["MAX_LEN = 128     \n","TRAIN_BATCH_SIZE = 4\n","TEST_BATCH_SIZE = 2\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09226399d28c47759132106cb5b75762","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6a65f646154b487f9f37d158e78e87f8","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57e16465fbf8476ab52497f83703257f","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5af03ddec5af41d5ad6fd4a77a1a4d94","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xpxhRReEbbKV","executionInfo":{"status":"ok","timestamp":1639191030383,"user_tz":-480,"elapsed":1854,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"2c8d8ff8-af13-495b-f454-48baeb8912a9"},"source":["data = pd.read_csv(\"/content/drive/MyDrive/NLP/Final Project/New Dataset/Bank/ner_datasetreference_new.csv\", encoding='unicode_escape')\n","data.head()\n","\n","data.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sentence #      47959\n","Word          1048575\n","POS           1048575\n","Tag           1048575\n","dtype: int64"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1cC06g2KbeMK","executionInfo":{"status":"ok","timestamp":1639191030384,"user_tz":-480,"elapsed":5,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"368f5ed4-6acb-4274-ceba-5d01bb120b7e"},"source":["'''\n","step 2a: process NE tags and POS tags\n","'''\n","# NE \n","\"\"\"There are 8 category tags, each with a \"beginning\" and \"inside\" variant, and the \"outside\" tag. It is not really clear what these tags mean - \"geo\" probably stands for geographical entity, \"gpe\" for geopolitical entity, and so on. They do not seem to correspond with what the publisher says on Kaggle. Some tags seem to be underrepresented. Let's print them by frequency (highest to lowest): \"\"\"\n","\n","# tags = {}\n","# for tag, count in zip(frequencies_NE.index, frequencies_NE):\n","#     if tag != \"O\":\n","#         if tag[2:5] not in tags.keys():\n","#             tags[tag[2:5]] = count\n","#         else:\n","#             tags[tag[2:5]] += count\n","#     continue\n","\n","# print(sorted(tags.items(), key=lambda x: x[1], reverse=True))\n","\n","\"\"\"Let's remove \"art\", \"eve\" and \"nat\" named entities, as performance on them will probably be not comparable to the other named entities. \"\"\"\n","\n","entities_to_remove = [\"B-art\", \"I-art\", \"B-eve\", \"I-eve\", \"B-nat\", \"I-nat\"]\n","data = data[~data.Tag.isin(entities_to_remove)]\n","data.head()\n","data.count()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sentence #      47920\n","Word          1047063\n","POS           1047063\n","Tag           1047063\n","dtype: int64"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cyOD0W-ubhkx","executionInfo":{"status":"ok","timestamp":1639191030384,"user_tz":-480,"elapsed":4,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"8c6e5723-c5c6-47ea-d0ff-189bc3f0c068"},"source":["\"\"\"We create 2 dictionaries for NE: one that maps individual tags to indices, and one that maps indices to their individual tags. This is necessary in order to create the labels (as computers work with numbers = indices, rather than words = tags) - see further in this notebook.\"\"\"\n","\n","labels_to_ids = {k: v for v, k in enumerate(data.Tag.unique())}\n","ids_to_labels = {v: k for v, k in enumerate(data.Tag.unique())}\n","print(labels_to_ids)\n","print(ids_to_labels)\n","\n","\"\"\"We create 2 dictionaries for POS: one that maps individual tags to indices, and one that maps indices to their individual tags. This is necessary in order to create the labels (as computers work with numbers = indices, rather than words = tags) - see further in this notebook.\"\"\"\n","\n","POS_labels_to_ids = {k: v for v, k in enumerate(data.POS.unique())}\n","POS_ids_to_labels = {v: k for v, k in enumerate(data.POS.unique())}\n","print(POS_labels_to_ids)\n","print(POS_ids_to_labels)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'I-per': 8, 'I-gpe': 9, 'I-tim': 10}\n","{0: 'O', 1: 'B-geo', 2: 'B-gpe', 3: 'B-per', 4: 'I-geo', 5: 'B-org', 6: 'I-org', 7: 'B-tim', 8: 'I-per', 9: 'I-gpe', 10: 'I-tim'}\n","{'NNS': 0, 'IN': 1, 'VBP': 2, 'VBN': 3, 'NNP': 4, 'TO': 5, 'VB': 6, 'DT': 7, 'NN': 8, 'CC': 9, 'JJ': 10, '.': 11, 'VBD': 12, 'WP': 13, '``': 14, 'CD': 15, 'PRP': 16, 'VBZ': 17, 'POS': 18, 'VBG': 19, 'RB': 20, ',': 21, 'WRB': 22, 'PRP$': 23, 'MD': 24, 'WDT': 25, 'JJR': 26, ':': 27, 'JJS': 28, 'WP$': 29, 'RP': 30, 'PDT': 31, 'NNPS': 32, 'EX': 33, 'RBS': 34, 'LRB': 35, 'RRB': 36, '$': 37, 'RBR': 38, ';': 39, 'UH': 40, 'FW': 41}\n","{0: 'NNS', 1: 'IN', 2: 'VBP', 3: 'VBN', 4: 'NNP', 5: 'TO', 6: 'VB', 7: 'DT', 8: 'NN', 9: 'CC', 10: 'JJ', 11: '.', 12: 'VBD', 13: 'WP', 14: '``', 15: 'CD', 16: 'PRP', 17: 'VBZ', 18: 'POS', 19: 'VBG', 20: 'RB', 21: ',', 22: 'WRB', 23: 'PRP$', 24: 'MD', 25: 'WDT', 26: 'JJR', 27: ':', 28: 'JJS', 29: 'WP$', 30: 'RP', 31: 'PDT', 32: 'NNPS', 33: 'EX', 34: 'RBS', 35: 'LRB', 36: 'RRB', 37: '$', 38: 'RBR', 39: ';', 40: 'UH', 41: 'FW'}\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G7jXWM2dbjHU","executionInfo":{"status":"ok","timestamp":1639191030829,"user_tz":-480,"elapsed":7,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"d4bb48d6-b5e0-43d8-b05d-03e820a06879"},"source":["# count NE tag\n","print(\"Number of NE tags: {}\".format(len(data.Tag.unique()))) # 17个\n","frequencies_NE = data.Tag.value_counts()\n","frequencies_NE\n","Ner_Tag = list(data.Tag.unique())\n","Ner_Number = [i for i in range(len(Ner_Tag))]\n","Ner = list(zip(Ner_Tag,Ner_Number))\n","print(Ner)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of NE tags: 11\n","[('O', 0), ('B-geo', 1), ('B-gpe', 2), ('B-per', 3), ('I-geo', 4), ('B-org', 5), ('I-org', 6), ('B-tim', 7), ('I-per', 8), ('I-gpe', 9), ('I-tim', 10)]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAeanPgGbk3_","executionInfo":{"status":"ok","timestamp":1639191030830,"user_tz":-480,"elapsed":6,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"2b31f04d-54fc-47c0-9131-947073805e8e"},"source":["# count POS tag\n","print(\"Number of POS tags: {}\".format(len(data.POS.unique()))) # 42个\n","frequencies_POS = data.POS.value_counts()\n","frequencies_POS\n","POS_Tag = list(data.POS.unique())\n","POS_Number = [i for i in range(len(POS_Tag))]\n","POS = list(zip(POS_Tag,POS_Number))\n","print(POS)\n","print(len(POS_Tag))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of POS tags: 42\n","[('NNS', 0), ('IN', 1), ('VBP', 2), ('VBN', 3), ('NNP', 4), ('TO', 5), ('VB', 6), ('DT', 7), ('NN', 8), ('CC', 9), ('JJ', 10), ('.', 11), ('VBD', 12), ('WP', 13), ('``', 14), ('CD', 15), ('PRP', 16), ('VBZ', 17), ('POS', 18), ('VBG', 19), ('RB', 20), (',', 21), ('WRB', 22), ('PRP$', 23), ('MD', 24), ('WDT', 25), ('JJR', 26), (':', 27), ('JJS', 28), ('WP$', 29), ('RP', 30), ('PDT', 31), ('NNPS', 32), ('EX', 33), ('RBS', 34), ('LRB', 35), ('RRB', 36), ('$', 37), ('RBR', 38), (';', 39), ('UH', 40), ('FW', 41)]\n","42\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"xEBq_ryJbmPU","executionInfo":{"status":"ok","timestamp":1639191031153,"user_tz":-480,"elapsed":328,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"02260ce5-cbac-444f-ce2c-173c261e0330"},"source":["# pandas has a very handy \"forward fill\" function to fill missing values based on the last upper non-nan value\n","data = data.fillna(method='ffill')\n","data.head()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>Thousands</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sentence: 1</td>\n","      <td>of</td>\n","      <td>IN</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sentence: 1</td>\n","      <td>demonstrators</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentence: 1</td>\n","      <td>have</td>\n","      <td>VBP</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sentence: 1</td>\n","      <td>marched</td>\n","      <td>VBN</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Sentence #           Word  POS Tag\n","0  Sentence: 1      Thousands  NNS   O\n","1  Sentence: 1             of   IN   O\n","2  Sentence: 1  demonstrators  NNS   O\n","3  Sentence: 1           have  VBP   O\n","4  Sentence: 1        marched  VBN   O"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"id":"_Uw1ALcVbnaw","executionInfo":{"status":"ok","timestamp":1639191177108,"user_tz":-480,"elapsed":145959,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"8d87ee26-299b-4d42-f320-40bf2827d7d9"},"source":["# let's create a new column called \"sentence\" which groups the words by sentence \n","data['sentence'] = data[['Sentence #','Word','Tag', 'POS']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n","# let's also create a new column called \"word_labels\" which groups the tags by sentence \n","data['word_labels'] = data[['Sentence #','Word','Tag', 'POS']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n","data['word_labels_POS'] = data[['Sentence #','Word','Tag', 'POS']].groupby(['Sentence #'])['POS'].transform(lambda x: '_,_,_'.join(x))\n","data.head()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","      <th>word_labels_POS</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>Thousands</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","      <td>Thousands of demonstrators have marched throug...</td>\n","      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n","      <td>NNS_,_,_IN_,_,_NNS_,_,_VBP_,_,_VBN_,_,_IN_,_,_...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sentence: 1</td>\n","      <td>of</td>\n","      <td>IN</td>\n","      <td>O</td>\n","      <td>Thousands of demonstrators have marched throug...</td>\n","      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n","      <td>NNS_,_,_IN_,_,_NNS_,_,_VBP_,_,_VBN_,_,_IN_,_,_...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sentence: 1</td>\n","      <td>demonstrators</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","      <td>Thousands of demonstrators have marched throug...</td>\n","      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n","      <td>NNS_,_,_IN_,_,_NNS_,_,_VBP_,_,_VBN_,_,_IN_,_,_...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentence: 1</td>\n","      <td>have</td>\n","      <td>VBP</td>\n","      <td>O</td>\n","      <td>Thousands of demonstrators have marched throug...</td>\n","      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n","      <td>NNS_,_,_IN_,_,_NNS_,_,_VBP_,_,_VBN_,_,_IN_,_,_...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sentence: 1</td>\n","      <td>marched</td>\n","      <td>VBN</td>\n","      <td>O</td>\n","      <td>Thousands of demonstrators have marched throug...</td>\n","      <td>O,O,O,O,O,O,B-geo,O,O,O,O,O,B-geo,O,O,O,O,O,B-...</td>\n","      <td>NNS_,_,_IN_,_,_NNS_,_,_VBP_,_,_VBN_,_,_IN_,_,_...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Sentence #  ...                                    word_labels_POS\n","0  Sentence: 1  ...  NNS_,_,_IN_,_,_NNS_,_,_VBP_,_,_VBN_,_,_IN_,_,_...\n","1  Sentence: 1  ...  NNS_,_,_IN_,_,_NNS_,_,_VBP_,_,_VBN_,_,_IN_,_,_...\n","2  Sentence: 1  ...  NNS_,_,_IN_,_,_NNS_,_,_VBP_,_,_VBN_,_,_IN_,_,_...\n","3  Sentence: 1  ...  NNS_,_,_IN_,_,_NNS_,_,_VBP_,_,_VBN_,_,_IN_,_,_...\n","4  Sentence: 1  ...  NNS_,_,_IN_,_,_NNS_,_,_VBP_,_,_VBN_,_,_IN_,_,_...\n","\n","[5 rows x 7 columns]"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5K8DkrYmbsU5","executionInfo":{"status":"ok","timestamp":1639191177592,"user_tz":-480,"elapsed":487,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"6c9fa0fb-e6cd-4fc8-a0f2-bf42d5bf983a"},"source":["\"\"\"Let's only keep the \"sentence\" and \"word_labels\" and \"word_labels_POS\" columns, and drop duplicates:\"\"\"\n","\n","data = data[[\"sentence\", \"word_labels\", 'word_labels_POS']].drop_duplicates().reset_index(drop=True)\n","data.head()\n","\n","len(data)\n","\"\"\"Let's verify that a random sentence and its corresponding tags are correct:\"\"\"\n","\n","print(data.iloc[41].sentence)\n","print(data.iloc[41].word_labels)\n","print(data.iloc[41].word_labels_POS)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bedfordshire police said Tuesday that Omar Khayam was arrested in Bedford for breaching the conditions of his parole .\n","B-gpe,O,O,B-tim,O,B-per,I-per,O,O,O,B-geo,O,O,O,O,O,O,O,O\n","NNP_,_,_NNS_,_,_VBD_,_,_NNP_,_,_IN_,_,_NNP_,_,_NNP_,_,_VBD_,_,_VBN_,_,_IN_,_,_NNP_,_,_IN_,_,_VBG_,_,_DT_,_,_NNS_,_,_IN_,_,_PRP$_,_,_NN_,_,_.\n"]}]},{"cell_type":"code","metadata":{"id":"Koyj62b7btmv"},"source":["train_df, validate_df, test_df = \\\n","              np.split(data.sample(frac=1, random_state=42), \n","                       [int(.85*len(data)), int(.925*len(data))])\n","# train_df['word_labels'][33891]\n","\n","# train_df.index= range(len(train_df))\n","# validate_df.index= range(len(validate_df))\n","# test_df.index= range(len(test_df))\n","\n","train_df = train_df.reset_index(drop=True)\n","validate_df = validate_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","data_combine_dict = {'train':train_df, 'validation':validate_df, 'test':test_df}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"blADhhnVBS44"},"source":["class Preprocess_Data(Dataset):\n","  def __init__(self, dataset, tokenizer, max_len): #usage -> train, validation, test\n","        self.len = len(dataset)\n","        self.data = dataset\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","  def __getitem__(self, index):\n","\n","        # step 1: get the sentence and word labels \n","        # print(str(index))\n","        sentence = self.data.sentence[index].strip().split()\n","        # print(str(sentence))\n","        word_labels = self.data.word_labels[index].split(\",\")\n","        # print(str(word_labels))\n","        pos_labels = self.data.word_labels_POS[index].split(\"_,_,_\")\n","        # print(str(pos_labels))\n","        # print('error place 2')\n","        \n","        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n","        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n","        encoding = self.tokenizer(sentence,\n","                              is_split_into_words=True,\n","                              return_offsets_mapping=True,  #Set to True to return (char_start, char_end) for each token (default False)\n","                              padding='max_length', \n","                              truncation=True, \n","                              max_length=self.max_len)\n","        \n","        \n","        # step 3: create token labels only for first word pieces of each tokenized word\n","\n","        labels = [labels_to_ids[label] for label in word_labels]\n","        pos = [POS_labels_to_ids[label_POS] for label_POS in pos_labels]\n","\n","        # print('error place 3')\n","        # create an empty array of -100 of length max_length\n","        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n","        encoded_pos = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * len(POS_Tag)\n","        \n","        # set only labels whose first offset position is 0 and the second is not 0\n","        i = 0\n","        for idx, mapping in enumerate(encoding[\"offset_mapping\"]): #training_set[i][\"input_ids\"]\n","          if mapping[0] == 0 and mapping[1] != 0:\n","            # overwrite label\n","            encoded_labels[idx] = labels[i]\n","            encoded_pos[idx] = pos[i]\n","            i += 1\n","\n","        # step 4: turn everything into PyTorch tensors\n","        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n","        item['labels'] = torch.as_tensor(encoded_labels)\n","\n","        # step 5: pos\n","        item['pos_ids'] = torch.as_tensor(encoded_pos)\n","\n","        return item\n","\n","  def __len__(self):\n","        return self.len        \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1lQPUyEGcErj","executionInfo":{"status":"ok","timestamp":1639191177891,"user_tz":-480,"elapsed":8,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"76a9e828-2e92-4625-c1d3-ddde652f80b8"},"source":["dataset = data_combine_dict\n","\n","training_set = Preprocess_Data(train_df, tokenizer, MAX_LEN)\n","validation_set = Preprocess_Data(validate_df, tokenizer, MAX_LEN)\n","testing_set = Preprocess_Data(test_df, tokenizer, MAX_LEN)\n","print(len(training_set),len(validation_set),len(testing_set))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["40439 3568 3569\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y8UNHHU3cIZ0","executionInfo":{"status":"ok","timestamp":1639191177891,"user_tz":-480,"elapsed":7,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"7bc69a56-3d19-44ec-d2f9-995a7e718dac"},"source":["i = 1\n","for token,token_type_id,number,pos,mask,label in zip(tokenizer.convert_ids_to_tokens(training_set[i][\"input_ids\"]), training_set[i][\"token_type_ids\"],training_set[i][\"input_ids\"],training_set[i][\"pos_ids\"],training_set[i][\"attention_mask\"],training_set[i][\"labels\"]):\n","  print('{0:13}  {1:10}  {2:10}  {3:10}  {4:10}  {5}'.format(token, token_type_id, number, pos, mask, label))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS]                   0         101          42           1  -100\n","A                       0         138           7           1  0\n","video                   0        1888           8           1  0\n","released                0        1308           3           1  0\n","to                      0        1106           5           1  0\n","the                     0        1103           7           1  0\n","Arabic                  0        4944           4           1  1\n","news                    0        2371           8           1  0\n","channel                 0        3094           8           1  0\n","Al                      0        2586           4           1  0\n","-                       0         118          42           1  -100\n","J                       0         147          42           1  -100\n","##az                    0       10961          42           1  -100\n","##eera                  0       27472          42           1  -100\n","in                      0        1107           1           1  0\n","November                0        1379           4           1  7\n","showed                  0        2799          12           1  0\n","the                     0        1103           7           1  0\n","murder                  0        3513           8           1  0\n","of                      0        1104           1           1  0\n","a                       0         170           7           1  0\n","western                 0        2466          10           1  0\n","woman                   0        1590           8           1  0\n",",                       0         117          21           1  0\n","believed                0        2475           3           1  0\n","to                      0        1106           5           1  0\n","be                      0        1129           6           1  0\n","Mrs                     0        2823           4           1  3\n",".                       0         119          42           1  -100\n","Hassan                  0       13583           4           1  8\n",".                       0         119          11           1  0\n","[SEP]                   0         102          42           1  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n","[PAD]                   0           0          42           0  -100\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ckSW6ZQ0fYJ","executionInfo":{"status":"ok","timestamp":1639191177892,"user_tz":-480,"elapsed":7,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"b6127381-d750-457c-944d-1dbaddcdda79"},"source":["# Define the Dataloader\n","training_loader = DataLoader(training_set, batch_size = TRAIN_BATCH_SIZE, shuffle=True,num_workers=0)\n","validation_loader = DataLoader(validation_set,batch_size = TRAIN_BATCH_SIZE, shuffle=True,num_workers=0)\n","testing_loader = DataLoader(testing_set,batch_size = TEST_BATCH_SIZE, shuffle=True,num_workers=0)\n","\n","print(len(training_loader),len(validation_loader),len(testing_loader))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10110 892 1785\n"]}]},{"cell_type":"markdown","metadata":{"id":"6X7rxuYMy6_z"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"6hF-6klfKomd"},"source":["# **3. Define the Model**"]},{"cell_type":"markdown","metadata":{"id":"IKXls9ALdeJt"},"source":["### 1) Train the Model"]},{"cell_type":"code","metadata":{"id":"ndNwQePxFFdY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639191178493,"user_tz":-480,"elapsed":608,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"2994a0e8-4275-4d13-e8ed-0f48b52bbd49"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Dec 11 02:52:57 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P0    44W / 400W |      0MiB / 40536MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6LvvmRHQMH2g","executionInfo":{"status":"ok","timestamp":1639191179926,"user_tz":-480,"elapsed":347,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"d22e71b9-b5a3-47fd-ff67-b87417baf665"},"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","metadata":{"id":"VCEY_Fb2a7Hm"},"source":["EPOCHS = 4\n","LEARNING_RATE = 1e-05\n","MAX_GRAD_NORM = 10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Txi4qKmD5tf"},"source":["### Redesign the model"]},{"cell_type":"code","metadata":{"id":"zVAQF_WQ7O_z"},"source":["from transformers.models.bert.modeling_bert import BertSelfAttention,BertSelfOutput,BertAttention,BertIntermediate,BertOutput,BertLayer,BertEncoder,BertPooler,BertPredictionHeadTransform,BertLMPredictionHead,BertOnlyMLMHead,BertOnlyNSPHead,BertPreTrainingHeads,BertPreTrainedModel,BertForPreTrainingOutput\n","from packaging import version\n","from transformers.file_utils import (\n","    ModelOutput,\n","    add_code_sample_docstrings,\n","    add_start_docstrings,\n","    add_start_docstrings_to_model_forward,\n","    replace_return_docstrings,\n",")\n","from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions,TokenClassifierOutput\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","\n","class BertEmbeddings1(nn.Module):\n","    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n","        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n","        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n","\n","        #self.pos_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n","        self.pos_embeddings = nn.Embedding(len(POS_Tag)+1,config.hidden_size)    \n","\n","        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n","        # any TensorFlow checkpoint file\n","        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n","        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n","        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n","        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n","            self.register_buffer(\n","                \"token_type_ids\",\n","                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n","                persistent=False,\n","            )\n","\n","    def forward(\n","        self, input_ids=None, pos_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n","    ):\n","        if input_ids is not None:\n","            input_shape = input_ids.size()\n","        else:\n","            input_shape = inputs_embeds.size()[:-1]\n","\n","        seq_length = input_shape[1]\n","\n","        if position_ids is None:\n","            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n","\n","        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n","        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n","        # issue #5664\n","        if token_type_ids is None:\n","            if hasattr(self, \"token_type_ids\"):\n","                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n","                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n","                token_type_ids = buffered_token_type_ids_expanded\n","            else:\n","                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n","\n","        if inputs_embeds is None:\n","            inputs_embeds = self.word_embeddings(input_ids)\n","        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n","        pos_embeddings = self.pos_embeddings(pos_ids)\n","\n","        embeddings = inputs_embeds + token_type_embeddings + pos_embeddings\n","        if self.position_embedding_type == \"absolute\":\n","            position_embeddings = self.position_embeddings(position_ids)\n","            embeddings += position_embeddings\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings\n","\n","class BertModel1(BertPreTrainedModel):\n","    \"\"\"\n","\n","    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n","    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n","    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n","    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n","\n","    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n","    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n","    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n","    input to the forward pass.\n","    \"\"\"\n","\n","    def __init__(self, config, add_pooling_layer=True):\n","        super().__init__(config)\n","        self.config = config\n","\n","        self.embeddings = BertEmbeddings1(config)\n","        self.encoder = BertEncoder(config)\n","\n","        self.pooler = BertPooler(config) if add_pooling_layer else None\n","\n","        self.init_weights()\n","\n","    def get_input_embeddings(self):\n","        return self.embeddings.word_embeddings\n","\n","    def set_input_embeddings(self, value):\n","        self.embeddings.word_embeddings = value\n","\n","    def _prune_heads(self, heads_to_prune):\n","        \"\"\"\n","        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n","        class PreTrainedModel\n","        \"\"\"\n","        for layer, heads in heads_to_prune.items():\n","            self.encoder.layer[layer].attention.prune_heads(heads)\n","\n","    #@add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n","    #@add_code_sample_docstrings(\n","    #    processor_class=_TOKENIZER_FOR_DOC,\n","    #    checkpoint=_CHECKPOINT_FOR_DOC,\n","    #   output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n","    #    config_class=_CONFIG_FOR_DOC,\n","    #)\n","    def forward(\n","        self,\n","        input_ids=None,\n","        pos_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        past_key_values=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n","            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n","            the model is configured as a decoder.\n","        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n","            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n","\n","            - 1 for tokens that are **not masked**,\n","            - 0 for tokens that are **masked**.\n","        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n","            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n","\n","            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n","            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n","            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n","        use_cache (:obj:`bool`, `optional`):\n","            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n","            decoding (see :obj:`past_key_values`).\n","        \"\"\"\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        if self.config.is_decoder:\n","            use_cache = use_cache if use_cache is not None else self.config.use_cache\n","        else:\n","            use_cache = False\n","\n","        if input_ids is not None and inputs_embeds is not None:\n","            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n","        elif input_ids is not None:\n","            input_shape = input_ids.size()\n","        elif inputs_embeds is not None:\n","            input_shape = inputs_embeds.size()[:-1]\n","        else:\n","            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n","\n","        batch_size, seq_length = input_shape\n","        device = input_ids.device if input_ids is not None else inputs_embeds.device\n","\n","        # past_key_values_length\n","        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n","\n","        if attention_mask is None:\n","            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n","\n","        if token_type_ids is None:\n","            if hasattr(self.embeddings, \"token_type_ids\"):\n","                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n","                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n","                token_type_ids = buffered_token_type_ids_expanded\n","            else:\n","                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n","        \n","        # if pos_ids is None:\n","        #   pos_ids = torch.zeros(input_shape,dtype=torch.long,device=device)\n","        # else:\n","        #   pos_ids = pos_ids.to(device)\n","\n","        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n","        # ourselves in which case we just need to make it broadcastable to all heads.\n","        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n","\n","        # If a 2D or 3D attention mask is provided for the cross-attention\n","        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n","        if self.config.is_decoder and encoder_hidden_states is not None:\n","            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n","            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n","            if encoder_attention_mask is None:\n","                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n","            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n","        else:\n","            encoder_extended_attention_mask = None\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n","        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n","        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n","\n","        embedding_output = self.embeddings(\n","            input_ids=input_ids,\n","            pos_ids=pos_ids,\n","            position_ids=position_ids,\n","            token_type_ids=token_type_ids,\n","            inputs_embeds=inputs_embeds,\n","            past_key_values_length=past_key_values_length,\n","        )\n","        encoder_outputs = self.encoder(\n","            embedding_output,\n","            attention_mask=extended_attention_mask,\n","            head_mask=head_mask,\n","            encoder_hidden_states=encoder_hidden_states,\n","            encoder_attention_mask=encoder_extended_attention_mask,\n","            past_key_values=past_key_values,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        sequence_output = encoder_outputs[0]\n","        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n","\n","        if not return_dict:\n","            return (sequence_output, pooled_output) + encoder_outputs[1:]\n","\n","        return BaseModelOutputWithPoolingAndCrossAttentions(\n","            last_hidden_state=sequence_output,\n","            pooler_output=pooled_output,\n","            past_key_values=encoder_outputs.past_key_values,\n","            hidden_states=encoder_outputs.hidden_states,\n","            attentions=encoder_outputs.attentions,\n","            cross_attentions=encoder_outputs.cross_attentions,\n","        )\n","\n","#@add_start_docstrings(\n","#    \"\"\"\n","#    Bert Model with two heads on top as done during the pretraining: a `masked language modeling` head and a `next\n","#    sentence prediction (classification)` head.\n","#    \"\"\",\n","#    BERT_START_DOCSTRING,\n","#)\n","\n","class BertForTokenClassification1(BertPreTrainedModel):\n","\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.bert = BertModel1(config, add_pooling_layer=False)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","\n","        self.init_weights()\n","\n","    #@add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n","    #@add_code_sample_docstrings(\n","    #    processor_class=_TOKENIZER_FOR_DOC,\n","    #    checkpoint=_CHECKPOINT_FOR_DOC,\n","    #    output_type=TokenClassifierOutput,\n","    #    config_class=_CONFIG_FOR_DOC,\n","    #)\n","    def forward(\n","        self,\n","        input_ids=None,\n","        pos_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n","            1]``.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.bert(\n","            input_ids,\n","            pos_ids=pos_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)\n","                active_labels = torch.where(\n","                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n","                )\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return TokenClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n","\n","\n","#@add_start_docstrings(\n","#    \"\"\"\n","#    Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n","#    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n","#    \"\"\",\n","#    BERT_START_DOCSTRING,\n","#)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o0tDKcDYOlQM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639191180250,"user_tz":-480,"elapsed":8,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"25e05ecc-3e55-4b66-be0a-2a6cdd919bdf"},"source":["device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'cuda'"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"Y4u9CmSKEGV0"},"source":["### Train the model"]},{"cell_type":"code","metadata":{"id":"K8vCJCAFLWgK"},"source":["# Define the model by just BertForTokenClassification\n","model = BertForTokenClassification1.from_pretrained('bert-base-cased', num_labels=len(Ner_Tag))\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"blREmCNMPgpY"},"source":["optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aw7dwnyx_d_L"},"source":["import time\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3800AVERAin"},"source":["def train(epoch):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    # put model in training mode\n","    model.train()\n","    \n","    for idx, batch in enumerate(training_loader):\n","\n","        # if idx >200:\n","        #   break\n","        \n","        ids = batch['input_ids'].to(device, dtype = torch.long)\n","        mask = batch['attention_mask'].to(device, dtype = torch.long)\n","        labels = batch['labels'].to(device, dtype = torch.long)\n","        pos = batch['pos_ids'].to(device,dtype = torch.long)\n","\n","        outputs = model(input_ids=ids, pos_ids=pos,attention_mask=mask, labels=labels)\n","        loss = outputs[0]\n","        tr_logits = outputs[1]\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 100==0:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","            if idx!=0:\n","              print(\"--- %s seconds ---\" % (time.time() - start_time))\n","            start_time = time.time()    \n","           \n","        # compute training accuracy\n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","        \n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        \n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","\n","        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","        tr_accuracy += tmp_tr_accuracy\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","    print(f\"Training loss epoch: {epoch_loss}\")\n","    print(f\"Training accuracy epoch: {tr_accuracy}\")\n","\n","    # --------------------------------------------------------------------------------------------------------------------\n","    # After the completion of each training epoch\n","    # measure our performance on validation set.\n","\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(validation_loader):\n","\n","            # if idx >200:\n","            #   break\n","            \n","            ids = batch['input_ids'].to(device, dtype = torch.long)\n","            mask = batch['attention_mask'].to(device, dtype = torch.long)\n","            labels = batch['labels'].to(device, dtype = torch.long)\n","            pos = batch['pos_ids'].to(device,dtype = torch.long)\n","            \n","            # outputs= model(input_ids=ids, attention_mask=mask, labels=labels)\n","            outputs = model(input_ids=ids, pos_ids=pos,attention_mask=mask, labels=labels)\n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    print(f\"Validation Loss: {eval_loss}\")\n","    print(f\"Validation Accuracy: {eval_accuracy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bYynGVa0aUYt","executionInfo":{"status":"ok","timestamp":1639191314501,"user_tz":-480,"elapsed":12,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"597b4a5f-a9a6-45df-dd72-b3e77b9e6d47"},"source":["len(training_loader),len(validation_loader),len(testing_loader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10110, 892, 1785)"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"6XTItahi_1kf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639191314501,"user_tz":-480,"elapsed":9,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"b8d6a812-081f-4147-e0ca-6b973352c2f7"},"source":["from seqeval.metrics import classification_report\n","New_NerDict = dict((v,k) for k,v in dict(Ner).items())\n","New_NerDict"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'O',\n"," 1: 'B-geo',\n"," 2: 'B-gpe',\n"," 3: 'B-per',\n"," 4: 'I-geo',\n"," 5: 'B-org',\n"," 6: 'I-org',\n"," 7: 'B-tim',\n"," 8: 'I-per',\n"," 9: 'I-gpe',\n"," 10: 'I-tim'}"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1IXBJw6APRDlVbWtWEGIYrBnUgk46U-Pd"},"id":"kjAskZ2r_8vf","outputId":"e2e0dd41-09a7-4861-952a-47a072cbd54e","executionInfo":{"status":"ok","timestamp":1639193702010,"user_tz":-480,"elapsed":0,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}}},"source":["EPOCHS = 3\n","for epoch in range(EPOCHS):\n","\n","    directory = \"/content/drive/MyDrive/NLP/Final Project/New Dataset/Bank/POS_Embedding_layer/Cased/Model_\"+str(epoch+1)\n","\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train(epoch)\n","    labels, predictions = valid(model, testing_loader)\n","\n","    labels_value = [[New_NerDict[i.item()] for i in labels]]\n","    pred_value = [[New_NerDict[i.item()] for i in predictions]]\n","\n","    print(classification_report(labels_value, pred_value))\n","\n","    tokenizer.save_vocabulary(directory)\n","    # save the model weights and its configuration file\n","    model.save_pretrained(directory)\n","    print('All files saved')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"kv3PQZypdQ_t"},"source":["### 2) Evaluate the Model"]},{"cell_type":"code","metadata":{"id":"jFIfRvYXdQRv"},"source":["def valid(model, testing_loader):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(testing_loader):\n","            \n","            ids = batch['input_ids'].to(device, dtype = torch.long)\n","            mask = batch['attention_mask'].to(device, dtype = torch.long)\n","            labels = batch['labels'].to(device, dtype = torch.long)\n","            pos = batch['pos_ids'].to(device,dtype = torch.long)\n","\n","            outputs = model(input_ids=ids, pos_ids=pos,attention_mask=mask, labels=labels)\n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","                if idx!=0:\n","                  print(\"--- %s seconds ---\" % (time.time() - start_time))\n","                start_time = time.time()\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    print(f\"Testing Loss: {eval_loss}\")\n","    print(f\"Testing Accuracy: {eval_accuracy}\")\n","\n","    return eval_labels, eval_preds\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7ArcZ0wusBi"},"source":["len(testing_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d6eqElS_5v-e"},"source":["model.load_state_dict(torch.load('/content/drive/MyDrive/NLP/Final Project/Code/Baseline/Baseline Saved Model/pytorch_model.bin'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJN32jfGd3sQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637304480661,"user_tz":-480,"elapsed":371651,"user":{"displayName":"Xuanjin JI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgH0VUB3HpvI_MX3OiKITS606e9ji6IlLVLHhb9Hg=s64","userId":"14027953670718372733"}},"outputId":"49938f39-7477-420b-bad6-3ab61ba58c9f"},"source":["labels, predictions = valid(model, testing_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l-j7ltuGjWAB"},"source":["New_NerDict = dict((v,k) for k,v in dict(Ner).items())\n","New_NerDict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WUkwANjxkP2q"},"source":["labels_value = [[New_NerDict[i.item()] for i in labels]]\n","pred_value = [[New_NerDict[i.item()] for i in predictions]]\n","print(labels_value)\n","print(pred_value)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CdHNOEf5d7Sf"},"source":["from seqeval.metrics import classification_report\n","\n","print(classification_report(labels_value, pred_value))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tLT4WtJ_MS3F"},"source":["### 3) Save Model"]},{"cell_type":"code","metadata":{"id":"oq6073uHMWsu"},"source":["import os\n","\n","directory = \"./model\"\n","\n","if not os.path.exists(directory):\n","    os.makedirs(directory)\n","\n","# save vocabulary of the tokenizer\n","tokenizer.save_vocabulary(directory)\n","# save the model weights and its configuration file\n","model.save_pretrained(directory)\n","print('All files saved')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qT66WzNhmGTT"},"source":["#torch.save(model, 'model.pth')\n","\n","#torch.save(model.state_dict(), 'model_weights.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"101dcJ0w5rKK"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]}]}